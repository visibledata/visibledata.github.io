{
  "hash": "b154e17b119b473270fadc7fa7d2b646",
  "result": {
    "markdown": "---\ntitle: 'The Office: Who is who?'\ndescription: \"More stub!\"\ndraft: true\nabstract: >\n  Long stubb!.\ndate: '2019-12-16'\ncode-fold: true\nimage: /posts/softblock-demo/three-designs.jpg\n---\n\n\nI'm going to basically copy Julia Silge's work from here: https://juliasilge.com/blog/tidy-text-classification/\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tidyverse\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(\"tidytext\")\nlibrary(\"schrute\")\n```\n:::\n\nI'd like to tidy up the data a little bit:\n\n::: {.cell}\n\n```{.r .cell-code}\ntheoffice_characters <- theoffice %>%\n  select(season:text) %>%\n  mutate(season = as.numeric(season),\n         episode = as.numeric(episode))\ntheoffice_characters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 55,130 × 7\n   season episode episode_name director   writer                 character text \n    <dbl>   <dbl> <chr>        <chr>      <chr>                  <chr>     <chr>\n 1      1       1 Pilot        Ken Kwapis Ricky Gervais;Stephen… Michael   All …\n 2      1       1 Pilot        Ken Kwapis Ricky Gervais;Stephen… Jim       Oh, …\n 3      1       1 Pilot        Ken Kwapis Ricky Gervais;Stephen… Michael   So y…\n 4      1       1 Pilot        Ken Kwapis Ricky Gervais;Stephen… Jim       Actu…\n 5      1       1 Pilot        Ken Kwapis Ricky Gervais;Stephen… Michael   All …\n 6      1       1 Pilot        Ken Kwapis Ricky Gervais;Stephen… Michael   Yes,…\n 7      1       1 Pilot        Ken Kwapis Ricky Gervais;Stephen… Michael   I've…\n 8      1       1 Pilot        Ken Kwapis Ricky Gervais;Stephen… Pam       Well…\n 9      1       1 Pilot        Ken Kwapis Ricky Gervais;Stephen… Michael   If y…\n10      1       1 Pilot        Ken Kwapis Ricky Gervais;Stephen… Pam       What?\n# … with 55,120 more rows\n```\n:::\n:::\n\nIn Julia's article she adds a `document` row without really highlighting why... I think it's actually crucial, so let's do that:\n\n::: {.cell}\n\n```{.r .cell-code}\ntheoffice_characters <- theoffice_characters %>%\n  mutate(document = row_number())\n```\n:::\n\n\nI want to see how good Jim and Dwight's impressions of one another are, which appear in \"Product Recall\" - Season 3, Episode 21. \n\nLet's use every episode before this as our training set:\n\n::: {.cell}\n\n```{.r .cell-code}\ntheoffice_before_product_recall <- theoffice_characters %>%\n  filter(season <= 3,\n         episode < 21)\n```\n:::\n\nFor the moment we only care about Jim and Dwight, so let's extract out the unigrams for each speaker using `unnest_tokens()` and throw away the stop words:\n\n::: {.cell}\n\n```{.r .cell-code}\njim_dwight_unigrams_before_product_recall <- theoffice_characters %>%\n  filter(character %in% c(\"Dwight\", \"Jim\")) %>%\n  select(character, text, document) %>%\n  unnest_tokens(word, text) %>%\n  anti_join(get_stopwords())\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\n```\n:::\n:::\n\nI'm going to duplicate one of Julia's charts so we can compare the most common words used by Dwight and Jim.\n\n::: {.cell}\n\n```{.r .cell-code}\njim_dwight_unigrams_before_product_recall %>%\n  count(character, word, sort = TRUE) %>%\n  group_by(character) %>%\n  top_n(20) %>%\n  ungroup() %>%\n  ggplot(aes(reorder_within(word, n, character), n,\n    fill = character\n  )) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  scale_x_reordered() +\n  coord_flip() +\n  facet_wrap(~character, scales = \"free\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(\n    x = NULL, y = \"Word count\",\n    title = \"Most frequent words after removing stop words\",\n    subtitle = \"Words like 'said' occupy similar ranks but other words are quite different\"\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nThese lists are very similar to one another. That's because the stop words in the `{tidytext}` package are collated from prose, and not from spoken word - or **dialogue**.\n\nWe \n\n::: {.cell}\n\n```{.r .cell-code}\ntheoffice_characters %>%\n  filter(character %in% c(\"Dwight\", \"Jim\")) %>%\n  select(character, text, document) %>%\n  unnest_tokens(ngrams, text, token = \"ngrams\", n = 2) %>%\n  separate(ngrams, into = c(\"word_1\",\n                            \"word_2\")) %>%\n  filter(!word_1 %in% stop_words$word,\n         !word_2 %in% stop_words$word) %>%\n  count(character, word_1, word_2, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Expected 2 pieces. Additional pieces discarded in 12536 rows [5, 6, 69,\n70, 126, 127, 128, 132, 133, 134, 144, 145, 156, 165, 166, 198, 241, 242, 268,\n269, ...].\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8,636 × 4\n   character word_1    word_2       n\n   <chr>     <chr>     <chr>    <int>\n 1 Jim       <NA>      <NA>       936\n 2 Dwight    <NA>      <NA>       737\n 3 Dwight    dwight    schrute     50\n 4 Dwight    hey       hey         42\n 5 Dwight    ha        ha          37\n 6 Dwight    dunder    mifflin     36\n 7 Dwight    regional  manager     33\n 8 Dwight    wait      wait        24\n 9 Dwight    assistant regional    20\n10 Dwight    la        la          17\n# … with 8,626 more rows\n```\n:::\n:::\n\nWe need to create ourselves a set of filler words:\n\n::: {.cell}\n\n```{.r .cell-code}\nfiller_words <- tibble(\n  word = c(\"yeah\", \"oh\", \"well\", \"like\", \"uh\", \"okay\", \"just\", \"um\")\n)\n```\n:::\n\nNow we have ourselves \n\n::: {.cell}\n\n```{.r .cell-code}\njim_dwight_unigrams_before_product_recall <- theoffice_characters %>%\n  filter(character %in% c(\"Dwight\", \"Jim\")) %>%\n  select(character, text, document) %>%\n  unnest_tokens(word, text) %>%\n  anti_join(get_stopwords()) %>%\n  anti_join(filler_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\nJoining, by = \"word\"\n```\n:::\n\n```{.r .cell-code}\njim_dwight_unigrams_before_product_recall %>%\n  count(character, word, sort = TRUE) %>%\n  group_by(character) %>%\n  top_n(20) %>%\n  ungroup() %>%\n  ggplot(aes(reorder_within(word, n, character), n,\n    fill = character\n  )) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  scale_x_reordered() +\n  coord_flip() +\n  facet_wrap(~character, scales = \"free\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(\n    x = NULL, y = \"Word count\",\n    title = \"Most frequent words after removing stop words\",\n    subtitle = \"Words like 'said' occupy similar ranks but other words are quite different\"\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n## Building our model\n\nThe `{rsample}` package is designe for splitting up data into training and test sets.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"rsample\")\n```\n:::\n\nBut we're going to be unfair and split our data as follows:\n\n- Training: the episodes before the impressions\n- Test: the episode of the impressions\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_training_data <- theoffice_before_product_recall %>%\n  select(character, text, document) %>%\n  unnest_tokens(word, text) %>%\n  anti_join(get_stopwords()) %>%\n  anti_join(filler_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\nJoining, by = \"word\"\n```\n:::\n:::\n\nTransform into a sparse matrix:\n\n::: {.cell}\n\n```{.r .cell-code}\ntheoffice_sparse_words <- tidy_training_data %>%\n  count(document, word) %>%\n  inner_join(tidy_training_data) %>%\n  cast_sparse(document, word, n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = c(\"document\", \"word\")\n```\n:::\n:::\n\n\n\nNow we build a data.frame to store our response variable:\n\n::: {.cell}\n\n```{.r .cell-code}\nword_rownames <- as.integer(rownames(theoffice_sparse_words))\n\ntheoffice_rejoined_product_recall <- tibble(document = word_rownames) %>%\n  left_join(theoffice_before_product_recall %>%\n              select(document, character))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"document\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(theoffice_rejoined_product_recall)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10850     2\n```\n:::\n\n```{.r .cell-code}\ndim(theoffice_sparse_words)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10850  7932\n```\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"glmnet\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'Matrix'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded glmnet 4.1-4\n```\n:::\n\n```{.r .cell-code}\nlibrary(\"doMC\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: foreach\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'foreach'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: iterators\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: parallel\n```\n:::\n\n```{.r .cell-code}\nregisterDoMC(cores = 8)\n\nis_jim <- theoffice_rejoined_product_recall$character == \"Jim\"\nmodel <- cv.glmnet(theoffice_sparse_words, is_jim,\n  family = \"binomial\",\n  parallel = TRUE, keep = TRUE\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\n\ncoefs <- model$glmnet.fit %>%\n  tidy() %>%\n  filter(lambda == model$lambda.1se)\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoefs %>%\n  group_by(estimate > 0) %>%\n  top_n(10, abs(estimate)) %>%\n  ungroup() %>%\n  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}