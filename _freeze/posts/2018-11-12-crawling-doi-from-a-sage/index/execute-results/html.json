{
  "hash": "533351672733e945c7411556a0ba74fb",
  "result": {
    "markdown": "---\ntitle: Crawling DOI from a SAGE\ndate: '2018-11-12'\nslug: crawling-doi-from-a-sage\ntags:\n  - web scraping\n  - list columns\n  - parallel programming\nimage: \"/posts/2018-11-12-crawling-doi-from-a-sage/2018-11-12_futured-dois.png\"\n---\n\nRecently a friend told me they were doing a systematic review of the Hand Therapy Journal published by SAGE. They wanted a way to scrape the journal for all publications - without going mad in the process. It seemed like a good excuse to get them motivated to learn R and for me to practice web scraping with the `rvest` package. \n\nWe'll go through the following steps:\n\n1. Generate URLs for all issues of the journal\n1. Inspect the source code for the page for the DOI\n1. Scrape all the pages\n1. Scrape all the pages using parallel processing via the `future` package\n\nLet's load all the packaes we're going to use up front:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\nlibrary(\"glue\")\nlibrary(\"httr\")\nlibrary(\"future\")\n```\n:::\n\n### URLs for each journal issue\n\nLet's simplify things and only consider issues of the journal up to the date this blogpost was written (2018-11-12). I can't guarantee that the journal won't completely change their URL scheme tomorrow, but until they do change things all issues have the following URL structure:\n\n    https://journals.sagepub.com/toc/hthb/{volume}/{issue}\n    \nThere have always been 4 issues a year, and the most recent volume is 23. Let's setup a `tibble()` with this data:\n\n::: {.cell}\n\n```{.r .cell-code}\nissue_urls <- tibble(volume = 1:23, issue_1 = 1, issue_2 = 2, issue_3 = 3, issue_4 = 4)\n```\n:::\n\nI'll now use `gather()` to convert this into a long and tidy dataset that iterates through all issues:\n\n::: {.cell}\n\n```{.r .cell-code}\nissue_urls <- issue_urls %>%\n  gather(issue.colname, issue, issue_1:issue_4) %>%\n  select(-issue.colname) %>%\n  arrange(volume)\n```\n:::\n\nNow we can construct our URLs using `glue()`\n\n::: {.cell}\n\n```{.r .cell-code}\nissue_urls <- issue_urls %>%\n  mutate(issue_url = glue(\"https://journals.sagepub.com/toc/hthb/{volume}/{issue}\"))\nhead(issue_urls)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  volume issue issue_url                                \n   <int> <dbl> <glue>                                   \n1      1     1 https://journals.sagepub.com/toc/hthb/1/1\n2      1     2 https://journals.sagepub.com/toc/hthb/1/2\n3      1     3 https://journals.sagepub.com/toc/hthb/1/3\n4      1     4 https://journals.sagepub.com/toc/hthb/1/4\n5      2     1 https://journals.sagepub.com/toc/hthb/2/1\n6      2     2 https://journals.sagepub.com/toc/hthb/2/2\n```\n:::\n:::\n\n### Inspect the source code for the DOI\n\nInspecting the source code reveals that the title of each article in the issue has the attribute `data-item-name`, with the value `click-article-title`.\n\n<img src='2018-11-12_click-article-title.png' style='width:100%;'/>\n\nLet's use the most recent issue as a toy example:\n\n::: {.cell}\n\n```{.r .cell-code}\n\"https://journals.sagepub.com/toc/hthb/23/4\" %>%\n  read_html() %>%\n  html_nodes(\"[data-item-name=click-article-title]\") %>%\n  html_attrs() %>%\n  .[[1]]\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n                      data-item-name                                class \n               \"click-article-title\"                         \"ref nowrap\" \n                                href \n\"/doi/full/10.1177/1758998318784316\" \n```\n:::\n:::\n\n\nThe DOI for the article is *almost* the `href` value, there's some fluff we'll get rid of later. But we know enough we can create a function for extracting the `href` value \n\n::: {.cell}\n\n```{.r .cell-code}\nget_article_dois_from_issue <- function(issue_url) {\n  \n  issue_page <- tryCatch(issue_url %>%\n    read_html(),\n  error = function(c) NA\n  )\n\n  if (is.na(issue_page)) {\n    return(NA)\n  }\n\n  issue_url %>%\n    read_html() %>%\n    html_nodes(\"[data-item-name=click-article-title]\") %>%\n    html_attr(\"href\")\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n\"https://journals.sagepub.com/toc/hthb/23/4\" %>%\n  get_article_dois_from_issue()\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] \"/doi/full/10.1177/1758998318784316\"\n\n[[2]]\n[1] \"/doi/full/10.1177/1758998318796010\"\n\n[[3]]\n[1] \"/doi/full/10.1177/1758998318798668\"\n\n[[4]]\n[1] \"/doi/full/10.1177/1758998318809574\"\n```\n:::\n:::\n\n\n### Scrape all the pages\n\nThe wonderful `purrr` package allows us to insert these (almost) DOIs into the rows of our `tibble()` as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nexample_dois <- issue_urls %>%\n  slice(52:54) %>%\n  mutate(doi = map(issue_url, function(x)get_article_dois_from_issue(x)))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nWarning: All elements of `...` must be named.\nDid you want `data = doi`?\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  volume issue issue_url                                  data            \n   <int> <dbl> <glue>                                     <list>          \n1     13     4 https://journals.sagepub.com/toc/hthb/13/4 <tibble [3 × 1]>\n2     14     1 https://journals.sagepub.com/toc/hthb/14/1 <tibble [5 × 1]>\n3     14     2 https://journals.sagepub.com/toc/hthb/14/2 <tibble [6 × 1]>\n```\n:::\n:::\n\nThe `unnest()` function from `tidyr` allows us to unpack these list columns\n\n::: {.cell}\n\n```{.r .cell-code}\nexample_dois %>%\n  unnest(doi)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 14 × 4\n   volume issue issue_url                                  doi                  \n    <int> <dbl> <glue>                                     <chr>                \n 1     13     4 https://journals.sagepub.com/toc/hthb/13/4 /doi/pdf/10.1177/175…\n 2     13     4 https://journals.sagepub.com/toc/hthb/13/4 /doi/pdf/10.1177/175…\n 3     13     4 https://journals.sagepub.com/toc/hthb/13/4 /doi/pdf/10.1177/175…\n 4     14     1 https://journals.sagepub.com/toc/hthb/14/1 /doi/full/10.1258/ht…\n 5     14     1 https://journals.sagepub.com/toc/hthb/14/1 /doi/full/10.1258/ht…\n 6     14     1 https://journals.sagepub.com/toc/hthb/14/1 /doi/full/10.1258/ht…\n 7     14     1 https://journals.sagepub.com/toc/hthb/14/1 /doi/full/10.1258/ht…\n 8     14     1 https://journals.sagepub.com/toc/hthb/14/1 /doi/full/10.1258/ht…\n 9     14     2 https://journals.sagepub.com/toc/hthb/14/2 /doi/full/10.1258/ht…\n10     14     2 https://journals.sagepub.com/toc/hthb/14/2 /doi/full/10.1258/ht…\n11     14     2 https://journals.sagepub.com/toc/hthb/14/2 /doi/full/10.1258/ht…\n12     14     2 https://journals.sagepub.com/toc/hthb/14/2 /doi/full/10.1258/ht…\n13     14     2 https://journals.sagepub.com/toc/hthb/14/2 /doi/full/10.1258/ht…\n14     14     2 https://journals.sagepub.com/toc/hthb/14/2 /doi/full/10.1258/ht…\n```\n:::\n:::\n\nAll DOI begin with `10.`[^1] which we can use to tidy up these *almost* DOI into real DOI:\n\n::: {.cell}\n\n```{.r .cell-code}\nexample_dois %>%\n  unnest(doi) %>%\n  mutate(doi = str_replace(doi, \".*/10.\", \"http://doi.org/10.\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 14 × 4\n   volume issue issue_url                                  doi                  \n    <int> <dbl> <glue>                                     <chr>                \n 1     13     4 https://journals.sagepub.com/toc/hthb/13/4 http://doi.org/10.11…\n 2     13     4 https://journals.sagepub.com/toc/hthb/13/4 http://doi.org/10.11…\n 3     13     4 https://journals.sagepub.com/toc/hthb/13/4 http://doi.org/10.11…\n 4     14     1 https://journals.sagepub.com/toc/hthb/14/1 http://doi.org/10.12…\n 5     14     1 https://journals.sagepub.com/toc/hthb/14/1 http://doi.org/10.12…\n 6     14     1 https://journals.sagepub.com/toc/hthb/14/1 http://doi.org/10.12…\n 7     14     1 https://journals.sagepub.com/toc/hthb/14/1 http://doi.org/10.12…\n 8     14     1 https://journals.sagepub.com/toc/hthb/14/1 http://doi.org/10.12…\n 9     14     2 https://journals.sagepub.com/toc/hthb/14/2 http://doi.org/10.12…\n10     14     2 https://journals.sagepub.com/toc/hthb/14/2 http://doi.org/10.12…\n11     14     2 https://journals.sagepub.com/toc/hthb/14/2 http://doi.org/10.12…\n12     14     2 https://journals.sagepub.com/toc/hthb/14/2 http://doi.org/10.12…\n13     14     2 https://journals.sagepub.com/toc/hthb/14/2 http://doi.org/10.12…\n14     14     2 https://journals.sagepub.com/toc/hthb/14/2 http://doi.org/10.12…\n```\n:::\n:::\n\nIt's painfully slow going through all 92 issues in this fashion, thankfully it's fairly easy to run this in parallel with the `future` package.\n\n### Scrape all the pages with future\n\nTo begin our work with the `future` package we must tell it our plan to use multicore evaluation as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nplan(multiprocess)\n```\n:::\n\nWe use `future()` to tell the `future` package to run according to the plan we just set:\n\n::: {.cell}\n\n```{.r .cell-code}\nstart_scrape <- Sys.time()\nscraped_dois <- issue_urls %>%\n  mutate(dois = map(issue_url, ~future(get_article_dois_from_issue(.x))))\nend_scrape <- Sys.time()\n```\n:::\n\nThis whole process hasn't taken much time (at the time of writing):\n\n::: {.cell}\n\n```{.r .cell-code}\nend_scrape - start_scrape\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 1.755138 mins\n```\n:::\n:::\n\nBut our dataset isn't ready to work with yet, our list column is full of `MulticoreFuture` things:\n\n::: {.cell}\n\n```{.r .cell-code}\nscraped_dois\n```\n:::\n\n<img src='2018-11-12_futured-dois.png'/>  \n\nWe use the `value()` function to extract the value of our future calculations and `unnest()` as previously:\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nscraped_dois %>%\n  mutate(dois = map(dois, ~value(.x))) %>%\n  unnest(dois) %>%\n  filter(!is.na(dois)) %>%\n  rename(doi = dois) %>%\n  mutate(doi = str_replace(doi, \".*/10.\", \"http://doi.org/10.\")) %>%\n  select(-issue_url)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 459 × 3\n   volume issue doi                                      \n    <int> <dbl> <chr>                                    \n 1      1     1 http://doi.org/10.1177/175899839600100401\n 2      1     1 http://doi.org/10.1177/175899839600100402\n 3      1     1 http://doi.org/10.1177/175899839600100403\n 4      1     1 http://doi.org/10.1177/175899839600100404\n 5      1     1 http://doi.org/10.1177/175899839600100405\n 6      1     1 http://doi.org/10.1177/175899839600100406\n 7      1     1 http://doi.org/10.1177/175899839600100407\n 8      1     1 http://doi.org/10.1177/175899839600100408\n 9      1     1 http://doi.org/10.1177/175899839600100409\n10      1     1 http://doi.org/10.1177/175899839600100410\n# … with 449 more rows\n```\n:::\n:::\n\nThis was the final output my friend needed - all 459 DOI-issued articles from the journal. It was really easy to put all of this together and finally get a chance to use the `future` package properly. If I was asked how to make this more rigorous I'd recommend the following:\n\n- Programmatically discover the most recent volume and issue\n- Don't assume a max of 4 issues per volume, allow the code to iterate through a volume.\n\n[^1]: See the DOI Handbook [DOI: 10.1000/182](http://doi.org/10.1000/182).",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}