[
  {
    "objectID": "posts/softblock-demo/index.html",
    "href": "posts/softblock-demo/index.html",
    "title": "Stub",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hadley2022,\n  author = {Charlotte Hadley},\n  title = {Stub},\n  date = {2022-05-10},\n  url = {https://visibledata.co.uk/posts/softblock-demo},\n  langid = {en},\n  abstract = {Long stubb!.}\n}\nFor attribution, please cite this work as:\nCharlotte Hadley. 2022. “Stub.” May 10, 2022. https://visibledata.co.uk/posts/softblock-demo."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visible Data",
    "section": "",
    "text": "We’re a UK-based data science consultancy and training organisation run by Charlotte Hadley. We specialise in the R language and reproducible research methods.\nWe have over a decade’s experience in building training courses and syllabi with a focus on students without a traditional STEM or programming background.\nPlease get in touch if you’re interested in working with us by email - charlie@visibledata.co.uk\n\n\n\n\n\n\n\n\n\n\n\n    \n        \n            \n                \n                R in 3 Months is an incredible opportunity to learn R with weekly personalised video feedback.\n            \n            \n        \n    \n    \n        \n            \n                \n                We've created 5+ courses @ LinkedIn Learning on building interactive dataviz & shiny apps with R.\n            \n            \n        \n    \n    \n        \n            \n                \n                We're extremely proud of this $99 course on building static and interactive maps with R."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Charlie Hadley and Visible Data is my data science and training consultancy. My handle elsewhere is almost universally @charliejhadley.\nI’m an R instructor and data science consultant. This website is as a portfolio of my courses, charts and other things that don’t have a good home elsewhere. I’m also an Open Data and Reproducible Research evangelist.\nAll tech is political. Our common future and freedom requires intersectionality.\nI really enjoy talking and delivering workshops about dataviz, R and reproducible research - please do get in touch charlie@visibledata.co.uk. The timeline below gives more information about my career changes and a selection of the workshops and talks I’ve given."
  },
  {
    "objectID": "about.html#more",
    "href": "about.html#more",
    "title": "About",
    "section": "More",
    "text": "More"
  },
  {
    "objectID": "about.html#can-i-make-a-website-like-this",
    "href": "about.html#can-i-make-a-website-like-this",
    "title": "About",
    "section": "Can I make a website like this?",
    "text": "Can I make a website like this?\nYup! I’m indebted to Drew Dimmery who makes the source of his Quarto website available here - https://github.com/ddimmery/quarto-website"
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Training",
    "section": "",
    "text": "We have a wide variety of on-demand training courses available from several online learning providers and also provide bespoke training courses both in-person and online.\nIn addition we deliver regular public courses through JB International and the Royal Statistical Society. I’m also a visiting lecturer at Birmingham City University."
  },
  {
    "objectID": "training.html#r-in-3-months",
    "href": "training.html#r-in-3-months",
    "title": "Training",
    "section": "R in 3 Months",
    "text": "R in 3 Months\n\n\n\n\n\n\nI’m particularly proud of my work as a co-instructor with David Keyes on the excellent R in 3 Months course that runs twice a year in the Spring and Fall.\nThe course includes weekly assignments where students use their own datasets and I record personalised feedback videos for each student. Over the course of the 12 weeks students build up a solid understanding of R and how to use RMarkdown to create rich and easy to read reports.\nThe course is $999 or $399 for those in countries with lower GDPs."
  },
  {
    "objectID": "training.html#on-demand-training",
    "href": "training.html#on-demand-training",
    "title": "Training",
    "section": "On-demand Training",
    "text": "On-demand Training\n\nLinkedIn Learning\nI’ve been recording training courses for LinkedIn Learning since 2016. Access to these courses is available through a LinkedIn Premium subscription or can be purchased a la carte. Many Universities provide students and staff with free access to LinkedIn Learning - check with your IT department.\n\n\n\n        \n            \n                \n                Go from zero to well designed shiny apps in 2h50m with a thorough, easy to follow explanation of reactivity.\n            \n            \n        \n\n\n        \n            \n                \n                A thorough introduction to tidyverse concerpts and how to use them to solve common data wrangling tasks.\n            \n            \n        \n\n\n        \n            \n                \n                Learn how to make interactive charts, tables, maps and more in this dedicated 5h30min course.\n            \n            \n        \n\n\n\n        \n            \n                \n                Learn to use RMarkdown to build reports and slide decks in both PDF and interactive HTML.\n            \n            \n        \n\n\n        \n            \n                \n                Learn how to combine {shiny} and RMarkdown together to create slide decks with interactive elements.\n            \n            \n        \n\n\n        \n            \n                \n                Need to make maps but don't know any GIS? After this course you can make static and interactive maps!\n            \n            \n        \n\n\n\n\n\n\nMapping with R ($99)\n\n\n\n\n\nI’ve designed this course specifically for folks who’ve never built a map or worked with GIS datasets before. The course introduces you to the excellent {sf} package and how to use it alongside the tidyverse for wrangling and exploring spatial data.\nYou’ll then learn how to create static maps with {ggplot2} and interactive maps with {leaflet}. I’ll also ensure you go away with sufficiency knowledge to work with projections, CRS and where to obtain shapefiles.\nThis course is available through the excellent R for the Rest of Us course catalogue."
  },
  {
    "objectID": "training.html#public-courses",
    "href": "training.html#public-courses",
    "title": "Training",
    "section": "Public courses",
    "text": "Public courses\n\n\nI partner with JB International to deliver monthly scheduled public R courses.  JB International have 20+ years experience in hosting and organising corporate training courses - but we also often see academics in these public courses too. Public courses are an affordable choice and an excellent opportunity to learn alongside other folks. If you’d prefer a bespoke course, please see below.\nI also deliver 3 courses a year through the Royal Statistical Society.  These courses are attended equally by both academics and folks from industry. This happens to be the most cost effective way to attend our courses, for non-RSS members attendance is £693.60 + VAT."
  },
  {
    "objectID": "training.html#custom-training-courses",
    "href": "training.html#custom-training-courses",
    "title": "Training",
    "section": "Custom training courses",
    "text": "Custom training courses\n\nOur training courses focus on using the R language for data science, reporting or data visualisation. Our courses are delivered using RMardown lecture slides for foundational material and use live coding exercises to replicate the real-world experience of using R.We encourage learners to tell us:\n“we need that chart/dashboard to do X”\nor\n“that’s not good enough for our needs - we need Y”\nI’d recommend considering booking a bespoke course through our partner JB International. They’ve been working with both small and (very) large corporates since 1995 and are better suited to processing payments and booking requests than my small consultancy. I genuinely enjoy working with these folks - they’re very fair to me and to their customers.If you would prefer to get in touch directly with me, then please email me at charlie@visibledata.co.uk."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "{{< include research.md >}}"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Capital Circles\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2024\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nData Quest: Motorway Services UK\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2024\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nPros, Cons and Neutrals lists?\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nCurves and stones\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nBordering countries graph\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2024\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\n{highcharts} doesn’t like empty factors (and the .by argument is quite nice, actually)\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2024\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the archive: 30DayChartChallenge 2022 Pictogram\n\n\n\n\n\n\nData visualisation\n\n\n30DayChartChallenge 2022\n\n\n\n\n\n\n\n\n\nOct 21, 2024\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the archive: 30DayChartChallenge 2022 Waffles\n\n\n\n\n\n\nData visualisation\n\n\n30DayChartChallenge 2022\n\n\n\n\n\n\n\n\n\nOct 21, 2024\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nHow I learned to stop replicating everything from {xaringan} and love Quarto\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2022\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nCreating an R-Ladies quarto format\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2022\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nOverriding individual code chunk options\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2022\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nWriting verbatim SQL in pipe chains using {dbplyr}\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2022\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nShiny modules to reduce duplication in apps\n\n\n\n\n\n\nshiny\n\n\n\nHelp reduce the duplication of code in {shiny} apps through the use of modules\n\n\n\n\n\nJul 9, 2019\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nShiny modules for useful controls\n\n\n\n\n\n\nshiny\n\n\n\nIf you need to re-use combinations of controls in {shiny} apps (or across multiple app) then modules can help!\n\n\n\n\n\nMar 25, 2019\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nOne weird regex matches 97% of DOI…\n\n\n\n\n\n\nreproducible research\n\n\n\nThis one weird regex matches 97% of DOI…\n\n\n\n\n\nMar 13, 2019\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nWhere is the centre of University of Oxford?\n\n\n\n\n\n\ndataviz\n\n\nGIS\n\n\n\nUniversity of Oxford has over 800 years of history to it, but if you want to organise a meeting at the centre of campus… where do you meet?\n\n\n\n\n\nFeb 18, 2019\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nKeeping secrets in blogdown\n\n\n\n\n\n\n\n\nKeeping API keys and passwords secret in a blogdown blog using the static/ folder and the secret package.\n\n\n\n\n\nJan 2, 2019\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ggmap after July 2018\n\n\n\n\n\n\ndataviz\n\n\nR\n\n\n\n\n\n\n\n\n\nDec 5, 2018\n\n\nCharlie Hadley, Charlie Joey Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nChristmas Drinks ggplot2 Advent Calendar 2018\n\n\n\n\n\n\ndataviz\n\n\n\nThere are just about 24 Christmassy hot drinks available in the big UK cafes. I’ve made an advent calendae of them in ggplot2, read this post to find out how to do the same.\n\n\n\n\n\nNov 28, 2018\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nCrawling DOI from a SAGE\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2018\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nGreat circles with sf and leaflet\n\n\n\n\n\n\nGIS\n\n\n\nGreat circles are the shortest journeys between two points on a map, which can be easily computed and manipulated through the use of the excellent sf library, and visualised interactively with leaflet.\n\n\n\n\n\nFeb 28, 2018\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nCrawling with tidygraph\n\n\n\n\n\nIn this blogpost I use crawling the internal links between pages in the University of Oxford’s Research Data website as an excuse for finally getting to grips with tidygraph and ggraph\n\n\n\n\n\nFeb 27, 2018\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nRecipes for learning\n\n\n\n\n\n\ndataviz\n\n\n\nIn this blogpost I introduce one of my favourite datasets, a treasure trove of 57,000+ recipes scraped from recipe websites for a fascinating research paper (DOI: 10.1038/srep00196) comparing how different cuisines choose ingredients. I first used this dataset in May 2015 to learn how to use R and continue to use it to test my understanding of new techniques and analysis. \n\n\n\n\n\nFeb 18, 2018\n\n\nCharlie Hadley\n\n\n\n\n\n\n\n\n\n\n\n\nWhere’s the love for htmlwidgets?\n\n\n\n\n\n\ndataviz\n\n\n\nIf you’ve never heard of them, htmlwidgets are an amazing part of the work that RStudio (plus ramnathv and timelyportfolio) have undertaken to make R a beautiful tool for doing data science and communicating things about your data. Using leaflet, plotly, highcharter and more it’s possible to create rich interactive charts, maps and other visualisations directly from R without having to learn JavaScript\n\n\n\n\n\nFeb 7, 2018\n\n\nCharlie Hadley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html#experience-timeline",
    "href": "about.html#experience-timeline",
    "title": "About",
    "section": "Experience timeline",
    "text": "Experience timeline\n\n\n    \n        \n            \n                \n                \n                    \n                \n            \n        \n        \n            \n                \n                \n                    \n                    \n                        \n                            2022\n                        \n                        \n                            \n                                \n                                    \n                                    \n                                        Visiting Lecturer: Birmingham City University\n                                        In 2022 I'm debuting my MSc module ENG7218 Data Science for Healthcare Applications as a visiting lecturer at Birmingham City University.\n                                    \n                                \n                            \n                        \n                        \n                        \n                    \n                    \n                        \n                            2021\n                        \n                        \n                            \n                                \n                                    \n                                    \n                                        R in 3 Months\n                                        Since 2021 I've been co-instructor on the awesome R in 3 Months program, which runs twice a year in the Spring and Fall. Each intake has 50+ students who submit weekly assignments and participate in both synchronous and asynchronous weekly content. I'm primarily responsible for weekly feedback and it's the most fun I have teaching each year. I definitely recommend joining the course.\n                                    \n                                \n                            \n                        \n                    \n                                            \n                    \n                    \n                        \n                            2019\n                        \n                                                \n                            \n                                \n                                    \n                                    \n                                        Job change: Full-time self-employed at Visible Data Ltd\n                                        From mid 2019 I've been working as a full-time independent data science consultant and trainer through my consultancy Visible Data Ltd. I primarily partner with third-party training providers but please do get in touch if you would like custom courses directly. Since 2021 I've also been working on consultancy projects via R for the Rest of Us\n                                    \n                                \n                            \n                        \n                        \n                            \n                                \n                                    \n                                    \n                                        R-Ladies: Chicago\n                                        Talk about using R to create interactive data visualisations with htmlwidgets and {shiny}.\n                                    \n                                \n                            \n                             \n                                \n                                    \n                                    \n                                        DHoxSS 2019\n                                        Invited talk at DHoxSS about data visualisation to increase research impact.\n                                    \n                                \n                            \n                        \n                        \n                            \n                                \n                                    \n                                    \n                                        BristolR\n                                        Talk about building reproducible data visualisation workflows with R and {shiny}.\n                                    \n                                \n                            \n                             \n                                \n                                    \n                                    \n                                        rstudio::conf workshop\n                                        As part of my CPD I attended this workshop from RStudio designed to train folks in how to train others to use {shiny}.\n                                    \n                                \n                            \n                        \n                    \n                                            \n                                        \n                    \n                        \n                            2017\n                        \n                                                \n                             \n                                \n                                    \n                                    \n                                        Job Change: Service Delivery Manager at University of Oxford\n                                        After successfully managing the Live Data pilot project I was promoted to Service Delivery Manager for the newly instated Interactive Data Network. The pilot project only provided sufficienct resources for a small-scale service. I was responsible for growing the reputation of the data visualisation service and planning sufficienct instructure to develop an University-wide service.\n                                    \n                                \n                            \n                             \n                        \n                             \n                                \n                                    \n                                    \n                                        Conference Host: The Missing Link in Publishing: Interactive Data Viz\n                                        This conference was designed to launch the Interactive Data Network at University of Oxford. I was responsible for organising and hosting this one day conference, and also delivered a training workshop. The conference invited speakers from the FT, ONS and OPU to discuss how interactive data visualisation is used to engage their audiences and improve the comprehension of data-rich articles.\n                                    \n                                \n                            \n                                                \n                        \n                        \n                            \n                                \n                                    \n                                    \n                                        OSDC Conference Workshop\n                                        Invited workshop on using R and data visualisations to explore datasets.\n                                    \n                                \n                            \n                             \n                                \n                                    \n                                    \n                                        Royal Statistical Society Conference\n                                        Invited one day workshop on using R to build interactive data visualisations to tell stories.\n                                    \n                                \n                            \n                        \n                    \n                                            \n                                                                \n                        \n                            2016\n                        \n                        \n                            \n                                \n                                    \n                                    \n                                        IDCC Conference Talk\n                                        Submitted talk to the IDCC conference about my successful Live Data pilot project to launch a data visualisation service at University of Oxford.\n                                    \n                                \n                            \n                             \n                                \n                                    \n                                    \n                                        DHoxSS Workshop\n                                        Four day workshop introducing humanities researchers to using the Wolfram Language for analysing and visualising humanities data.\n                                    \n                                \n                            \n                        \n                        \n                                                \n                    \n                    \n                        \n                            2015\n                        \n                        \n                            \n                                \n                                    \n                                    \n                                        Job change: Research Support Engineer at University of Oxford\n                                        In 2015 I joined the Research Support team at University of Oxford as a Research Software Engineer. My daily duties involved supporting researchers in reproducible research methods. Early in my employment I was tasked with being project manager for the Live Data project with the aim of launching a data visualisation service for all researchers at Oxford. This project was successful and led to me being promoted to Service Delivery Manager in 2017.\n                                    \n                                \n                            \n                        \n                    \n                        \n                    \n                    \n                        \n                            2014\n                        \n                        \n                            \n                                \n                                    \n                                    \n                                        Visiting Lecturer: American University in Cairo\n                                        As part of a program to roll out teaching with Mathematica across the physical sciences I was invited as a visiting lecturer to the American University in Cairo to train lecturers in the Wolfram Language and the CBM pedagogical approach.\n                                    \n                                \n                            \n                        \n                    \n                                            \n                    \n                    \n                        \n                            2012\n                        \n                        \n                            \n                                \n                                    \n                                    \n                                        Job change: Technical Consultant at Wolfram Research\n                                        In 2012 I joined Wolfram Research as a pre-sales technical consultant. Initially I was primarily responsible for developing and delivering customer-personalised technical demonstrations of Mathematica, including at C-level meetings. I also revamped several of the commercial training courses, and taught on the CQF program. Towards the end of my employment I was a senior consultant working on-site to build products for customers."
  },
  {
    "objectID": "about.html#overall-timeline",
    "href": "about.html#overall-timeline",
    "title": "About",
    "section": "Overall timeline",
    "text": "Overall timeline\n\n\n                    \n                        \n                            \n                                2 June\n                                Event One\n                                It will be as simple as occidental in fact it will be Occidental Cambridge friend\n                                \n                                    Read more\n                                \n                            \n                        \n                        \n                            \n                                5 June\n                                Event Two\n                                Everyone realizes why a new common language one could refuse translators.\n                                \n                                    Read more\n                                \n                            \n                        \n                        \n                            \n                                7 June\n                                Event Three\n                                Everyone realizes why a new common language one could refuse translators.\n                                \n                                    Read more\n                                \n                            \n                        \n                        \n                            \n                                8 June\n                                Event Four\n                                Languages only differ in their pronunciation and their most common words.\n                                \n                                    Read more"
  },
  {
    "objectID": "about.html#testimonials",
    "href": "about.html#testimonials",
    "title": "About",
    "section": "Testimonials",
    "text": "Testimonials\n\n\n\nCharlie’s expertise of R made the training session I had absolutely brilliant! We covered a mass of material, without it seeming overwhelming.\n\nData Scientist, Insurance Industry\n\n\n\n\n\nCharlie’s enthusiasm & passion for all things R, are hard not to get caught up in. I will look forward to reading blogs by Charlie and hopefully attending more trainings in the future.\n\nBenjamin Sutton, Brit Insurance\n\n\n\n\n\n\n\nThank you so much Charlie! I’m really impressed with the quality of this course, and I’ll be referring back to certain videos/topics as they come up in my geospatial journey ;) Thanks again!\n\nData Scientist, Fishing Industry\n\n\n\n\n\nThank you for all the help! I feel like searching the internet for help with R is like looking for a very specific needle across a giant field of haystacks, and you both are the metal detectors that get us where we need to be more quickly.\n\nTechnical Director, Tropical Health\n\n\n\n\n\nI thank you for the last feedback, I loved it because now I can change the style with css, which I had no idea about. You are a sweetheart, because when I listen to your video I feel light and at peace!\n\nResearcher, Plant Science"
  },
  {
    "objectID": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html",
    "href": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html",
    "title": "Shiny modules for useful controls",
    "section": "",
    "text": "Shiny Modules are an “advanced” feature of Shiny apps that developers can use to reduce code duplication, simplify complex inter-relating controls, allow UI elements to be compartmentalised into R packages… and generally be a Shiny magician.\nMuch of the stuff written about modules gets lost in the weeds. We’re not going to do that here. We’re going to make the Shiny app I’ve embedded below (as an iframe), both with and without modules.\nThis app is slightly complicated, but deliberately so. I want to give you a real world example of using Shiny modules.\nWe’ll go through the following steps:"
  },
  {
    "objectID": "posts/2019-07-09-shiny-modules-to-reduce-duplication-in-apps/index.html",
    "href": "posts/2019-07-09-shiny-modules-to-reduce-duplication-in-apps/index.html",
    "title": "Shiny modules to reduce duplication in apps",
    "section": "",
    "text": "Shiny apps are awesome, with a bit of training you can build fairly impressive interactive web apps. But at some point, the subject of “shiny modules” will rear its head.\nWhen I started learning modules, I found most of the existing articles focus on the technical-side of things and don’t focus on the benefits of using modules - they can significantly simplify and improve your apps.\nI’m going to put together a few different tutorials on real-world Shiny apps and how modules can be used to improve them. As these tutorials develop I’ll link to them below.\nBut for now, we’re going to build this Shiny app:\n\n\n\n\n\n\n\n\nThe shiny app displays data from the WDI package\nEach “page” of the Shiny app details a different “development indicator” from the WDI package\nUsers select a country of interest from a pull-down menu\nThe chart, text and table underneath the pull-down menu all update when a country is selected\nThe charts, text and tables are the same on each page except for two variables; the selected country and the indicator detailed on that page.\n\n\n\n\nHow to use this tutorial\nIt’s often useful to skim read through a tutorial before attempting to run the code on your own machine. If you do want to follow along with the code, you will need to install the {usethis} package before starting.\nThis tutorial is split into\n\nShiny app without modules\nShiny app\n\n\n\nShiny app without modules\nThe module-free version of the Shiny app can be downloaded onto your machine by running this code:\n\nusethis::use_course(\"https://github.com/charliejhadley/training_shiny-module/raw/master/wdi_indicators_modules/01_without-modules.zip\")\n\nOnce the RStudio project has opened, let’s take a look at the structure of the ui.R file in the app:\n\ncountries_list &lt;- c(...)\n\nnavbarPage(\n  \"WDI\",\n  tabPanel(\n    \"Internet\",\n    fluidPage(\n      selectInput(\"internet_country\",\n                  choices = countries_list),\n      ...\n    )\n  ),\n  tabPanel(\n    \"Bank branches\",\n    fluidPage(\n      selectInput(\"bank_branches_country\",\n                  choices = countries_list),\n      ...\n    )\n  ),\n  ...\n)\n\nWe’re essentailly duplicating the same selectInput() in each of our tabPanel()s. If there were many controls being repeated we could make an argument for using modules from this file alone.\nLet’s take a look at the server.R file of this app:\n\nfunction(input, output, session){\n  \n  output$internet_timeline &lt;- renderPlot({\n    \n    wdi_data %&gt;%\n      gg_wdi_indicator_timeline(input$internet_country,\n                                ...)\n    \n  })\n  \n  output$internet_comparison_table &lt;- renderUI({\n    \n    ranking_table &lt;- wdi_data %&gt;%\n    filter(country == input$internet_country) %&gt;%\n    \n    ranking_table %&gt;%\n        datatable()\n        \n  })\n  \n  output$bank_branches_timeline &lt;- renderPlot({\n    \n    wdi_data %&gt;%\n      gg_wdi_indicator_timeline(input$bank_branches_country,\n                              ...)\n    \n  })\n  \n  output$bank_branches_comparison_table &lt;- renderUI({\n    \n    ranking_table &lt;- wdi_data %&gt;%\n    filter(country == input$bank_branches_country) %&gt;%\n    ...\n    \n    ranking_table %&gt;%\n        datatable(...)\n        \n  })\n  \n}\n\nThere’s a lot of duplication in this file. If we wanted to add a new tab about the number of secondary school students, we would have to add all of the following:\n\n## ui.R\ntabPanel(\n  \"Secondary schools\",\n  fluidPage(\n    selectInput(\"secondary_schools_country\",\n                choices = countries_list),\n    ...\n  )\n)\n## server.R\noutput$secondary_schools_timeline &lt;- renderPlot({\n  \n  wdi_data %&gt;%\n    gg_wdi_indicator_timeline(input$secondary_schools_country,\n                              ...)\n  \n})\n\noutput$secondary_schools_comparison_table &lt;- renderUI({\n  \n  ranking_table &lt;- wdi_data %&gt;%\n    filter(country == input$secondary_schools_country) %&gt;%\n    ...\n  \n  ranking_table %&gt;%\n    datatable(...)\n  \n})\n\nLet’s breakdown the advantages to re-writing this app to use modules.\n\nWhat would be the benefits of switching to use modules?\n\nWithout modules, if we wanted to change the look of the “comparison tables” we would need to do that X times - once for each output$*_comparison_table object. Modules therefore help reduce transcription or copy/paste errors.\nModules will allow us to change the\nModules will reduce script file length, making the code easier to read and understand\nCurrently, if we wanted to change\n\nIf we needed to add another tab to our\nEach time we add a new tab to our shiny app, we’ll need to create a new pair of render*() functions and corresponding inputs in the ui.R file.\nBy re-designing our app to use modules, we’ll benefit from the following:\n\nReduced script file lenght, improving readability\nSimpler feature updates, changing the module code will update all pages at once.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hadley2019,\n  author = {Hadley, Charlie},\n  title = {Shiny Modules to Reduce Duplication in Apps},\n  date = {2019-07-09},\n  url = {https://visibledata.co.uk/posts/2019-07-09-shiny-modules-to-reduce-duplication-in-apps},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHadley, Charlie. 2019. “Shiny Modules to Reduce Duplication in\nApps.” July 9, 2019. https://visibledata.co.uk/posts/2019-07-09-shiny-modules-to-reduce-duplication-in-apps."
  },
  {
    "objectID": "posts/2022-06-17_test/index.html",
    "href": "posts/2022-06-17_test/index.html",
    "title": "Test new post",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hadley2022,\n  author = {Charlotte Hadley},\n  title = {Test New Post},\n  date = {2022-06-17},\n  url = {https://visibledata.co.uk/posts/2022-06-17_test},\n  langid = {en},\n  abstract = {Long stubb!.}\n}\nFor attribution, please cite this work as:\nCharlotte Hadley. 2022. “Test New Post.” June 17, 2022. https://visibledata.co.uk/posts/2022-06-17_test."
  },
  {
    "objectID": "posts/2019-03-13_extracting-doi-from-text/index.html",
    "href": "posts/2019-03-13_extracting-doi-from-text/index.html",
    "title": "One weird regex matches 97% of DOI…",
    "section": "",
    "text": "At least once a month I’m tasked with something that involves working with DOI, but I’ve never bothered to write a function for extracting these from texts. Time to end that by putting together a tidyversesque add_doi() function!\n… wait, why?\nThe R for Data Science book has lots of excellent advice, including this snippet about reducing duplicated code by writing functions:\n\nSo, what are DOI? DOI (Digital Object Identifiers) are the gold standard for citations. They’re guaranteed to point directly to the resource you care about. But how can we reliably extracting DOI from, for example, the following references?\n\nlibrary(\"tidyverse\")\nexample_references &lt;- tibble(\n  text_citation = c('Gueorgi Kossinets and Duncan J. Watts, \"Origins of Homophily in an Evolving Social Network,\" American Journal of Sociology 115 (2009):414, accessed December 5, 2014, doi:10.1086/599247',\n                        'Morey, C. C., Cong, Y., Zheng, Y., Price, M., & Morey, R. D. (2015). The color-sharing bonus: Roles of perceptual organization and attentive processes in visual working memory. Archives of Scientific Psychology, 3, 18–29. https://doi.org/10.1037/arc0000014',\n                        'Barros, B., Read, T. & Verdejo, M. F. (2008) Virtual collaborative experimentation:\nan approach combining remote and local labs. IEEE Transactions on Education. 51 (2),\n242–250. Available from: doi:10.1109/TE.2007.908071'))\n\nCrossRef have a great blogpost about how to match DOIs using regular expressions where they recommend the following regex, which matches 97% of the 74.9 million DOI they tested. The majority of the ~500,000 not matched by this regex are from the bad old days of the early noughties, and outside of our interest.\n\ndoi_regex &lt;- \"10.\\\\d{4,9}/[-._;()/:a-z0-9A-Z]+\"\n\nUsing str_extract() from the tidyverse package stringr allows us to extract the DOIs from our reference:\n\nexample_references %&gt;%\n  mutate(doi = str_extract(text_citation, doi_regex))\n\n# A tibble: 3 × 2\n  text_citation                                                            doi  \n  &lt;chr&gt;                                                                    &lt;chr&gt;\n1 \"Gueorgi Kossinets and Duncan J. Watts, \\\"Origins of Homophily in an Ev… 10.1…\n2 \"Morey, C. C., Cong, Y., Zheng, Y., Price, M., & Morey, R. D. (2015). T… 10.1…\n3 \"Barros, B., Read, T. & Verdejo, M. F. (2008) Virtual collaborative exp… 10.1…\n\n\nThis can all be rolled together into a function; add_doi(). If you’re unfamiliar with !!, enquo and := that’s because I’m using tidyeval, find out more about tidyeval here.\n\nadd_doi &lt;- function(.data, citation_column, name = \"doi\") {\n  citation_column &lt;- enquo(citation_column)\n\n  if (name != \"n\" && name %in% colnames(.data)) {\n    rlang::abort(glue::glue(\"Column `{name}` already exists in the data\"))\n  }\n  \n  .data %&gt;%\n    mutate(!!name := str_extract(!!citation_column, \"10.\\\\d{4,9}/[-._;()/:a-z0-9A-Z]+\"))\n}\n\nBecause the function is written with tidyeval I can use naked column names just as a I would with dplyr::add_count():\n\nexample_references %&gt;%\n  add_doi(text_citation)\n\n# A tibble: 3 × 2\n  text_citation                                                            doi  \n  &lt;chr&gt;                                                                    &lt;chr&gt;\n1 \"Gueorgi Kossinets and Duncan J. Watts, \\\"Origins of Homophily in an Ev… 10.1…\n2 \"Morey, C. C., Cong, Y., Zheng, Y., Price, M., & Morey, R. D. (2015). T… 10.1…\n3 \"Barros, B., Read, T. & Verdejo, M. F. (2008) Virtual collaborative exp… 10.1…\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hadley2019,\n  author = {Hadley, Charlie},\n  title = {One Weird Regex Matches 97\\% of {DOI...}},\n  date = {2019-03-13},\n  url = {https://visibledata.co.uk/posts/2019-03-13_extracting-doi-from-text},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHadley, Charlie. 2019. “One Weird Regex Matches 97% of\nDOI...” March 13, 2019. https://visibledata.co.uk/posts/2019-03-13_extracting-doi-from-text."
  },
  {
    "objectID": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#how-to-follow-along",
    "href": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#how-to-follow-along",
    "title": "Shiny modules for useful controls",
    "section": "How to follow along",
    "text": "How to follow along\nTo follow along you are advised to create a new RStudio Project and create the files detailed below in turn."
  },
  {
    "objectID": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#whats-the-app",
    "href": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#whats-the-app",
    "title": "Shiny modules for useful controls",
    "section": "What’s the app?",
    "text": "What’s the app?\nThe app allows the viewer to interact with data from the Online Labour Index. This is a project undertaken by the University of Oxford’s Internet Institute to study the online labour market, and ultimately to provide an economic indicator for the gig economy. The data is deposited on Figshare, which means anyone can access (and importantly, cite) the data with the following DOI: 10.6084/m9.figshare.3761562."
  },
  {
    "objectID": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#data-import-wrangling-and-stuff",
    "href": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#data-import-wrangling-and-stuff",
    "title": "Shiny modules for useful controls",
    "section": "Data import, wrangling and stuff",
    "text": "Data import, wrangling and stuff\nLet’s completely ignore the details of accessing and wrangling the data. This is a tutorial on Shiny modules.\nCreate yourself a new script file called data-processing.R and insert this code. Please note that when this script is run it will download a dataset that in early 2019 is 37Mb in size, this will double every 3 years1.\n\n## This code should be inserted into your data-processing.R script\nfs_deposit_id &lt;- 3761562\ndeposit_details &lt;- fs_details(fs_deposit_id)\n\ndeposit_details &lt;- unlist(deposit_details$files)\ndeposit_details &lt;-\n  data.frame(split(deposit_details, names(deposit_details)), stringsAsFactors = F)\n\nimported_country_group_data &lt;- deposit_details %&gt;%\n  filter(str_detect(name, \"bcountrydata_\")) %&gt;%\n  pull(download_url) %&gt;%\n  read_csv() %&gt;%\n  mutate(timestamp = as_date(timestamp)) %&gt;%\n  rename(date = timestamp)\n\ngigs_by_country_group &lt;- imported_country_group_data %&gt;%\n  group_by(date, country_group) %&gt;%\n  summarise(jobs = sum(count)) %&gt;%\n  ungroup()\n\ngigs_by_occupation &lt;- imported_country_group_data %&gt;%\n  group_by(date, occupation) %&gt;%\n  summarise(count = sum(count)) %&gt;%\n  rename(jobs = count) %&gt;%\n  ungroup()\n\nPlease note, we don’t run this code yet. It’s going to be loaded in our server.R file."
  },
  {
    "objectID": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#server.r-skeleton",
    "href": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#server.r-skeleton",
    "title": "Shiny modules for useful controls",
    "section": "server.R skeleton",
    "text": "server.R skeleton\nHere’s the skeleton for our server.R file, go ahead and create this on your machine:\n\n## this is how your server.R file should look\nlibrary(\"shiny\")\nlibrary(\"tidyverse\")\nlibrary(\"rfigshare\")\nlibrary(\"lubridate\")\nlibrary(\"xts\")\nlibrary(\"ggsci\")\n\nsource(\"data-processing.R\", local = TRUE)\n\nfunction(input, output, session){\n  \n  \n}\n\nSome things to note about our file:\n\nWe’ve loaded all of the packages needed for the whole app, not just the data wrangling.\nWe’ve used source() to run the data wrangling script when the Shiny app loads\nOur shiny server function contains three arguments. Many Shiny tutorials ommit the session argument, but it’s integral for Shiny modules to work correctly.\n\nWe’re about to run our script file, but first register for a Figshare.com account so you can use the rfigshare package. Now run this script."
  },
  {
    "objectID": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#gg_ma_timeseries",
    "href": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#gg_ma_timeseries",
    "title": "Shiny modules for useful controls",
    "section": "gg_ma_timeseries",
    "text": "gg_ma_timeseries\nLet’s assume for a minute that the dataviz we’re creating here is important enough that we might want to use it in other places than just this Shiny app. The easiest way to support this is by placing the script for the dataviz in its own file.\nCreate a new file called gg_ma_timeseries.R and add the following code, we’ll walk through the potentially mysterious enquo part of the functions in two minutes.\n\n## this is how your gg_ma_timeseries.R file should look\ngg_ma_timeseries &lt;- function(.data, date, value, category) {\n  date &lt;- enquo(date)\n  \n  value &lt;- enquo(value)\n  \n  category &lt;- enquo(category)\n  \n  n_colours &lt;- .data %&gt;%\n    pull(!!category) %&gt;%\n    unique() %&gt;%\n    length()\n  \n  colours_from_startrek &lt;- colorRampPalette(pal_startrek(palette = c(\"uniform\"))(7))(n_colours)\n  \n  \n  .data %&gt;%\n    ggplot(aes(\n      x = !!date,\n      y = !!value,\n      color = !!category\n    )) +\n    geom_line() +\n    theme_bw() +\n    scale_color_manual(values = colours_from_startrek) +\n    scale_x_date(expand = c(0.01, 0.01)) +\n    scale_y_continuous(expand = c(0.01, 0)) +\n    labs(\n      title = \"Example dataviz of Online Labour Index data\",\n      subtitle = \"DOI:10.6084/m9.figshare.376156\")\n}\n\nma_job_count &lt;- function(.data, date, value, category, window_width){\n  \n  date &lt;- enquo(date)\n  \n  value &lt;- enquo(value)\n  \n  category &lt;- enquo(category)\n  \n  window_width &lt;- as.numeric(window_width)\n  \n  .data %&gt;%\n    group_by(!!category) %&gt;%\n    arrange(!!date) %&gt;%\n    mutate(!!value := rollmean(!!value,\n                           k = window_width,\n                           na.pad = TRUE,\n                           align = \"right\"\n    )) %&gt;%\n    filter(!is.na(!!value)) %&gt;%\n    ungroup()\n  \n}\n\n\ngg_ma_timeseries() is the function which creates our ggplot2 timeseries dataviz\nma_job_count() is a little utility function for smoothing the data with xts::rollmean().\n\nLet’s load this file in our server.R script, so the file now has all of the code in it from before. Run this entire script.\n\n## this is how your server.R file should look\nlibrary(\"shiny\")\nlibrary(\"tidyverse\")\nlibrary(\"rfigshare\")\nlibrary(\"lubridate\")\nlibrary(\"xts\")\nlibrary(\"ggsci\")\n\nsource(\"data-processing.R\", local = TRUE)\n\nsource(\"gg_ma_timeseries.R\", local = TRUE)\n\nfunction(input, output, session){\n  \n  \n}\n\nI want to prove to you that this works. Create a new script file scratch-pad.R where we can experiment and play. Add the following code, and then run the script.\n\n## this is how your scratch-pad.R file should look\ngigs_by_occupation %&gt;%\n  ma_job_count(date, jobs, occupation, 28) %&gt;%\n  gg_ma_timeseries(date, jobs, occupation)\n\n\n\n\n\n\n\n\nNotice how the column names jobs, occupation and date have been given to gg_ma_timeseries without quotation marks? This is exactly the same as how we use functions from dplyr, e.g. iris %&gt;% mutate(species = toupper(species)). These are what we call naked column names.\nThat’s why the functions use enquo() internally, it’s implementing non-standard evaluation with tidyeval. However, there’s zero need to understand this now (if you don’t want to). This is a Shiny module tutorial, so let’s move on to the ui.R file."
  },
  {
    "objectID": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#ui.r-skeleton",
    "href": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#ui.r-skeleton",
    "title": "Shiny modules for useful controls",
    "section": "ui.R skeleton",
    "text": "ui.R skeleton\nHere’s the skeleton for our ui.R file, go ahead and create this on your machine:\n\n## this is how your ui.R file should look\nlibrary(\"shiny\")\nlibrary(\"shinycustomloader\")\nshinyServer(navbarPage(\n  \"Shiny Modules\",\n  tabPanel(\n    \"By occupation\",\n    fluidPage()\n  ),\n  tabPanel(\n    \"By country\",\n    fluidPage()\n  )\n))"
  },
  {
    "objectID": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#shiny-app-without-modules",
    "href": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#shiny-app-without-modules",
    "title": "Shiny modules for useful controls",
    "section": "Shiny app without modules",
    "text": "Shiny app without modules\nA complete copy of this Shiny app that doesn’t use modules is available on Github here: https://github.com/charliejhadley/training_shiny-module/tree/master/gg_ma_timeseries/shiny-without-modules.\nThere’s a tremendous amount of duplication in the ui.R file to get the radioButtons() to look how I want them to. We need to use Shiny modules to simplify this, which we’ll do in the section below.\n\n## DO NOT COPY THIS CODE.\n## This is how the ui.R file looks without using modules.\nlibrary(\"shiny\")\nlibrary(\"shinycustomloader\")\n\nshinyServer(navbarPage(\n  \"Shiny without Modules\",\n  tabPanel(\n    \"By occupation\",\n    fluidPage(fluidRow(column(\n      radioButtons(\n        \"by_occupation_landing_rollmean_k\",\n        label = \"\",\n        choices = list(\n          \"Show daily value\" = 1,\n          \"Show 28-day moving average\" = 28\n        ),\n        selected = 28,\n        inline = TRUE\n      ),\n      width = 12\n    )),\n    withLoader(plotOutput(\"gg_gigs_by_occupation\")),\n    type = \"html\", loader = \"dnaspin\"\n    )\n  ),\n  tabPanel(\n    \"By country\",\n    fluidPage(fluidRow(column(\n      radioButtons(\n        \"by_country_group_landing_rollmean_k\",\n        label = \"\",\n        choices = list(\n          \"Show daily value\" = 1,\n          \"Show 28-day moving average\" = 28\n        ),\n        selected = 28,\n        inline = TRUE\n      ),\n      width = 12\n    )),\n    withLoader(plotOutput(\"gg_gigs_by_country_group\")),\n    type = \"html\", loader = \"dnaspin\"\n    )\n  )\n))\n\nThe server.R file is fairly tidy because we’ve already abstracted a lot of the app into functions:\n\n## DO NOT COPY THIS CODE.\n## This is how the server.R file looks without using modules.\nlibrary(\"shiny\")\nlibrary(\"tidyverse\")\nlibrary(\"rfigshare\")\nlibrary(\"lubridate\")\nlibrary(\"xts\")\nlibrary(\"ggsci\")\n\nsource(\"data-processing.R\", local = TRUE)\n\nsource(\"gg_ma_timeseries.R\", local = TRUE)\n\nfunction(input, output, session) {\n  output$gg_gigs_by_occupation &lt;- renderPlot({\n    \n    gigs_by_occupation %&gt;%\n      ma_job_count(date, jobs, occupation, input$by_occupation_landing_rollmean_k) %&gt;%\n      gg_ma_timeseries(date, jobs, occupation)\n    \n  })\n\n  output$gg_gigs_by_country_group &lt;- renderPlot({\n    \n    gigs_by_country_group %&gt;%\n      ma_job_count(date, jobs, country_group, input$by_country_group_landing_rollmean_k) %&gt;%\n      gg_ma_timeseries(date, jobs, country_group)\n    \n  })\n}"
  },
  {
    "objectID": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#introducing-the-shiny-module",
    "href": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#introducing-the-shiny-module",
    "title": "Shiny modules for useful controls",
    "section": "Introducing the Shiny Module",
    "text": "Introducing the Shiny Module\nThe Shiny Module is composed of two components:\n\ngg_ma_timeseries_input() creates an instance of our controls.\ngg_ma_timeseries_output() creates an instance of the chart, which is dependent on a set of controls (specified in the server.R file).\n\n\n\n\nIt’s crucial that we place the code for these elements of the module in the correct place:\n\n\n\n\nRules for modules\nThis is one of the things left out of the other Shiny module tutorials I’ve read. Most tutorials deal with a toy example and don’t split the code into distinct ui.R and server.R files. Let’s establish some hard and fast rules about modules:\n\nPlace your module code in a subfolder called /modules.\nCode for inputs (controls) must be sourced in the ui.R file.\nCode for outputs (charts) must be sourced in the server.R file."
  },
  {
    "objectID": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#namespaces-with-ns",
    "href": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#namespaces-with-ns",
    "title": "Shiny modules for useful controls",
    "section": "Namespaces with NS",
    "text": "Namespaces with NS\nRemember our server.R function?\n\nserver &lt;- function(input, output, session){\n  \n}\n\nInside of our modules we need to manipulate values from both the input and output objects. In programming speak, we need to be careful about namespaces. In Shiny, this means we need to use the NS() function to guarantee we’re refering to the values we think we’re refering to. Let’s create the control for our modules, in a new script file: /modules/client-side_gg-ma-timeseries.R.\n\n## This is how your /modules/client-side_gg-ma-timeseries.R file should look\ngg_ma_timeseries_input &lt;- function(id) {\n  ns &lt;- NS(id)\n  tagList(\n    \"This entire tab is a shiny module, including; this text, the radio buttons and the chart.\",\n    fluidRow(column(\n      radioButtons(\n        ns(\"landing_rollmean_k\"),\n        label = \"\",\n        choices = list(\n          \"Show daily value\" = 1,\n          \"Show 28-day moving average\" = 28\n        ),\n        selected = 28,\n        inline = TRUE\n      ),\n      width = 12\n    ))\n  )\n}\n\nThings to take away from the gg_ma_timeseries_input() function:\n\nns &lt;- NS(id) ensures we’re refering to values from the server input object\nns(\"landing_rollmean_k\") translates to input$landing_rollmean_k\nIf returning multiple ui elements they must be contained within tagList()… but it’s a great idea to use tagList() by default, it will prevent mishaps.\n\nThat’s our control. We need to create our output function that will display the chart rendered by the server, which we’ll call gg_ma_timeseries_output(). It takes a while to download and display the data in our chart, so it’s a good idea to use the excellent [shinycustomloader] package to show an animated “loading” GIF to the user. Please update your /modules/client-side_gg-ma-timeseries.R file so it looks like this:\n\n## This is how your /modules/client-side_gg-ma-timeseries.R file should look\nlibrary(\"shinycustomloader\")\n\ngg_ma_timeseries_input &lt;- function(id) {\n  ns &lt;- NS(id)\n  tagList(\n    \"This entire tab is a shiny module, including; this text, the radio buttons and the chart.\",\n    fluidRow(column(\n      radioButtons(\n        ns(\"landing_rollmean_k\"),\n        label = \"\",\n        choices = list(\n          \"Show daily value\" = 1,\n          \"Show 28-day moving average\" = 28\n        ),\n        selected = 28,\n        inline = TRUE\n      ),\n      width = 12\n    ))\n  )\n}\n\ngg_ma_timeseries_output &lt;- function(id) {\n  ns &lt;- NS(id)\n  withLoader(plotOutput(ns(\"ma_plot\")), type = \"html\", loader = \"dnaspin\")\n}\n\nOur output function is more complex than our input, because ns(\"ma_plot\") refers to an instace of output$ma_plot from the Shiny module, which we haven’t created yet\nThe trick to how NS() works inside of gg_ma_timeseries_output() is more complicated than our input function, and is explained in the section below. However, let’s\n\n## This is how your ui.R file should look\nlibrary(\"shiny\")\n\nsource(\"modules/client-side_gg-ma-timeseries.R\", local = TRUE)\n\nshinyServer(navbarPage(\n  \"Shiny Modules\",\n  tabPanel(\n    \"By occupation\",\n    fluidPage(\n      gg_ma_timeseries_input(\"occupation_controls\"),\n      gg_ma_timeseries_output(\"occupation_chart\")\n    )\n  ),\n  tabPanel(\n    \"By country\",\n    fluidPage(\n      gg_ma_timeseries_input(\"by_country_controls\"),\n      gg_ma_timeseries_output(\"by_country_chart\")\n    )\n  )\n))\n\n\nWe’re going to be extracting\nns &lt;- NS(id) ensures we’re\nns(\"ma_plot\")"
  },
  {
    "objectID": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#things-they-dont-tell-you",
    "href": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#things-they-dont-tell-you",
    "title": "Shiny modules for useful controls",
    "section": "Things they don’t tell you",
    "text": "Things they don’t tell you\n\nLoad the ui in the ui.R file\nDelete the scratch-pad.R file"
  },
  {
    "objectID": "posts/child-doc/index.html",
    "href": "posts/child-doc/index.html",
    "title": "I have children!",
    "section": "",
    "text": "Code\n2 + 2\n\n\n[1] 4\n\n\n\n\n\n\nFootnotes\n\n\nThis is the thing↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hadley2022,\n  author = {Charlotte Hadley},\n  title = {I Have Children!},\n  date = {2022-05-10},\n  url = {https://visibledata.co.uk/posts/child-doc},\n  langid = {en},\n  abstract = {Long stubb!.}\n}\nFor attribution, please cite this work as:\nCharlotte Hadley. 2022. “I Have Children!” May 10, 2022. https://visibledata.co.uk/posts/child-doc."
  },
  {
    "objectID": "posts/child-doc/blogpost-about-bees.html",
    "href": "posts/child-doc/blogpost-about-bees.html",
    "title": "Visible Data",
    "section": "",
    "text": "2 + 2\n\n[1] 4\n\n\n\n\n\n\nFootnotes\n\n\nThis is the thing↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hadley,\n  author = {Charlotte Hadley},\n  url = {https://visibledata.co.uk/posts/child-doc/blogpost-about-bees.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCharlotte Hadley. n.d. https://visibledata.co.uk/posts/child-doc/blogpost-about-bees.html."
  },
  {
    "objectID": "posts/2018-12-05-using-ggmap-after-july-2018/index.html",
    "href": "posts/2018-12-05-using-ggmap-after-july-2018/index.html",
    "title": "Using ggmap after July 2018",
    "section": "",
    "text": "ggmap is an awesome package for creating maps with ggplot2. If you’ve seen a nice looking map built with ggplot2 I can almost guarantee you that it uses ggmap to look that good. The ggmap package is used to obtain base maps from the APIs of a number of online mapping tools, the one we care about here is the base maps from Google Maps (because they’re very good).\nIf you’re reading instructions for using ggmap written before July 2018 you will fail to replicate the results unless you make some changes to your workflow. You will need to give Google your billing information, but can create maps for free."
  },
  {
    "objectID": "posts/2018-12-05-using-ggmap-after-july-2018/index.html#whats-changed",
    "href": "posts/2018-12-05-using-ggmap-after-july-2018/index.html#whats-changed",
    "title": "Using ggmap after July 2018",
    "section": "What’s changed?",
    "text": "What’s changed?\nBefore July 2018 it was possible to use the Google Maps API for free without configuration or use account, this is now no longer possible. See the pricing page for explicit details from Google themselves.\nYou must do the following to obtain base maps from Google:\n\nHave a Google Cloud account.\nEnable the Google Maps Platform.\nProvide Google with your billing details.\n\nYou will then be allocated $200 of free usage per month, every month. All API calls will be made against these limits.\nIf you’re exclusively making static maps with ggmap then you’re extremely unlikely to ever be charged by Google. I’ve summarised the pricing policy as of December 2018 below:"
  },
  {
    "objectID": "posts/2018-12-05-using-ggmap-after-july-2018/index.html#lets-make-our-maps",
    "href": "posts/2018-12-05-using-ggmap-after-july-2018/index.html#lets-make-our-maps",
    "title": "Using ggmap after July 2018",
    "section": "Let’s make our maps",
    "text": "Let’s make our maps\nWe’ll break this into two stages:\n\nPreparing our Google Maps Platform account\nSetting up and using ggmap\n\n\nPreparing our Google Maps Platform account\n\nNavigate to https://cloud.google.com/maps-platform/ and click “Get Started”\nSelect “Maps” in the dialog, leave the other things for another tutorial.\n\n\n\nCreate a new project for tracking your ggmap-incurred expenses, I suggest using the name ggmap\n\n\n\nYou’ll be prompted to create a billing account, click “create a billing account”\nSelect your country, agree to the terms and conditions, and make a decision about marketing. Please note you’ll likely be told you’re getting $300 per month rather than $200 for 12 months, it’s because Google are trying to keep devs happy during the roll out.\n\n\n\nFill in your payment details!\nAs of 2018 there’s a modal dialog that you want to click through, I assume this will be designed away in the future so don’t worry if you don’t see it.\n\n\n\nSuccess! Now you have an API key to use in R. You’ll want to copy it down somewhere.\n\n\nYou need to have your API key with you EVERY time that you use ggmap for the time being. Make sure you keep a note of it from above, or else you can find it with these steps:\n\nNavigate to https://console.cloud.google.com/apis/dashboard and select the project (I went with ggmap) you created from the pull down list:\n\n\n\nSelect credentials from the left-hand nav and you’ll be shown your API key\n\n\n\n\nSetting up and using ggmap\nThe instructions for these steps can be found in Github Issue #51.\n\nThere’s a dev version of the ggmap package that uses the new APIs, which we can install as follows:\n\n\ndevtools::install_github(\"dkahle/ggmap\", ref = \"tidyup\")\n\n\nYou must now restart R.\nLoad the library and provide your API key\n\n\nregister_google(key = \"My-FAKE-KEY\")\n\n\nNow we can get our base maps using get_googlemap()\n\n\nbase_map &lt;- get_googlemap(center = c(2.2945, 48.858222), maptype = \"roadmap\")\nggmap(base_map)"
  },
  {
    "objectID": "posts/2019-02-18_oxford-centres/index.html",
    "href": "posts/2019-02-18_oxford-centres/index.html",
    "title": "Where is the centre of University of Oxford?",
    "section": "",
    "text": "My team at University of Oxford provide research data management and reproducible research advice to researchers across the whole of the University. A fairly common point of contention is where should we meet, and where is the centre of the University?\nWe broadly agreed that the centre of the University should be calculated as the (weighted) centre of mass of specific buildings or campuses within the University. Two of our favourite options were:\n\nCentre of mass of all colleges\nCentre of mass of the University divisions\n\nIn this blogpost I go through all the steps of scraping and combining data from the OxPoints service and Wikipedia to create the chart below:\n\n\nHow to follow along\nIf you want to follow along, please follow these steps first:\n\nCreate a new RStudio project\nCreate a data directory by running dir.create(\"data\")\nCreate a script file in the data directory called obtain-oxpoints-data.R, we’ll use this to scrape data from Oxpoint.\n\n\n\nGet data from OxPoints\nOxPoints is a tool for accessing geographic information about University of Oxford and was mostly put together by my friend and ex-colleague Alex Dutton. Now I’ve highlighted Alex is a friend, I can be slightly more candid in scraping data from the service…\nWe’re going to extract the shapefiles for all colleges in the university and store the data in ESRI shapefiles.\nWe need to load the following libraries at the top of our data-raw/obtain-oxpoints-data.R script:\n\nlibrary(\"jsonlite\")\nlibrary(\"tidyverse\")\nlibrary(\"sf\")\nlibrary(\"here\")\nlibrary(\"rvest\")\nlibrary(\"mapview\")\n\nThere’s not a fantastically well documented way to obtain college data from the API. After jigging about with the search fields the query below returns everything we need, and then we then subset the data (after much trial and error):\n\nraw_json_oxpoints_colleges <- read_json(\"https://maps.ox.ac.uk/api/places/search?type=%2Funiversity%2Fcollege&inoxford=true&-type_exact=%5C%2Funiversity%5C%2Fsub-library&-type_exact=%5C%2Funiversity%5C%2Froom&count=50&facet=type_exact\")\n\noxpoints_colleges <- raw_json_oxpoints_colleges$`_embedded`$pois\n\nThe oxpoints_colleges object contains data for all 38 colleges (at the time of writing). We’re going to iteratively extract the college data through this get_college_data() function.\n\nget_college_data <- function(oxpoints_data,\n                             college_index) {\n  college_properties <- names(oxpoints_data[[college_index]])\n\n  extracted_properties <-\n    college_properties[college_properties %in% c(\n      \"id\",\n      \"lat\",\n      \"lon\",\n      \"name\",\n      \"name_sort\",\n      \"shape\",\n      \"social_facebook\",\n      \"website\"\n    )]\n\n  oxpoints_data[[college_index]] %>%\n    .[extracted_properties] %>%\n    as_tibble() %>%\n    mutate_if(is.list, funs(as.character(.))) %>%\n    mutate_all(funs(parse_guess(.)))\n}\n\nLet’s extract the first college to create an object we can iteratively fill:\n\ncollege_oxpoints_data <- oxpoints_colleges %>%\n  get_college_data(1)\n\nNow we can use pwalk to iterate over all other colleges:\n\ntibble(x = 2:38) %>%\n  pwalk(function(x){college_oxpoints_data <<- college_oxpoints_data %>%\n    bind_rows(get_college_data(oxpoints_colleges, x))})\n\nOur college_oxpoints_data object contains the geometry of each college in the shape column and two columns lat and long of unknown provenance - let’s drop those as well as name_sort and id.\n\ncollege_oxpoints_data <- college_oxpoints_data %>%\n  select(-lat, -lon, -id, -name_sort)\n\n\n\n# A tibble: 6 × 4\n  name                   shape                           social_facebook website\n  <chr>                  <chr>                           <chr>           <chr>  \n1 All Souls College      POLYGON ((-1.2529353 51.753888… https://www.fa… http:/…\n2 Balliol College        POLYGON ((-1.2584285 51.755520… <NA>            http:/…\n3 Brasenose College      POLYGON ((-1.2553351 51.752854… https://www.fa… http:/…\n4 Christ Church          POLYGON ((-1.2546005 51.751651… https://www.fa… http:/…\n5 Corpus Christi College MULTIPOLYGON (((-1.2532213 51.… https://www.fa… http:/…\n6 Exeter College         MULTIPOLYGON (((-1.2553241 51.… <NA>            http:/…\n\n\nLet’s augment this data with whatever we can easily collect from the Wikipedia page on the Oxford Colleges.\n\n\nExtract data from Wikipedia table\nThe Colleges of the University of Oxford Wikipedia page provides a useful summary of the colleges, including; year of foundation, financial assets and number of students. We’ve already loaded the rvest package at the top of our script file, which is the go-to tool for web scraping with R. First, let’s import the web page using read_html.\n\ncolleges_of_oxford <- read_html(\"https://en.wikipedia.org/wiki/Colleges_of_the_University_of_Oxford\")\n\nWe want to extract the tables from the web page, which we achieve using html_nodes(). Specifically, we only care about the 3rd table.\n\ncolleges_of_oxford <- colleges_of_oxford %>%\n  html_nodes(\"table\")\ncolleges_of_oxford <- colleges_of_oxford[[3]]\n\nThe rvest package does contain a useful function (html_table()) for parsing well-formed tables into tibbles. Unfortunately, our table is not very well formed at all. The first row spans the whole table and contains a glossary for abbreviations in the table, please don’t format your own tables like this. I’ve decided to get around this by extracting all rows from the table, which have the html tag <tr> and extract the contents of these nodes into character vectors using html_text().\n\ncolleges_of_oxford <- colleges_of_oxford %>%\n  html_nodes(\"tbody\") %>%\n  html_nodes(\"tr\") %>%\n  html_text()\n\nWe’ll throw away the top row by asking for parts 2:length(.) and also clean up and nasty trailing/leading white space with str_trim().\n\ncolleges_of_oxford <- colleges_of_oxford %>%\n  .[2:{length(.) - 1}] %>%\n  str_trim() \n\nNow we can convert our vector of characters into a tibble! In the original table, the newline character \\n separates pieces of data in each row. So we can use str_replace_all to replace any sequence of repeating \\n with a ; and then parse this as a table with read_delim(). Let’s also tidy up the column names before we forget.\n\ncolleges_of_oxford <- colleges_of_oxford %>%\n  str_replace_all(\"\\n{1,}\", \";\") %>% \n  tibble(text = .) %>% \n  separate(text,\n           into = c(\"name\", \"foundation.year\", \"sister.college\", \"total.assets\", \"financial.endowment\", \"undergraduates\", \"post.graduates\", \"visiting.students\", \"male.students\", \"female.students\", \"total.students\", \"assets.per.student\"),\n           sep = \";\")\n\nThere are several messy columns that we need to convert into numeric data:\n\ncolleges_of_oxford %>%\n  slice(8:9) %>%\n  select(name, foundation.year, total.assets, financial.endowment)\n\n# A tibble: 2 × 4\n  name                      foundation.year   total.assets    financial.endowme…\n  <chr>                     <chr>             <chr>           <chr>             \n1 Harris Manchester College 1786College: 1996 £40,301,000[27] £14,371,000[27]   \n2 Hertford College          1282College: 1740 £79,183,000[28] £60,552,000[28]   \n\n\nTurns out readr::parse_number() is clever enough to fix our column. It’s convenient to use mutate_at to modify many columns at once, having previously re-arranged columns so all the targets are together:\n\ncolleges_of_oxford <- colleges_of_oxford %>%\n  select(name, sister.college, everything()) %>% \n  select(name, sister.college, foundation.year, undergraduates, post.graduates, total.students) %>% \n  mutate(across(foundation.year:total.students, ~parse_number(.x)))\n\nWe’ve now got all of the information I wanted about the colleges, so let’s combine the tables with left_join(). But this isn’t a well formatted dataset for GIS analysis/visualisation. For that we need to use the sf package.\n\ncollege_data <- college_oxpoints_data %>%\n  left_join(colleges_of_oxford)\n\n\n\nCreating an sf object\nsf tibbles are the augmented tibbles of the excellent sf package. They contain both the necessary geometric data for computing & visualising GIS features, and additional non-geometric data about each feature (i.e. the founding year of the college).\nLet’s remove the shape column from college_data to provide us with the purely non-geometric data columns, and we’ll append the\nLet’s extract all the non-geometric data columns from college_data with select() and create a new tibble college_nongeometric_data that we’ll append the geometric data to below.\n\ncollege_nongeometric_data <- college_data %>%\n  select(-shape)\n\nIn my opinion, this is the one and only fiddly part of using sf. We need to create an sfc object, which is achieved with the st_as_sfc() function. However, this function needs a vector of the features and not a tibble; which is why in the code below we use .[[1]]. If you’re unfamiliar with this trick, please refer to Hadley Wickham’s pepper shaker tutorial.\n\ncollege_features <- college_data %>%\n  select(shape) %>%\n  .[[1]]\ncollege_geometries <- st_as_sfc(college_features)\n\nWe can now augment college_nongeometric_data with this sfc object via the st_geometry() function. Finally, we’ll create a copy of this with a sensible name: college_shapes\n\nst_geometry(college_nongeometric_data) <- college_geometries\ncollege_shapes <- college_nongeometric_data\n\nOur sf tibble currently suffers from two issues:\n\nThere’s no projection. We’ll assign the standard WGS84 projection with st_set_crs.\nThe geometry objects are currently POLYGON Z and MULTIPOLYGON Z with zero values for z. As all values are zero, they’re pointless and can be removed with st_zm.\n\n\ncollege_shapes <- college_shapes %>%\n  st_set_crs(4326) %>%\n  st_zm()\n\n\n\nUnderstanding our shapefiles with mapview\nBefore continuing forwards, we should ensure our shapefiles are what we assume they are: the grounds of each college in the University of Oxford. The mapview package is incredibly useful for exploring and understanding sf tibbles. If you’ve been following along precisely, the script file your in already loads mpaview!\n\nmp_colleges <- college_shapes %>%\n  mapview()\nmp_colleges\n\n\n\n\n\nHow does this help us? Well, we can see that the colleges of the University are mostly clustered together around the city centre and there are some outliers like Wolfson College. If you clicked around a little, you’d also notice that some colleges have multiple sites (they’re MULTIPOLYGONS in the data).\n\ncollege_shapes %>%\n  slice(which(st_geometry_type(college_shapes) == \"MULTIPOLYGON\")) %>%\n  mapview(zcol = \"name\", alpha.regions = 1)\n\n\n\n\n\nFortunately, for all five of these colleges the largest polygon is the actual college and all other polygons can be considered superfluous to the calculation of where the centre of Oxford is. So how do we go about extracting the largest polygon from MULTIPOLYGON items?\nEdzer Pebesma is the creator of sf and was kind enough to advise me on Twitter that there’s an unexported function largest_ring that returns the largest polygon. Let’s re-do our data cleaning from above, but add three steps:\n\nUse st_zm() and st_set_crs() to remove the Z coordinates and set the projection of the data\nUse st_cast() to convert all POLYGONs to MULTIPOLYGONs\nExtract the largest polygon with largest_ring()\n\n\ncollege_features <- college_data %>%\n  select(shape) %>%\n  .[[1]]\ncollege_geometries <- st_as_sfc(college_features) %>%\n  st_zm() %>%\n  st_set_crs(4326) %>%\n  st_cast(\"MULTIPOLYGON\") %>%\n  sf:::largest_ring()\n\nst_geometry(college_nongeometric_data) <- college_geometries\ncollege_shapes <- college_nongeometric_data\n\nNow let’s check out the colleges from before which had multiple sites, looks like we’ve been successful!\n\ncollege_shapes %>%\n  filter(name %in% c(\"Corpus Christi College\", \"Exeter College\", \"Magdalen College\", \"Merton College\", \"Pembroke College\")) %>%\n  mapview(zcol = \"name\")\n\n\n\n\n\nIt makes sense to export this now nicely formatted GIS data for use in the actual visualisation scripts. GeoJSON is my preferred GIS data format, and we can create that directly using write_sf()\n\ncollege_shapes %>%\n  write_sf(here(\"posts\", \"2019-02-18_oxford-centres\", \"shapefiles_oxford_colleges.json\"),\n           driver = \"GeoJSON\")\n\n\n\nVisualising college shapefiles with ggmap\nggmap is the best tool for obtaining static map tiles for inclusion in ggplot2 charts, most folks prefer the Google Maps tiles. Unfortunately, since July 2018 users need to provide Google with billing details in order to obtain map tiles. Read my blogpost on how to setup your Google Maps API Key before running the code below.\nAt the time of writing (January 2018), the version of ggmap on CRAN doesn’t support the new API changes. Run packageVersion(\"ggmap\") in your console, if the version is higher than 2.7 then you’re in luck! If not, you need to install the development version of the package as follows:\n\ndevtools::install_github(\"dkahle/ggmap\", ref = \"tidyup\")\n\nLet’s create a new script file for visualising the college shapes, oxford-college-shapes.R. We’ll need to load the following packages at the top of the script\n\nlibrary(\"tidyverse\")\nlibrary(\"here\")\nlibrary(\"ggmap\")\nlibrary(\"sf\")\nlibrary(\"glue\")\nlibrary(\"ggrepel\")\n\nOur GIS data was exported in the previous section, let’s import it ready for use in this script:\n\ncollege_shapes <- read_sf(here(\"posts\", \"2019-02-18_oxford-centres\", \"shapefiles_oxford_colleges.json\"))\n\nBefore creating our maps we need to register our Google Maps token using register_google():\n\nregister_google(key = \"My-FAKE-KEY\")\n\n\n\n\nget_googlemap() gets us a map from the Google Maps API but requires both a center and (integer) zoom level, I’ve fiddled around for a while and think the following work well for us.\n\nbase_map <- get_googlemap(center = c(-1.257411, 51.7601055), \n                          maptype = \"roadmap\",\n                          zoom = 14)\n\nIn order to create ggplot2 charts containing map tiles from ggmap we need to start with the ggmap() function and add layers with +\n\ngg_oxford_city <- ggmap(base_map)\ngg_oxford_city +\n  geom_sf(data = college_shapes, inherit.aes = FALSE, alpha = 0)\n\n\n\n\nThis is too busy a map to really communicate anything, it would be great if we could remove some of the labels and landmarks from the base map. To do that, we need to use the style argument of ggmap().\nThe Google Maps JavaScript API doesn’t make it easy to figure out how to manipulate multiple styles at once. This page advises, “do not combine multiple operations into a single style operation”. What’s that mean?!\n\nIf modifying just one style, we can give the style argument a vector\n\n\nget_googlemap(center = c(-1.257411, 51.7601055), \n                          maptype = \"roadmap\",\n                          zoom = 14,\n              style = c(feature = \"feature:poi.park\", visiblity = \"off\")) %>%\n  ggmap()\n\n\n\n\n\nIf modifying more than one style, we need to give a string containing multiple styles\n\n\nget_googlemap(center = c(-1.257411, 51.7601055), \n                          maptype = \"roadmap\",\n                          zoom = 14,\n              style = \"&style=feature:poi|visibility:off&style=feature:poi.park|visibility:on&style=feature:landscape.man_made|visibility:off\") %>%\n  ggmap()\n\n\n\n\nLet’s create a utility function for creating these style strings.\n\nmake_ggmap_styles <- function(styles){\n  styles %>%\n    mutate(style = glue(\"&style=feature:{feature}|visibility:{visibility}\")) %>%\n    select(style) %>%\n    .[[1]] %>%\n    paste0(collapse = \"\")\n}\n\nHere are the things we’d like to show/hide:\n\nmy_styles <- tribble(\n  ~feature, ~visibility,\n  \"poi\", \"off\",\n  \"poi.park\", \"on\",\n  \"landscape.man_made\", \"off\"\n) \n\nNow we have a good looking gg_oxford_city that we can play with:\n\nbase_map <- get_googlemap(center = c(-1.257411, 51.7601055), \n                          maptype = \"roadmap\",\n                          zoom = 14,\n                          style = my_styles %>%\n                            make_ggmap_styles())\ngg_oxford_city <- ggmap(base_map)\ngg_oxford_city\n\n\n\n\nIt’s really incredibly how flexible the ggplot2 extensions system is, we can label sf objects using ggrepel::goem_label_repel with barely any effort. The only effort I did expend was on choosing a good value for nudge_x.\n\ngg_oxford_city +\n  geom_sf(data = college_shapes, inherit.aes = FALSE,\n          aes(fill = foundation.year < 1300)) +\n  geom_label_repel(\n    data = college_shapes %>%\n      select(!where(is.list)) %>% \n                  filter(foundation.year < 1300),\n    aes(label = name, geometry = geometry),\n    stat = \"sf_coordinates\",\n    nudge_x = 2,\n    inherit.aes = FALSE\n  )\n\n\n\n\n\n\nAnimating the centre of mass over time\nNow we’ve finally got to the point that we can compute the centre of the colleges over time! Let’s create a new script file college-centres-over-time.R and load our libraries.\n\nlibrary(\"tidyverse\")\nlibrary(\"ggmap\")\nlibrary(\"sf\")\nlibrary(\"glue\")\nlibrary(\"ggrepel\")\n\nAgain, we need to import our GIS data:\n\ncollege_shapes <- read_sf(here(\"posts\", \"2019-02-18_oxford-centres\", \"shapefiles_oxford_colleges.json\"))\n\nWe also need to ensure that we register with the Google Maps API when running this script and set-up our base map again.\n\nmake_ggmap_styles <- function(styles){\n  styles %>%\n    mutate(style = glue(\"&style=feature:{feature}|visibility:{visibility}\")) %>%\n    select(style) %>%\n    .[[1]] %>%\n    paste0(collapse = \"\")\n}\nmy_styles <- tribble(\n  ~feature, ~visibility,\n  \"poi\", \"off\",\n  \"poi.park\", \"on\",\n  \"landscape.man_made\", \"off\"\n)\nbase_map <- get_googlemap(center = c(-1.257411, 51.7601055), \n                          maptype = \"roadmap\",\n                          zoom = 14,\n                          style = my_styles %>%\n                            make_ggmap_styles())\ngg_oxford_city <- ggmap(base_map)\n\nLet’s see how the centre of mass of the colleges changes each century. I’ve created century_cut() which wraps cut() to create prettified centuries.\n\ncentury_cut <- function(x, lower = 0, upper, by = 100,\n                   sep = \"-\", above.char = \"+\") {\n\n labs <- c(paste(seq(lower, upper - by, by = by),\n                 seq(lower + by - 1, upper - 1, by = by),\n                 sep = sep),\n           paste(upper, above.char, sep = \"\"))\n\n cut(floor(x), breaks = c(seq(lower, upper, by = by), Inf),\n     right = FALSE, labels = labs)\n}\n\n\ncolleges_by_century <- college_shapes %>%\n  group_by(century = century_cut(foundation.year, lower = 1200, upper = 2100, by = 100))\n\n## Messy code to display just the name and century of creation\ndisplay_colleges_by_century <- colleges_by_century %>%\n  select(name, century)\nst_geometry(display_colleges_by_century) <- NULL\ndisplay_colleges_by_century %>%\n  arrange(century)\n\n# A tibble: 38 × 2\n# Groups:   century [9]\n   name                century  \n   <chr>               <fct>    \n 1 Balliol College     1200-1299\n 2 Hertford College    1200-1299\n 3 Merton College      1200-1299\n 4 St Edmund Hall      1200-1299\n 5 University College  1200-1299\n 6 Exeter College      1300-1399\n 7 New College         1300-1399\n 8 Oriel College       1300-1399\n 9 The Queen's College 1300-1399\n10 All Souls College   1400-1499\n# … with 28 more rows\n\n\nNow we’ve done this we can calculate the centres for each century:\n\ncolleges_by_century %>% \n  group_by(century) %>%\n  summarise(geometry = st_union(geometry)) %>% \n  st_centroid()\n\n\ncentury_centres <- colleges_by_century %>% \n  group_by(century) %>%\n  summarise(geometry = st_union(geometry)) %>% \n  st_centroid()\n\ngg_century_centres <- gg_oxford_city +\n  geom_sf(data = colleges_by_century,\n          aes(fill = century), inherit.aes = FALSE) +\n  geom_sf(data = century_centres,\n          aes(fill = century,\n              color = \"black\"), inherit.aes = FALSE, shape = 21) +\n  geom_label_repel(\n    data = century_centres,\n    aes(label = century, fill = century, geometry = geometry),\n    stat = \"sf_coordinates\",\n    color = \"white\",\n    nudge_x = -1,\n    inherit.aes = FALSE,\n    size = 5,\n    fontface = 'bold',\n    show.legend = FALSE\n  )\ngg_century_centres <- gg_century_centres +\n  labs(title = \"Centre of Oxford colleges established in each century\") +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d()\ngg_century_centres\n\n\n\n\n\nNow we’ve established the centre of the colleges for each century, let’s compute the cumulative centres as time progresses. For simplicity, let’s throw away all of the data except for the longitude and latitude of the centre of mass in each century. The centre_over_time_period() function returns the centre of mass for all colleges established within the specific time period:\n\ncentre_over_time_period <- function(shapes, lower, upper){\n  # print(\"a\")\n  suppressWarnings(shapes %>%\n  filter(foundation.year >= lower & foundation.year < upper) %>%\n  st_centroid() %>%\n  st_combine() %>%\n  st_centroid() %>%\n  st_coordinates() %>%\n  as_tibble() %>%\n  rename(long = X,\n           lat = Y))\n}\n\ncentre_over_time_period(college_shapes,\n                        lower = 1500,\n                        upper = 1600)\n\n# A tibble: 1 × 2\n   long   lat\n  <dbl> <dbl>\n1 -1.26  51.8\n\n\nUsing pmap() we can calculate the cumulative college centres:\n\ntime_periods <- tibble(\n  lower = 1200,\n  upper = seq(1300, 2100, 100)\n)\n\ncentres_through_history <- time_periods %>%\n  mutate(centre = pmap(list(lower, upper), function(lower, upper) {\n  centre_over_time_period(college_shapes,\n  lower,\n  upper)\n  })) %>%\n  unnest(cols = c(centre)) %>%\n  st_as_sf(coords = c(\"long\", \"lat\")) %>%\n  st_set_crs(4326) %>%\n  mutate(time_period = glue(\"{lower} - {upper}\"))\n\nWe can visualise these cumulative centres using geom_label_repel(), just as above. But with carefully selected nudge factors:\n\ngg_centres_through_time <- gg_oxford_city +\n  # geom_sf(data = centres_through_history) +\n  geom_label_repel(\n    data = centres_through_history,\n    aes(label = time_period, fill = time_period, geometry = geometry),\n    color = \"white\",\n    segment.color = \"black\",\n    stat = \"sf_coordinates\",\n    min.segment.length = 0.2,\n    nudge_x = sample(c(-1,1), nrow(centres_through_history), replace = TRUE),\n    nudge_y = sample(c(0.007, -0.007), nrow(centres_through_history), replace = TRUE),\n    inherit.aes = FALSE,\n    seed = 5,\n    size = 5,\n    fontface = 'bold',\n    show.legend = FALSE\n  ) +\n  labs(title = \"Centres of University of Oxford through time\") +\n  scale_fill_viridis_d()\ngg_centres_through_time\n\n\nI’m slightly late to the gganimate bandwagon, but let’s animate this! The transition_states() function will create a GIF that transitions through the time_period column, and I’ve used shadow_wake() so that the centre of mass can be tracked more easily by the reader.\n\nlibrary(\"gganimate\")\nanim <- gg_oxford_city +\n  geom_sf(data = centres_through_history, inherit.aes = FALSE) +\n  transition_states(time_period,\n                    transition_length = 2,\n                    state_length = 1)\nanim + \n  enter_fade() + \n  shadow_wake(wake_length = 0.05) +\n  exit_shrink()\nanim_save(here(\"oxford-centres.gif\"))\n\n\nI’d really love to be happy with this GIF, but it’s really not good! The main issue is that ggamp only allows integer zoom levels, so the map is either far too zoomed in or out. It’s also quite slow to experiment with geom_sf at the moment, I’m quite excited for Thomas Pederson’s promise to speed up geom_sf.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hadley2019,\n  author = {Charlotte Hadley},\n  title = {Where Is the Centre of {University} of {Oxford?}},\n  date = {2019-02-18},\n  url = {https://visibledata.co.uk/posts/2019-02-18_oxford-centres},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCharlotte Hadley. 2019. “Where Is the Centre of University of\nOxford?” February 18, 2019. https://visibledata.co.uk/posts/2019-02-18_oxford-centres."
  },
  {
    "objectID": "posts/2019-01-02_keeping-secrets-in-blogdown/index.html",
    "href": "posts/2019-01-02_keeping-secrets-in-blogdown/index.html",
    "title": "Keeping secrets in blogdown",
    "section": "",
    "text": "This blog is generated using the awesome blogdown package, which means all this wonderful stuff is generated from RMarkdown documents that are available in the Github repository visibledata/visibledata.github.io that powers the website. It’s therefore extremely important I don’t accidentally include the key anywhere in my repository. But I want to consistently use API keys anywhere in the website.\nIf you want to do something similar, I recommend following these instructions:\n\nAdd the following lines to your .gitignore file\n\n\ndata/secret-keys.R\ndata/secret-vault.vault\n\n\nAdd these changes to a commit and push these to Github.\nDon’t proceed until you’ve done step 2. You need to protect these two files.\nYou’re going to need to create an SSH key, so either load up the terminal (-nix systems) or Command Prompt (Windows) and run the following code (I chose blog_vault as the name for my file).\n\n\nssh-keygen -t rsa\n# Enter file in which to save the key (/Users/charliejhadley/.ssh/id_rsa):\n\nNow we’re going to create the static/data/secret-keys.R file where we will iteratively add our tokens and other secrets using Gabor Csardi’s awesome secret package.\n\nlibrary(\"secret\")\nlibrary(\"here\")\n\n## ==== Create a vault\n## Run these lines ONCE ONLY\nmy_vault <- here(\"data\", \"secret-vault.vault\")\ncreate_vault(my_vault)\n\n## ==== Create a user\n## This uses the ssh-key we created above, run this code ONCE ONLY\nkey_dir = \"/Users/charliejhadley/.ssh\"\ncharliejhadley_public_key <- file.path(key_dir, \"blog_vault.pub\")\ncharliejhadley_private_key <- file.path(key_dir, \"blog_vault\")\nadd_user(\"charliejhadley\", charliejhadley_public_key, vault = my_vault)\n\nOnce you’ve run this code, I’d recommend you explicitly comment out everything except for the first two lines.\nNow we can add a secrets to the vault using add_secret().\n\nadd_secret(\"ggmaps_fake_key\", \"foobar\", user = \"charliejhadley\", vault = my_vault)\n\nadd_secret(\"ggmap_key_2020\", \"AIzaSyABTxwpq4Ds2rOgOtblyZeco_QKXJdPruo\", user = \"charliejhadley\", vault = my_vault)\n\nPersonally, I’m deleting secrets from the static/data/secret-keys.R after I add them. I’d recommend that you do the same. Remember that if you have not disabled your .RHistory then your keys will be available to ne’er-do-wells in a plain text file.\nNow I can use my keys safely in blogposts, as follows:\n\nlibrary(\"here\")\nlibrary(\"secret\")\nlibrary(\"tidyverse\")\nlibrary(\"ggmap\")\n\nmy_vault <- here(\"data\", \"secret-vault.vault\")\ncharliejhadley_private_key <- file.path(\"~/.ssh\", \"blog_vault\")\nggmaps_rstats_key <- get_secret(\"ggmap_key_2020\", key = charliejhadley_private_key, vault = my_vault)\n\nregister_google(key = ggmaps_rstats_key)\nbase_map <- get_googlemap(center = c(2.2945, 48.858222), maptype = \"roadmap\")\nggmap(base_map)\n\n\n\n\nI highly recommend Hadley Wickham’s httr vignette on managing secrets if you want to learn more.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hadley2019,\n  author = {Charlotte Hadley},\n  title = {Keeping Secrets in Blogdown},\n  date = {2019-01-02},\n  url = {https://visibledata.co.uk/posts/2019-01-02_keeping-secrets-in-blogdown},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCharlotte Hadley. 2019. “Keeping Secrets in Blogdown.”\nJanuary 2, 2019. https://visibledata.co.uk/posts/2019-01-02_keeping-secrets-in-blogdown."
  },
  {
    "objectID": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html",
    "href": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html",
    "title": "Christmas Drinks ggplot2 Advent Calendar 2018",
    "section": "",
    "text": "This Christmas it suddently hit me. There are probably enough festive coffees at Starbucks etc that you could use them to populate a very tasty advent calendar.\nIn the interests of helping everyone explore the range of Christmassy drinks on offer, I’ll be updating my deposit 24 days of Christmass drinks on Figshare every year from now on. This blogpost takes you through how you can create the advent calendar below using ggplot2, I’m planning on creating an extension to make this easier in future 1."
  },
  {
    "objectID": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html#christmassy-drinks-of-2018",
    "href": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html#christmassy-drinks-of-2018",
    "title": "Christmas Drinks ggplot2 Advent Calendar 2018",
    "section": "Christmassy drinks of 2018",
    "text": "Christmassy drinks of 2018\nBefore we can collate our advent of Christmassy drinks, we need to make some decisions:\n\nWhat counts as a Christmassy drink?\n\nAny drink that I can buy all the way through 1st December through to 24th December (so no Pumpkin Spice lattes, sadly).\n\nWhich cafes should be included?\n\nIt’s my first year doing this. I’m going to choose places that are easy for me to get to!\n\nDo hot chocolates count?\n\nI really wish they didn’t. But I could only find 15 festive coffee-based drinks within easy reach of me.\n\n\nAfter searching through a few menus, here are my selections for the Christmassy drinks of 2018:"
  },
  {
    "objectID": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html#make-your-own-ggplot2-advent",
    "href": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html#make-your-own-ggplot2-advent",
    "title": "Christmas Drinks ggplot2 Advent Calendar 2018",
    "section": "Make your own ggplot2 advent",
    "text": "Make your own ggplot2 advent\nIf you want to follow along, please follow these steps first:\n\nCreate a new RStudio project\nCreate a data directory by running dir.create(\"data\")\nCreate a new script christmassy-drinks-advent.R, at the top of the script load all the packages we’ll need:\n\n\nlibrary(\"tidyverse\") # data manipulation\nlibrary(\"grid\") # rasterise images for ggplot2\nlibrary(\"ggpubr\") # set background image for ggplot2 panel\nlibrary(\"magick\") # manipulate images and export a .gif\nlibrary(\"here\") # see https://github.com/jennybc/here_here#readme\n\n\nDownload version 3 of the Figshare deposit DOI:10.6084/m9.figshare.7376228.v3 by adding this code to your script:\n\n\ndir.create(\"data\")\ndownload.file(\"https://ndownloader.figshare.com/articles/7376228/versions/3\",\n              \"data/christmassy-drinks-2018.zip\")\nunzip(\"data/christmassy-drinks-2018.zip\",\n      exdir = \"data\")\nunlink(\"data/christmassy-drinks-2018.zip\")\n\n\nCan we tidy up things a bit? There are images in the data folder, which isn’t very good practice.\n\n\ndir.create(\"images\")\nretailer_logos <- list.files(\"data\", \"*.png|.jpg\", full.names = TRUE)\nfile.copy(retailer_logos, \"images\")\nfile.remove(retailer_logos)\n\n\nIf you follow along exactly with this blogpost, you’ll have 200+ lines of code in one script file! Make your life easier by splitting your script in sections as follows (read more here). Please note there’s lots of duplication when following along, it doesn’t really take 200+ lines of code to make this!\n\n\n# ---- download data ----\ndownload.file(\"...\") \n# ---- advent doors ----\nggplot(\"...\")\n# ---- advent treats ---- \nggplot(\"...\")"
  },
  {
    "objectID": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html#ggplot2-advent-calendar-doors",
    "href": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html#ggplot2-advent-calendar-doors",
    "title": "Christmas Drinks ggplot2 Advent Calendar 2018",
    "section": "ggplot2 advent calendar doors",
    "text": "ggplot2 advent calendar doors\nIn my opinion, there are only really two true advent calendar layouts:\n\n\n\n\n\nWe can easily compute the centers of all the doors in these advent calendar layouts with tibble() and seq():\n\nportrait_door_centers <- tibble(\n  x = rep(seq(1, 10, 3), 6),\n  y = rep(seq(1, 16, 3), times = 1, each = 4))\n\nlandscape_door_centers <- tibble(\n  y = rep(seq(1, 10, 3), 6),\n  x = rep(seq(1, 16, 3), times = 1, each = 4))\n\nThis blogpost will be slightly shorter if we continue with a landscape advent calendar, so let’s do that.\nWhich geom_*() is most suitable for our advent calendar? Well, geom_tile() requires only the centre of our doors. The theme theme_void() throws away the unnecessary axes labels etc, leaving us with our idealised advent calendar:\n\ngg_advent_landscape <- landscape_door_centers %>%\n  ggplot(aes(x, y)) +\n  geom_tile(color = \"black\",\n            size = 0.6,\n            linetype = \"dotted\",\n            alpha=0) +\n  theme_void() +\n  coord_fixed()\ngg_advent_landscape\n\n\n\n\nHow about the door labels? Let’s augment the data with door numbers via mutate() and then add geom_label() to display the door number:\n\nset.seed(1)\nlandscape_door_centers <- landscape_door_centers %>%\n  mutate(door_number = sample(1:24))\ngg_advent_landscape <- landscape_door_centers %>%\n  ggplot(aes(x, y)) +\n  geom_tile(color = \"black\",\n            size = 0.6,\n            linetype = \"dotted\",\n            alpha = 0) +\n  geom_label(aes(label = door_number)) +\n  theme_void() +\n  coord_fixed()\ngg_advent_landscape"
  },
  {
    "objectID": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html#ggplot2-advent-calendar-treats",
    "href": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html#ggplot2-advent-calendar-treats",
    "title": "Christmas Drinks ggplot2 Advent Calendar 2018",
    "section": "ggplot2 advent calendar treats",
    "text": "ggplot2 advent calendar treats\nNow we’ve created the doors we need to create the Christmassy drink treats! Let’s import the drinks data and re-order them via the door_number column:\n\n\n\n\nset.seed(1)\nchristmassy_drinks_2018 <- read_csv(here(\"posts\", \"2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018\", \"24-days-of-christmassy-drinks.csv\")) %>%\n  filter(year == 2018) %>%\n  slice(landscape_door_centers$door_number) %>%\n  mutate(door_number = 1:24)\n\nlandscape_door_treats <- christmassy_drinks_2018 %>%\n  left_join(landscape_door_centers)\n\nThe festive drinks have very long names (e.g. “Salted caramel brownie hot chocolate”), so let’s insert line breaks every 10 characters to prettify our labels:\n\nlandscape_door_treats <- landscape_door_treats %>%\n  mutate(drink.name = gsub(\"(.{10,}?)\\\\s\", \"\\\\1\\n\", drink.name))\n\ngg_landscape_door_treats <- landscape_door_treats %>%\n  ggplot(aes(x, y)) +\n  geom_tile(color = \"black\",\n            size = 0.6,\n            linetype = \"dotted\",\n            alpha = 0) +\n  geom_label(aes(label = drink.name),\n             nudge_y = 0.5, # give room for logo\n             size = 4) +\n  theme_void() +\n  coord_fixed()\n\n\nIt would look great if we could add the retailers’ logos to the squares. Let’s first import these images from the data folder:\n\n\n\n\nstarbucks_logo <- image_read(here(\"images\", \"starbucks_logo_2018.png\")) %>%\n  rasterGrob(interpolate = T)\n\ncosta_logo <- image_read(here(\"images\", \"costa_logo_2018.png\")) %>%\n  rasterGrob(interpolate = T)\n\ncafe_nero_logo <- image_read(here(\"images\", \"cafe-nero_logo_2018.jpg\")) %>%\n  rasterGrob(interpolate = T)\n\ngreggs_logo <- image_read(here(\"images\", \"greggs_logo_2018.png\")) %>%\n  rasterGrob(interpolate = T)\n\npret_logo <- image_read(here(\"images\", \"pret_logo_2018.jpg\")) %>%\n  rasterGrob(interpolate = T)\n\nmc_cafe_logo <- image_read(here(\"images\", \"mc-café_logo_2018.png\")) %>%\n  rasterGrob(interpolate = T)\n\nInserting images into ggplot2 charts is most easily achieved using annotation_custom. After a lot of jiggery pokery, these are fairly good options for nudging the logo into an appropriate position in the door:\n\ndoor_center <- list(x = 1, y = 1)\n\nnudge_logo_ymin <- -1.25\nnudge_logo_ymax <- -0.25\nnudge_logo_xmin <- -0.5\nnudge_logo_xmax <- 0.5\n\ngg_one_logo <- gg_landscape_door_treats +\n  annotation_custom(starbucks_logo, \n                    ymin = door_center$y + nudge_logo_ymax, \n                    ymax = door_center$y + nudge_logo_ymin, \n                    xmin = door_center$x + nudge_logo_xmin, \n                    xmax = door_center$x + nudge_logo_xmax)\n\n\nBecause annotation_custom only allows us to add one image at a time we need to iteratively add logos for each and every door. I’m a purrr advocate for this kind of task, but if you don’t like this functional approach to things you could use a for loop instead.\npwalk allows us to iterate a function over every row of landscape_door_treats, which in this case adds a annotation_custom layer to gg_logoed_treats. We use the <<- 2 assignment operator so that the global version of gg_logoed_treatsis is updated. pwalk is the same as pmap but doesn’t generate output; side-effects are the only repurcussions of pwalk.\n\ngg_logoed_treats <- gg_landscape_door_treats\n\nlandscape_door_treats %>%\n  pwalk(function(x, y, retailer, ...){\n    \n    logo <- switch (retailer,\n      \"Starbucks\" = starbucks_logo,\n      \"Costa\" = costa_logo,\n      \"Café Nero\" = cafe_nero_logo,\n      \"Greggs\" = greggs_logo,\n      \"McCafé\" = mc_cafe_logo,\n      \"Pret\" = pret_logo\n    )\n    gg_logoed_treats <<- gg_logoed_treats +\n      annotation_custom(logo, ymin = y - 1.25, ymax = y - 0.25, xmin = x - 0.5, xmax = x + 0.5)\n    \n  })"
  },
  {
    "objectID": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html#background-images",
    "href": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html#background-images",
    "title": "Christmas Drinks ggplot2 Advent Calendar 2018",
    "section": "Background images",
    "text": "Background images\nWe should really add background images to the two parts of our advent calendar; a Christmassy image for the doors and a cardboard background for the insides of the doors.\nIf we want a background image applied to the panel() component of a ggplot2 chart we need the ggpubr package. The background_image() must be called immediately after ggplot(aes(x, y)) so as to avoid our geoms being hidden behind the background. I found beautiful license-free image from pixelbay which I think looks great for our advent calendar. Let’s add this file to the images directory and then create our now more festive advent calendar doors:\n\n\n\n\ndownload.file(\"https://cdn.pixabay.com/photo/2017/12/13/14/48/santa-3016939_640.jpg\",\n              here(\"posts\", \"2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018\", \"santa-3016939_640.jpg\"))\n\nchristmassy_background <- image_read(here(\"posts\", \"2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018\", \"santa-3016939_640.jpg\"))\n\ngg_advent_landscape <- landscape_door_centers %>%\n  ggplot(aes(x, y)) +\n  background_image(christmassy_background) +\n  geom_tile(color = \"black\",\n            size = 0.6,\n            linetype = \"dotted\",\n            alpha = 0) +\n  geom_label(aes(label = door_number)) +\n  theme_void() +\n  coord_fixed()\ngg_advent_landscape\n\n\nI think it’s a lot of fun to add a cardboard effect to the treats, so I found this license-free image from flickr which I really enjoy. Let’s add it to our images folder and then add it to our advent calendar treats chart:\n\n\n\n\ndownload.file(\"https://farm4.staticflickr.com/3134/3191148261_e3c1f3887b_o_d.jpg\",\n              here(\"images\", \"cardboard-effect.jpg\"))\n\ncardboard_background <- image_read(here(\"images\", \"cardboard-effect.jpg\"))\n\ngg_landscape_door_treats <- landscape_door_treats %>%\n  ggplot(aes(x, y)) +\n  background_image(cardboard_background) +\n  geom_tile(color = \"black\",\n            size = 0.6,\n            linetype = \"dotted\",\n            alpha = 0) +\n  geom_label(aes(label = drink.name),\n             nudge_y = 0.5, # give room for logo\n             size = 4) +\n  theme_void() +\n  coord_fixed()\ngg_landscape_door_treats"
  },
  {
    "objectID": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html#iteratively-opening-the-advent-calendar",
    "href": "posts/2018-11-28-christmas-drinks-ggplot2-advent-calendar-2018/index.html#iteratively-opening-the-advent-calendar",
    "title": "Christmas Drinks ggplot2 Advent Calendar 2018",
    "section": "Iteratively opening the advent calendar",
    "text": "Iteratively opening the advent calendar\nThe two images we’ve created so far are only really that useful if we’re going to print them out and stick together. And even then, it’s a waste of ink printing out a cardboard effect!\nLet’s create a function for opening the advent doors:\n\nopen_advent_doors <- function(gg_advent,\n                              advent_treats,\n                              open_doors) {\n  gg_local <- gg_advent\n\n  advent_treats %>%\n    filter(door_number %in% open_doors) %>%\n    pwalk(function(x, y, retailer, drink.name, ...) {\n      logo <- switch(retailer,\n        \"Starbucks\" = starbucks_logo,\n        \"Costa\" = costa_logo,\n        \"Café Nero\" = cafe_nero_logo,\n        \"Greggs\" = greggs_logo,\n        \"McCafé\" = mc_cafe_logo,\n        \"Pret\" = pret_logo\n      )\n      gg_local <<- gg_local +\n        annotation_custom(\n          cardboard_background %>%\n            image_crop(\"-200x400\") %>%\n            rasterGrob(interpolate = T),\n          ymin = y - 1.5,\n          ymax = y + 1.5,\n          xmin = x - 2,\n          xmax = x + 2\n        ) +\n        annotation_custom(\n          logo,\n          ymin = y - 1.25,\n          ymax = y - 0.25,\n          xmin = x - 0.5,\n          xmax = x + 0.5\n        ) +\n        geom_label(\n          data = tibble(x, y, drink.name),\n          aes(\n            label = drink.name,\n            x = x,\n            y = y\n          ),\n          nudge_y = 0.5,\n          # give room for logo\n          size = 5\n        )\n    })\n\n  gg_local\n}\n\nHere’s how our calendar looks after opening 10 doors:\n\nopen_advent_doors(gg_advent_landscape,\n                    landscape_door_treats,\n                    1:10)\n\n\nUnfortunately, I don’t know how to use the incredible gganimate package to animate between door openings. Until I learn how, I’m going to create a .gif by exporting each step of the advent being opened and stitch them together with the magick package.\n\ntibble(remaining_days = 1:24) %>%\n  rowwise() %>%\n  mutate(doors = list(1:remaining_days)) %>%\n  pwalk(function(doors, ...) {\n    opened_door <- open_advent_doors(\n      gg_advent_landscape,\n      landscape_door_treats,\n      doors\n    )\n\n    img_path <- here(\n        \"data\",\n        paste0(\n          \"gg_iterative_opened_door_\",\n          formatC(\n            length(doors),\n            width = 2,\n            format = \"d\",\n            flag = \"0\"\n          ),\n          \".png\"\n        )\n      )\n    \n    ggsave(\n      img_path,\n      opened_door\n    )\n    \n    image_read(img_path) %>%\n      image_trim() %>%\n      image_resize(\"1400x\") %>%\n      image_write(img_path)\n    \n  })\n\ndoor_open_images <- list.files(\"images\", \"gg_iterative_opened_door\", full.names = TRUE)\n\n\ndoor_open_images %>%\n  tibble(img = .) %>%\n  map(function(img){image_read(img)}) %>%\n  image_join() %>%\n  image_animate(fps = 0.5) %>%\n  image_write(here(\n        \"images\", \"gg_animated_door_openings.gif\"))"
  },
  {
    "objectID": "posts/2018-11-12-crawling-doi-from-a-sage/index.html",
    "href": "posts/2018-11-12-crawling-doi-from-a-sage/index.html",
    "title": "Crawling DOI from a SAGE",
    "section": "",
    "text": "We’ll go through the following steps:\n\nGenerate URLs for all issues of the journal\nInspect the source code for the page for the DOI\nScrape all the pages\nScrape all the pages using parallel processing via the future package\n\nLet’s load all the packaes we’re going to use up front:\n\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\nlibrary(\"glue\")\nlibrary(\"httr\")\nlibrary(\"future\")\n\n\nURLs for each journal issue\nLet’s simplify things and only consider issues of the journal up to the date this blogpost was written (2018-11-12). I can’t guarantee that the journal won’t completely change their URL scheme tomorrow, but until they do change things all issues have the following URL structure:\nhttps://journals.sagepub.com/toc/hthb/{volume}/{issue}\nThere have always been 4 issues a year, and the most recent volume is 23. Let’s setup a tibble() with this data:\n\nissue_urls <- tibble(volume = 1:23, issue_1 = 1, issue_2 = 2, issue_3 = 3, issue_4 = 4)\n\nI’ll now use gather() to convert this into a long and tidy dataset that iterates through all issues:\n\nissue_urls <- issue_urls %>%\n  gather(issue.colname, issue, issue_1:issue_4) %>%\n  select(-issue.colname) %>%\n  arrange(volume)\n\nNow we can construct our URLs using glue()\n\nissue_urls <- issue_urls %>%\n  mutate(issue_url = glue(\"https://journals.sagepub.com/toc/hthb/{volume}/{issue}\"))\nhead(issue_urls)\n\n# A tibble: 6 × 3\n  volume issue issue_url                                \n   <int> <dbl> <glue>                                   \n1      1     1 https://journals.sagepub.com/toc/hthb/1/1\n2      1     2 https://journals.sagepub.com/toc/hthb/1/2\n3      1     3 https://journals.sagepub.com/toc/hthb/1/3\n4      1     4 https://journals.sagepub.com/toc/hthb/1/4\n5      2     1 https://journals.sagepub.com/toc/hthb/2/1\n6      2     2 https://journals.sagepub.com/toc/hthb/2/2\n\n\n\n\nInspect the source code for the DOI\nInspecting the source code reveals that the title of each article in the issue has the attribute data-item-name, with the value click-article-title.\n\nLet’s use the most recent issue as a toy example:\n\n\"https://journals.sagepub.com/toc/hthb/23/4\" %>%\n  read_html() %>%\n  html_nodes(\"[data-item-name=click-article-title]\") %>%\n  html_attrs() %>%\n  .[[1]]\n\n\n\n                      data-item-name                                class \n               \"click-article-title\"                         \"ref nowrap\" \n                                href \n\"/doi/full/10.1177/1758998318784316\" \n\n\nThe DOI for the article is almost the href value, there’s some fluff we’ll get rid of later. But we know enough we can create a function for extracting the href value\n\nget_article_dois_from_issue <- function(issue_url) {\n  \n  issue_page <- tryCatch(issue_url %>%\n    read_html(),\n  error = function(c) NA\n  )\n\n  if (is.na(issue_page)) {\n    return(NA)\n  }\n\n  issue_url %>%\n    read_html() %>%\n    html_nodes(\"[data-item-name=click-article-title]\") %>%\n    html_attr(\"href\")\n}\n\n\n\"https://journals.sagepub.com/toc/hthb/23/4\" %>%\n  get_article_dois_from_issue()\n\n\n\n[[1]]\n[1] \"/doi/full/10.1177/1758998318784316\"\n\n[[2]]\n[1] \"/doi/full/10.1177/1758998318796010\"\n\n[[3]]\n[1] \"/doi/full/10.1177/1758998318798668\"\n\n[[4]]\n[1] \"/doi/full/10.1177/1758998318809574\"\n\n\n\n\nScrape all the pages\nThe wonderful purrr package allows us to insert these (almost) DOIs into the rows of our tibble() as follows:\n\nexample_dois <- issue_urls %>%\n  slice(52:54) %>%\n  mutate(doi = map(issue_url, function(x)get_article_dois_from_issue(x)))\n\n\n\nWarning: All elements of `...` must be named.\nDid you want `data = doi`?\n\n\n# A tibble: 3 × 4\n  volume issue issue_url                                  data            \n   <int> <dbl> <glue>                                     <list>          \n1     13     4 https://journals.sagepub.com/toc/hthb/13/4 <tibble [3 × 1]>\n2     14     1 https://journals.sagepub.com/toc/hthb/14/1 <tibble [5 × 1]>\n3     14     2 https://journals.sagepub.com/toc/hthb/14/2 <tibble [6 × 1]>\n\n\nThe unnest() function from tidyr allows us to unpack these list columns\n\nexample_dois %>%\n  unnest(doi)\n\n# A tibble: 14 × 4\n   volume issue issue_url                                  doi                  \n    <int> <dbl> <glue>                                     <chr>                \n 1     13     4 https://journals.sagepub.com/toc/hthb/13/4 /doi/pdf/10.1177/175…\n 2     13     4 https://journals.sagepub.com/toc/hthb/13/4 /doi/pdf/10.1177/175…\n 3     13     4 https://journals.sagepub.com/toc/hthb/13/4 /doi/pdf/10.1177/175…\n 4     14     1 https://journals.sagepub.com/toc/hthb/14/1 /doi/full/10.1258/ht…\n 5     14     1 https://journals.sagepub.com/toc/hthb/14/1 /doi/full/10.1258/ht…\n 6     14     1 https://journals.sagepub.com/toc/hthb/14/1 /doi/full/10.1258/ht…\n 7     14     1 https://journals.sagepub.com/toc/hthb/14/1 /doi/full/10.1258/ht…\n 8     14     1 https://journals.sagepub.com/toc/hthb/14/1 /doi/full/10.1258/ht…\n 9     14     2 https://journals.sagepub.com/toc/hthb/14/2 /doi/full/10.1258/ht…\n10     14     2 https://journals.sagepub.com/toc/hthb/14/2 /doi/full/10.1258/ht…\n11     14     2 https://journals.sagepub.com/toc/hthb/14/2 /doi/full/10.1258/ht…\n12     14     2 https://journals.sagepub.com/toc/hthb/14/2 /doi/full/10.1258/ht…\n13     14     2 https://journals.sagepub.com/toc/hthb/14/2 /doi/full/10.1258/ht…\n14     14     2 https://journals.sagepub.com/toc/hthb/14/2 /doi/full/10.1258/ht…\n\n\nAll DOI begin with 10.1 which we can use to tidy up these almost DOI into real DOI:\n\nexample_dois %>%\n  unnest(doi) %>%\n  mutate(doi = str_replace(doi, \".*/10.\", \"http://doi.org/10.\"))\n\n# A tibble: 14 × 4\n   volume issue issue_url                                  doi                  \n    <int> <dbl> <glue>                                     <chr>                \n 1     13     4 https://journals.sagepub.com/toc/hthb/13/4 http://doi.org/10.11…\n 2     13     4 https://journals.sagepub.com/toc/hthb/13/4 http://doi.org/10.11…\n 3     13     4 https://journals.sagepub.com/toc/hthb/13/4 http://doi.org/10.11…\n 4     14     1 https://journals.sagepub.com/toc/hthb/14/1 http://doi.org/10.12…\n 5     14     1 https://journals.sagepub.com/toc/hthb/14/1 http://doi.org/10.12…\n 6     14     1 https://journals.sagepub.com/toc/hthb/14/1 http://doi.org/10.12…\n 7     14     1 https://journals.sagepub.com/toc/hthb/14/1 http://doi.org/10.12…\n 8     14     1 https://journals.sagepub.com/toc/hthb/14/1 http://doi.org/10.12…\n 9     14     2 https://journals.sagepub.com/toc/hthb/14/2 http://doi.org/10.12…\n10     14     2 https://journals.sagepub.com/toc/hthb/14/2 http://doi.org/10.12…\n11     14     2 https://journals.sagepub.com/toc/hthb/14/2 http://doi.org/10.12…\n12     14     2 https://journals.sagepub.com/toc/hthb/14/2 http://doi.org/10.12…\n13     14     2 https://journals.sagepub.com/toc/hthb/14/2 http://doi.org/10.12…\n14     14     2 https://journals.sagepub.com/toc/hthb/14/2 http://doi.org/10.12…\n\n\nIt’s painfully slow going through all 92 issues in this fashion, thankfully it’s fairly easy to run this in parallel with the future package.\n\n\nScrape all the pages with future\nTo begin our work with the future package we must tell it our plan to use multicore evaluation as follows:\n\nplan(multiprocess)\n\nWe use future() to tell the future package to run according to the plan we just set:\n\nstart_scrape <- Sys.time()\nscraped_dois <- issue_urls %>%\n  mutate(dois = map(issue_url, ~future(get_article_dois_from_issue(.x))))\nend_scrape <- Sys.time()\n\nThis whole process hasn’t taken much time (at the time of writing):\n\nend_scrape - start_scrape\n\n\n\nTime difference of 1.755138 mins\n\n\nBut our dataset isn’t ready to work with yet, our list column is full of MulticoreFuture things:\n\nscraped_dois\n\n\nWe use the value() function to extract the value of our future calculations and unnest() as previously:\n\n\n\n\nscraped_dois %>%\n  mutate(dois = map(dois, ~value(.x))) %>%\n  unnest(dois) %>%\n  filter(!is.na(dois)) %>%\n  rename(doi = dois) %>%\n  mutate(doi = str_replace(doi, \".*/10.\", \"http://doi.org/10.\")) %>%\n  select(-issue_url)\n\n\n\n# A tibble: 459 × 3\n   volume issue doi                                      \n    <int> <dbl> <chr>                                    \n 1      1     1 http://doi.org/10.1177/175899839600100401\n 2      1     1 http://doi.org/10.1177/175899839600100402\n 3      1     1 http://doi.org/10.1177/175899839600100403\n 4      1     1 http://doi.org/10.1177/175899839600100404\n 5      1     1 http://doi.org/10.1177/175899839600100405\n 6      1     1 http://doi.org/10.1177/175899839600100406\n 7      1     1 http://doi.org/10.1177/175899839600100407\n 8      1     1 http://doi.org/10.1177/175899839600100408\n 9      1     1 http://doi.org/10.1177/175899839600100409\n10      1     1 http://doi.org/10.1177/175899839600100410\n# … with 449 more rows\n\n\nThis was the final output my friend needed - all 459 DOI-issued articles from the journal. It was really easy to put all of this together and finally get a chance to use the future package properly. If I was asked how to make this more rigorous I’d recommend the following:\n\nProgrammatically discover the most recent volume and issue\nDon’t assume a max of 4 issues per volume, allow the code to iterate through a volume.\n\n\n\n\n\n\nFootnotes\n\n\nSee the DOI Handbook DOI: 10.1000/182.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hadley2018,\n  author = {Charlotte Hadley},\n  title = {Crawling {DOI} from a {SAGE}},\n  date = {2018-11-12},\n  url = {https://visibledata.co.uk/posts/2018-11-12-crawling-doi-from-a-sage},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCharlotte Hadley. 2018. “Crawling DOI from a SAGE.”\nNovember 12, 2018. https://visibledata.co.uk/posts/2018-11-12-crawling-doi-from-a-sage."
  },
  {
    "objectID": "posts/2018-02-28_great-circles-with-sf-and-leaflet/index.html",
    "href": "posts/2018-02-28_great-circles-with-sf-and-leaflet/index.html",
    "title": "Great circles with sf and leaflet",
    "section": "",
    "text": "Surprisingly often I’m asked to help folks build maps which visualise the journeys between geographic locations, in the bad old days of early 2017 I would have used the geospheres package and sp.\nThat’s because the shortest distance between two points on a map is a great circle, and geospheres is capable of computing such things. Note that it’s these great circles that you see on flight information screens, their exact curvature depends on the projection of the map.\nHowever, I’m now an sf convert and try to do everything within that package. In my original version of this blogpost I introduced my painfully inefficient way to compute these great circles with sf and asked folks in the R4DS Slack group run by Jesse Maegan if they had any suggested improvements, which I’ve now baked in below.\nTypically my datasets are collections of journeys, here’s an example of what the data looks like (these are some of the journeys I took during the 3 years I worked for Wolfram Research):\nI’ve split this post into two sections below:"
  },
  {
    "objectID": "posts/2018-02-28_great-circles-with-sf-and-leaflet/index.html#linestrings",
    "href": "posts/2018-02-28_great-circles-with-sf-and-leaflet/index.html#linestrings",
    "title": "Great circles with sf and leaflet",
    "section": "LINESTRINGs",
    "text": "LINESTRINGs\nWe need LINESTRINGs for our great circles as these represent a “sequence of points connected by straight, non-self intersecting line pieces; one-dimensional geometry”. As with all simple features, LINESTRINGs are constructed from matrices:\n\nst_linestring(matrix(1:6, 3))\n\nLINESTRING (1 4, 2 5, 3 6)\n\n\nLet’s extract the start and end coordinates from our raw_journeys_data and convert them to a list with purrr::transpose:\n\nraw_journey_data %>% \n  select(-name) %>% \n  transpose()\n\n[[1]]\n[[1]]$start.long\n[1] -0.118092\n\n[[1]]$start.lat\n[1] 51.50986\n\n[[1]]$end.long\n[1] -119.6982\n\n[[1]]$end.lat\n[1] 34.42083\n\n\n[[2]]\n[[2]]$start.long\n[1] 31.49677\n\n[[2]]$start.lat\n[1] 30.0263\n\n[[2]]$end.long\n[1] 24.75357\n\n[[2]]$end.lat\n[1] 59.43696\n\n\n[[3]]\n[[3]]$start.long\n[1] 126.6333\n\n[[3]]$start.lat\n[1] 45.75\n\n[[3]]$end.long\n[1] 46.73859\n\n[[3]]$end.lat\n[1] 24.77426\n\n\nThese lists can easily be converted to matrices using map:\n\nraw_journey_data %>% \n  select(-name) %>% \n  transpose() %>% \n  map(~ matrix(flatten_dbl(.), nrow = 2, byrow = TRUE)) \n\n[[1]]\n            [,1]     [,2]\n[1,]   -0.118092 51.50986\n[2,] -119.698189 34.42083\n\n[[2]]\n         [,1]     [,2]\n[1,] 31.49677 30.02630\n[2,] 24.75357 59.43696\n\n[[3]]\n          [,1]     [,2]\n[1,] 126.63333 45.75000\n[2,]  46.73859 24.77426\n\n\n… and finally we can convert these to\n\nlist_of_linestrings <- raw_journey_data %>%\n  select(-name) %>%\n  transpose() %>%\n  map(~ matrix(flatten_dbl(.), nrow = 2, byrow = TRUE)) %>%\n  map(st_linestring)\nlist_of_linestrings\n\nLINESTRING (-0.118092 51.50986, -119.6982 34.42083)\n\n\nLINESTRING (31.49677 30.0263, 24.75357 59.43696)\n\n\nLINESTRING (126.6333 45.75, 46.73859 24.77426)"
  },
  {
    "objectID": "posts/2018-02-28_great-circles-with-sf-and-leaflet/index.html#collecting-features",
    "href": "posts/2018-02-28_great-circles-with-sf-and-leaflet/index.html#collecting-features",
    "title": "Great circles with sf and leaflet",
    "section": "Collecting features",
    "text": "Collecting features\nEventually we will create an \"sf\" \"data.frame\" which contains our LINESTRINGs in the geometry column. We create a set of features for this column from our list of LINESTRINGs with the st_sfc function. Note that we need to specify a coordinate system, the most obvious choice is the WGS84 coordinate reference system, that’s why we specify crs = 4326:\n\nlist_of_linestrings %>%\n  st_sfc(crs = 4326)\n\nGeometry set for 3 features \nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -119.6982 ymin: 24.77426 xmax: 126.6333 ymax: 59.43696\nGeodetic CRS:  WGS 84\n\n\nNow we have this set of features we create a collection of them in a \"sf\" \"data.frame\" with st_sf:\n\ncollection_sf <- list_of_linestrings %>%\n  st_sfc(crs = 4326) %>%\n  st_sf(geometry = .)\ncollection_sf\n\nSimple feature collection with 3 features and 0 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -119.6982 ymin: 24.77426 xmax: 126.6333 ymax: 59.43696\nGeodetic CRS:  WGS 84\n                        geometry\n1 LINESTRING (-0.118092 51.50...\n2 LINESTRING (31.49677 30.026...\n3 LINESTRING (126.6333 45.75,..."
  },
  {
    "objectID": "posts/2018-02-28_great-circles-with-sf-and-leaflet/index.html#joining-feature-collections-with-data",
    "href": "posts/2018-02-28_great-circles-with-sf-and-leaflet/index.html#joining-feature-collections-with-data",
    "title": "Great circles with sf and leaflet",
    "section": "Joining feature collections with data",
    "text": "Joining feature collections with data\nAs sf collections store all of the GIS data in a column it’s very easy to combine this object with our original data - we use dplyr::bind_columns\n\ncollection_sf %>%\n  bind_cols(raw_journey_data)\n\nSimple feature collection with 3 features and 5 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -119.6982 ymin: 24.77426 xmax: 126.6333 ymax: 59.43696\nGeodetic CRS:  WGS 84\n  start.long start.lat   end.long  end.lat                    name\n1  -0.118092  51.50986 -119.69819 34.42083 London to Santa Barbara\n2  31.496773  30.02630   24.75357 59.43696    New Cairo to Tallinn\n3 126.633333  45.75000   46.73859 24.77426        Harbin to Riyadh\n                        geometry\n1 LINESTRING (-0.118092 51.50...\n2 LINESTRING (31.49677 30.026...\n3 LINESTRING (126.6333 45.75,...\n\n\nWe can now perform calculations on our dataset, for instance compute great circles with a resolution of 100km:\n\ncollection_sf %>%\n  bind_cols(raw_journey_data) %>%\n  st_segmentize(units::set_units(100, km)) %>%\n  leaflet() %>%\n  addTiles() %>%\n  addPolylines(label = ~name)\n\n\n\n\n\nSadly, there’s no good solution for adding arrow heads to these great circles - they would be heavily distorted by the map projection and would look ugly. An alternative solution to showing where journeys start and end is to use different coloured circle markers:\n\ncollection_sf %>%\n  bind_cols(raw_journey_data) %>%\n  st_segmentize(units::set_units(100, km)) %>%\n  leaflet() %>%\n  addTiles() %>%\n  addPolylines() %>%\n  addCircleMarkers(\n    lng = ~start.long,\n    lat = ~start.lat,\n    color = \"green\",\n    opacity = 1,\n    radius = 2\n  ) %>%\n  addCircleMarkers(\n    lng = ~end.long,\n    lat = ~end.lat,\n    color = \"red\",\n    opacity = 1,\n    radius = 2\n  ) %>%\n  addLegend(\n    colors = c(\n      \"green\",\n      \"red\"\n    ),\n    labels = c(\n      \"Journey start\",\n      \"Journey end\"\n    )\n  )"
  },
  {
    "objectID": "posts/2018-02-27-crawling-with-tidygraph/index.html",
    "href": "posts/2018-02-27-crawling-with-tidygraph/index.html",
    "title": "Crawling with tidygraph",
    "section": "",
    "text": "The Rcrawler documentation includes an example 1 of how to build a network of the internal links in a website, using the code below. This takes up to 2 minutes, so I’ve made the data available in a Gist here.\n\nlibrary(\"Rcrawler\")\nRcrawler(\n    Website = \"http://researchdata.ox.ac.uk\",\n    no_cores = 4,\n    no_conn = 4 ,\n    NetworkData = TRUE,\n    statslinks = TRUE\n  )\n\nThe Rcrawler function creates two objects in your global workspace:\n\nNetwIndex: A list of all the web pages (URLs) from the website\nNetwEdges: The internal links between these pages\n\nIt’s much easier to work with these objects if we augment the NetwIndex object with the ids used in the NetwEdges object, and I’ll export the data into a Gist for reproducibility and so I don’t have to recrawl the site each time I generate this post!\n\nlibrary(\"tidyverse\")\nlibrary(\"gistr\")\nrdo_oxford_index <- tibble(\n  id = 1:length(NetwIndex),\n  url = NetwIndex\n) %>%\n  mutate(name = url)\n\nrdo_oxford_index %>%\n  write_csv(\"rdo_oxford_index.csv\")\n\nNetwEdges %>%\n  write_csv(\"rdo_oxford_edges.csv\")\n\nrdo_oxford_gist <- gist_create(\n  files = c(\n    \"rdo_oxford_edges.csv\",\n    \"rdo_oxford_index.csv\"\n  ),\n  description = \"Blogpost: crawling with tidygraph\"\n)\n\nNow we can import the data directly from the gist as follows:\n\nlibrary(\"tidyverse\")\nrdo_oxford_edges <- read_csv(\"https://gist.githubusercontent.com/charliejhadley/ba5a983e4e29cae29d379fc9daf1d873/raw/dea7df7ae9a1542372fe6203362991ec019bb1c3/rdo_oxford_edges.csv\")\nrdo_oxford_index <- read_csv(\"https://gist.githubusercontent.com/charliejhadley/ba5a983e4e29cae29d379fc9daf1d873/raw/c3ead87cd4dc95696432b4f6547e4d7762132721/rdo_oxford_index.csv\")\n\nLet’s generate an igraph object for manipulation with the tidygraph library, after first removing self-loops via the simplify function but retain multiple edges between pairs of webpages. The graph is too large to visualise sensibly at present:\n\nlibrary(\"igraph\")\nrdo_oxford_igraph <- graph_from_data_frame(\n  rdo_oxford_edges,\n  vertices = rdo_oxford_index\n) %>%\n  simplify(remove.multiple = FALSE)\ntibble(\n  edges = ecount(rdo_oxford_igraph),\n  vertices = vcount(rdo_oxford_igraph)\n)\n\n# A tibble: 1 × 2\n  edges vertices\n  <dbl>    <int>\n1 20330      383\n\n\nIt would be useful if we could augment the rdo_oxford_index object with some features that allow us to selectively visualise certain types of pages, which include:\n\npages that are generated due to pagination rules\nauto-generated pages for tags\nauto-generated pages for “portfolio items”\n\n\nrdo_oxford_index <- rdo_oxford_index %>%\n  mutate(paginated.page = if_else(str_detect(url, \"/page/\"),\n                                  TRUE,\n                                  FALSE)) %>%\n  mutate(tag.page = if_else(str_detect(url, \"/tag/\"),\n                                  TRUE,\n                                  FALSE)) %>%\n  mutate(portfolio.page = if_else(str_detect(url, \"/portfolio/\"),\n                                  TRUE,\n                                  FALSE))\n\nLet’s also markup the pages linked to from the websites navigation menu and footer, as these are linked to on every single page!\n\nlibrary(\"rvest\")\nnavbar_links <-\n  read_html(\n  \"https://gist.githubusercontent.com/charliejhadley/ba5a983e4e29cae29d379fc9daf1d873/raw/4294b8180866b70f081c05a1dca9cb3bfd172fe3/navigation.html\"\n  ) %>%\n  html_nodes(\"a\") %>%\n  html_attr('href') %>%\n  unique()\n\nfooter_links <- c(\"http://researchdata.ox.ac.uk/credits/\",\n                  \"http://researchdata.ox.ac.uk/rdm-delivery-group/\",\n                  \"https://www1.admin.ox.ac.uk/researchsupport/researchcommittees/scworkgroups/rdmopendata/\")\n\nrdo_oxford_index <- rdo_oxford_index %>%\n  mutate(navbar.page = if_else(url %in% navbar_links,\n                               TRUE,\n                               FALSE)) %>%\n  mutate(footer.page = if_else(url %in% footer_links,\n                               TRUE,\n                               FALSE)) %>%\n  mutate(node.id = row_number())\n\nNow’s time for me to learn how to use tidygraph! To filter out the paginated.pages, portfolio.pages and tag.pages and all links to the navbar.pages and footer.pages I need to follow these steps:\n\nConvert an igraph object to a \"tbl_graph\" \"igraph\" object\nActivate nodes before filtering on them\nActivate edges before filtering on them\n\n\nlibrary(\"tidygraph\")\nrdo_oxford_igraph <- graph_from_data_frame(\n  rdo_oxford_edges,\n  vertices = rdo_oxford_index\n) %>%\n  simplify(remove.multiple = FALSE)\n\nnavbar_page_new_ids <- rdo_oxford_index %>%\n  filter(navbar.page == TRUE) %>%\n  select(node.id) %>%\n  .[[1]]\n\nfooter_page_new_ids <- rdo_oxford_index %>%\n  filter(footer.page == TRUE) %>%\n  select(node.id) %>%\n  .[[1]]\n\nrdo_oxford_tidygraph <- rdo_oxford_igraph %>%\n  as_tbl_graph() %>%\n  # to_undirected() %>% \n  activate(nodes) %>%\n  filter(paginated.page == FALSE & portfolio.page == FALSE & tag.page == FALSE) %>%\n  activate(edges) %>%\n  filter(!to %in% navbar_page_new_ids) %>%\n  filter(!to %in% footer_page_new_ids) \n\nFinally, let’s add a group column so we can easily colour the different types of pages with the ggraph library:\n\nlibrary(\"ggraph\")\nrdo_oxford_tidygraph <- rdo_oxford_tidygraph %>%\n  activate(nodes) %>%\n  mutate(group = if_else(navbar.page == TRUE,\n                         \"Navbar Page\",\n                         \"Other Page\")) %>%\n  mutate(group = if_else(footer.page == TRUE,\n                         \"Footer Page\",\n                         group))\n\nrdo_oxford_tidygraph %>%\n  ggraph() +\n  geom_edge_fan() +\n  geom_node_point(aes(color = group)) +\n  theme_graph()\n\n\n\n\nActually, there are two more things I’d like to do: extract the largest connected component and make an interactive network viz. It was slightly frustrating to figure out from the documentation how to use the group_components function, but it became clear that it works similarly to group_indices:\n\nrdo_oxford_tidygraph %>%\n  activate(nodes) %>%\n  mutate(component = group_components()) %>%\n  filter(component == 1) %>%\n  ggraph() +\n  geom_edge_fan() +\n  geom_node_point(aes(color = group)) +\n  theme_graph()\n\n\n\n\nIt’s really easy to create interactive dataviz using htmlwidgets (what are htmlwidgets?), my favourite option for network visualisations is visNetwork, which happily consumes igraph objects…\n\nlibrary(\"visNetwork\")\nrdo_oxford_tidygraph %>%\n  activate(nodes) %>%\n  mutate(component = group_components()) %>%\n  filter(component == 1) %>%\n  mutate(title = url) %>%\n  mutate(label = \"\") %>% # remove long url labels from underneath nodes\n  as.igraph() %>%\n  visIgraph(idToLabel = FALSE) %>% # remove long url labels from underneath nodes\n  visOptions(highlightNearest = TRUE) %>%\n  visLegend()\n\n\n\n\n\nThis was a really fun way to learn how to use tidygraph and ggraph, which I’ve been putting off for a long time. I’m really grateful to Thomas Lin Pedersen for all his work on these amazing packages!\n\n\n\n\nFootnotes\n\n\nActually, there was an error in the example so I made a pull request to fix it 😄↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hadley2018,\n  author = {Charlie Hadley},\n  title = {Crawling with Tidygraph},\n  date = {2018-02-27},\n  url = {https://visibledata.co.uk/posts/2018-02-27-crawling-with-tidygraph},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCharlie Hadley. 2018. “Crawling with Tidygraph.” February\n27, 2018. https://visibledata.co.uk/posts/2018-02-27-crawling-with-tidygraph."
  },
  {
    "objectID": "posts/2018-02-18-recipes-for-learning/index.html",
    "href": "posts/2018-02-18-recipes-for-learning/index.html",
    "title": "Recipes for learning",
    "section": "",
    "text": "Python was clearly a mess of choices, I had to decide between Python 2.x or 3.x and there wasn’t a self-contained solution for building web applications. Learning R and Shiny was the obvious choice in just over three weeks, as was the dataset I’d focus on.\n\n\nOne of my absolute favourite datasets comes from a paper published in Nature, “Flavor Network and the principles of food pairing” (doi:10.1038/srep00196). The supplementary materials include two files:\n\nsrep00196-s2.csv: details the number of flavour compounds shared between over 1500 ingredients.\nsrep00196-s3.csv: contains over 57,000 recipes categorised by cuisine.\n\n\n\n\n\n\n\n\nIn the paper the authors are interested examining whether different cuisines prefer recipes with highly similar or dissimilar tasting ingredients, amongst other things. I’ve embedded one of the highly beautified hair ball networks from the paper, and I definitey recommend reading this Open Access paper for some interesting observations about human cuisines. Now, it turned that out this was a fairly challenging dataset to first start learning R with, let’s grab the data from Nature 1 and have a look at why.\n\n## Reproducibly download files to a temp location and unzip\nlibrary(\"tidyverse\")\nrecipes_data_dir <- tempdir()\ns2_zip <- tempfile(fileext = \".zip\")\ns3_zip <- tempfile(fileext = \".zip\")\n\ndownload.file(url = \"https://static-content.springer.com/esm/art%3A10.1038%2Fsrep00196/MediaObjects/41598_2011_BFsrep00196_MOESM2_ESM.zip\", destfile = s2_zip)\ndownload.file(url = \"https://static-content.springer.com/esm/art%3A10.1038%2Fsrep00196/MediaObjects/41598_2011_BFsrep00196_MOESM3_ESM.zip\", destfile = s3_zip)\n\nunzip(s2_zip, exdir = recipes_data_dir)\nunzip(s3_zip, exdir = recipes_data_dir)\n\nThe recipes are in a horrendous format. It’s not so bad that the first four lines are comments, that’s easy to handle, but rows of the data do not have consistent lengths. The first recipe contains 6 items and the second has 17, which means the data’s not rectangular and the standard importers aren’t going to be that happy.\n\nreadLines(file.path(recipes_data_dir, \"srep00196-s3.csv\")) %>%\n  head()\n\n[1] \"#\"                                                                                                                                         \n[2] \"# Flavor network and the principles of food pairing\"                                                                                       \n[3] \"# Yong-Yeol Ahn, Sebastian E. Ahnert, James P. Bagrow, and Albert-Laszlo Barabasi\"                                                         \n[4] \"# \"                                                                                                                                        \n[5] \"African,chicken,cinnamon,soy_sauce,onion,ginger\"                                                                                           \n[6] \"African,cane_molasses,ginger,cumin,garlic,tamarind,bread,coriander,vinegar,onion,beef,cayenne,parsley,wheat_bread,yogurt,vegetable_oil,egg\"\n\n\nI really struggled to understand what to do about this, so I asked my first R question on StackOverflow, “Importing and analysing non-rectangular .csv files in R”. My question was asked back in 2015 before the tidyverse was born, and so the best solution at the time was using read.table:\n\nread.table(file.path(recipes_data_dir, \"srep00196-s3.csv\"), \n           sep = \",\", \n           as.is = TRUE, \n           fill = TRUE, \n           na.strings = \"\") %>% \n  head()\n\n       V1            V2       V3          V4       V5       V6       V7\n1 African       chicken cinnamon   soy_sauce    onion   ginger     <NA>\n2 African cane_molasses   ginger       cumin   garlic tamarind    bread\n3 African        butter   pepper       onion cardamom  cayenne   ginger\n4 African     olive_oil   pepper       wheat     beef    onion cardamom\n5 African         honey    wheat       yeast     <NA>     <NA>     <NA>\n6 African        tomato cilantro lemon_juice    onion  cayenne scallion\n              V8      V9      V10  V11     V12     V13         V14    V15\n1           <NA>    <NA>     <NA> <NA>    <NA>    <NA>        <NA>   <NA>\n2      coriander vinegar    onion beef cayenne parsley wheat_bread yogurt\n3 cottage_cheese  garlic brassica <NA>    <NA>    <NA>        <NA>   <NA>\n4          cumin  garlic     rice leek    <NA>    <NA>        <NA>   <NA>\n5           <NA>    <NA>     <NA> <NA>    <NA>    <NA>        <NA>   <NA>\n6           <NA>    <NA>     <NA> <NA>    <NA>    <NA>        <NA>   <NA>\n            V16  V17\n1          <NA> <NA>\n2 vegetable_oil  egg\n3          <NA> <NA>\n4          <NA> <NA>\n5          <NA> <NA>\n6          <NA> <NA>\n\n\nAt the time I continued to analyse and visualise the data ready for my interview exercise, for which I also learned how to use GitHub! The outputs I used in my presentation are still available in my RecipeVisualisations repo. It turns out on reflection that there were two bad things that I did at the time:\n\nI didn’t know enough R to have a reproducible workflow so have lost some of my tidying scripts\nread.table uses a bad heuristic to decide how many columns there are in the data! The longest recipe is not 17 ingredients long.\n\nNow I’m much more proficient with R and have the wonderful purrr library to process these recipes into a tibble with the ingredients stored as a list:\n\nrecipe_lists <- readLines(file.path(recipes_data_dir, \"srep00196-s3.csv\")) %>%\n  strsplit(\",\")\nrecipe_lists <- recipe_lists[5:length(recipe_lists)]\nrecipes_df <- tibble(\n  cuisine = map_chr(recipe_lists, 1),\n  ingredients = map(recipe_lists, tail, -1)\n)\nrecipes_df %>%\n  head()\n\n# A tibble: 6 × 2\n  cuisine ingredients\n  <chr>   <list>     \n1 African <chr [5]>  \n2 African <chr [16]> \n3 African <chr [9]>  \n4 African <chr [10]> \n5 African <chr [3]>  \n6 African <chr [6]>  \n\n\nIt’s now much easier to operate on these lists of ingredients by using map* functions within mutate, for instance I can create a column containing the number of ingredients. Now we discover the joint longest recipes contain a ridiculous 32 ingredients.\n\nrecipes_df %>%\n  mutate(n.ingredients = map_int(ingredients, length)) %>%\n  arrange(desc(n.ingredients))\n\n# A tibble: 56,498 × 3\n   cuisine          ingredients n.ingredients\n   <chr>            <list>              <int>\n 1 EasternEuropean  <chr [32]>             32\n 2 SouthernEuropean <chr [32]>             32\n 3 NorthAmerican    <chr [30]>             30\n 4 NorthAmerican    <chr [29]>             29\n 5 NorthAmerican    <chr [29]>             29\n 6 NorthAmerican    <chr [29]>             29\n 7 EasternEuropean  <chr [28]>             28\n 8 NorthAmerican    <chr [28]>             28\n 9 NorthAmerican    <chr [27]>             27\n10 NorthAmerican    <chr [26]>             26\n# … with 56,488 more rows\n\n\nWith our ingredients in a list column it’s now also easy to filter recipes by specific ingredients:\n\nrecipes_df %>%\n  filter(str_detect(ingredients, \"garlic\"))\n\n# A tibble: 16,893 × 2\n   cuisine ingredients\n   <chr>   <list>     \n 1 African <chr [16]> \n 2 African <chr [9]>  \n 3 African <chr [10]> \n 4 African <chr [12]> \n 5 African <chr [11]> \n 6 African <chr [8]>  \n 7 African <chr [9]>  \n 8 African <chr [12]> \n 9 African <chr [15]> \n10 African <chr [10]> \n# … with 16,883 more rows\n\n\nI’m going to come back to using this dataset in the future to explore graph theory and machine learning examples, but for now let’s finish like it’s Summer 2017 with a ridgeline plot from the excellent ggplot2 extension ggridges:\n\nlibrary(\"ggridges\")\nrecipes_df %>%\n  mutate(n.ingredients = map_int(ingredients, length)) %>%\n  group_by(cuisine) %>%\n  mutate(median.ingredients = median(n.ingredients)) %>%\n  ungroup() %>%\n  arrange(desc(median.ingredients)) %>%\n  mutate(cuisine = fct_reorder(cuisine, median.ingredients)) %>%\n  ggplot(aes(x = n.ingredients, y = cuisine)) + \n  geom_density_ridges(scale = 3) + \n  theme_ridges() +\n  xlab(\"Number of ingredients\") +\n  ggtitle(\"Comparison of ingredients per recipe by cuisine\",\n          subtitle = \"Data from doi:10.1038/srep00196\")\n\n\n\n\nIn future whenever I want to work with this data I’ll import it through the following script:\n\nlibrary(\"tidyverse\")\nrecipes_data_dir <- tempdir()\ns2_zip <- tempfile(fileext = \".zip\")\ns3_zip <- tempfile(fileext = \".zip\")\n\ndownload.file(url = \"https://static-content.springer.com/esm/art%3A10.1038%2Fsrep00196/MediaObjects/41598_2011_BFsrep00196_MOESM2_ESM.zip\", destfile = s2_zip)\ndownload.file(url = \"https://static-content.springer.com/esm/art%3A10.1038%2Fsrep00196/MediaObjects/41598_2011_BFsrep00196_MOESM3_ESM.zip\", destfile = s3_zip)\n\nunzip(s2_zip, exdir = recipes_data_dir)\nunzip(s3_zip, exdir = recipes_data_dir)\nrecipe_lists <- readLines(file.path(recipes_data_dir, \"srep00196-s3.csv\")) %>%\n  strsplit(\",\")\nrecipe_lists <- recipe_lists[5:length(recipe_lists)]\nrecipes_df <- tibble(\n  cuisine = map_chr(recipe_lists, 1),\n  ingredients = map(recipe_lists, tail, -1)\n)\n\n\n\n\n\nFootnotes\n\n\nFrustratingly, these data sets are attached to the paper exclusively as supplementary materials, and so these are fragile links. Data should really have distinct DOI, modern journals like Springer Nature’s Scientific Data do this.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hadley2018,\n  author = {Charlie Hadley},\n  title = {Recipes for Learning},\n  date = {2018-02-18},\n  url = {https://visibledata.co.uk/posts/2018-02-18-recipes-for-learning},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCharlie Hadley. 2018. “Recipes for Learning.” February 18,\n2018. https://visibledata.co.uk/posts/2018-02-18-recipes-for-learning."
  },
  {
    "objectID": "posts/2018-02-07_where-s-the-love-for-htmlwidgets/index.html",
    "href": "posts/2018-02-07_where-s-the-love-for-htmlwidgets/index.html",
    "title": "Where’s the love for htmlwidgets?",
    "section": "",
    "text": "So why do I assume you haven’t heard of them?\nWell, they’re hardly ever spoken about or searched for! Hadley Wickham is a huge promoter of the benefits of visualisation in data science2 but has only ever mentioned htmlwidgets on Twitter explicitly 3 times prior to February 2018. And just compare the Google Search Trends for tidyverse and htmlwidgets, there’s desperately little love for htmlwidgets.\n\n\n\n\n\n\n\n\n\n\n\nThe crux of the problem really is that htmlwidgets is a developer tool. The actual packages that folks build using it are really popular! htmlwidgets spiritual home is htmlwidgets.org which is maintained by the RStudio folks, where they keep a list of featured htmlwidgets libraries available oin CRAN. Here’s a quick overview of these libraries (as of February 2018), with the following notes:\n\nso.tag contains the tag(s) used on StackOverflow for questions relating to this package\nmention.htmlwidgets is TRUE if the website for the package clearly mentions in simple language that it is a htmlwidgets library (or the outputs of the package are htmlwidgets)\nrglwidget was dropped from the list because it’s changed to rgl but this has not been reflected in the htmlwidgets.org website.\n\n\n\n\n\n\n\n\nIt’s really easy to get download figures for CRAN using the cranlogs library. In the table below I’ve collected the daily download figures for the featured_htmlwidget_libraries from the first date htmlwidgets was published until 2022-06-20 and then calculated the median daily downloads.\n\nlibrary(\"versions\")\n## Get first release date of htmlwidgets\n## This takes much longer than I'd like to run, so I've cheated\n# vhistory_htmlwidgets <- versions::available.versions(\"htmlwidgets\")\n# release_date_htlmlwidgets <- vhistory_htmlwidgets$htmlwidgets %>%\n#   filter(date == min(date)) %>%\n#   select(date) %>%\n#   .[[1]]\nrelease_date_htlmlwidgets <- lubridate::ymd(\"2014-12-09\")\n\nlibrary(\"cranlogs\")\ncran_downloads <- featured_htmlwidget_libraries %>%\n  select(package) %>%\n  pmap(function(package, ...) {\n    cran_downloads(packages = package, from = \"2014-12-09\", to = Sys.Date())\n  }) %>%\n  bind_rows() %>%\n  as_tibble()\n\nmedian_daily_downloads <- cran_downloads %>%\n  filter(count > 0) %>%\n  filter(package != \"htmlwidgets\") %>%\n  group_by(package) %>%\n  summarise(median.downloads = median(count)) %>%\n  arrange(desc(median.downloads))\n\ndt_median_daily_downloads <- median_daily_downloads %>%\n  datatable(\n    options = list(dom = \"t\", pageLength = 13),\n    colnames = c(\"htmlwidget library\", \"Median daily downloads (excluding zero download days)\")\n  )\nframeWidget(dt_median_daily_downloads)\n\n\n\n\n\n\nThe top 6 libraries all have an median of more than 100 daily downloads, is that impressive? Well, I don’t have any feel for that unfortunately. The cranlogs tells us about top 100 downloaded packages, but these download figures are going to suffer from a highly positively skewed distribution. It would be fun to investigate this distribution and come back to this question in the future. All I want to show here is that the htmlwidgets libraries are really popular, but that there’s little exposure for the htmlwidgets branding.\nI can demonstrate the growing popularity of the libraries with a timeseries chart using dygraphs, I’ve chosen to perform a simple 28-day moving average on the data as there is a lot of inter-day noise:\n\nlibrary(\"dygraphs\")\n## CSS hackery to place the legend nicely and account for the yellow colour in the \"Paired\"\n## colour scheme\ndygraph_css <- tempfile(fileext = \".css\")\ndygraph_css_file <- file(dygraph_css)\nwriteLines(\".dygraph-title {color: navy;font-weight: bold;}\n.dygraph-axis-label {font-weight: bold;}\n.dygraph-legend {background: #ebebeb !important;left: 100px !important;top: 100px !important;}\n.dygraph {background: #ebebeb !important;}\", dygraph_css_file)\nclose(dygraph_css_file)\n \ncran_downloads_28ma <- cran_downloads %>%\n  filter(package != \"htmlwidgets\") %>%\n  group_by(package) %>%\n  mutate(mean.count = rollmean(\n    count, k = 28, na.pad = TRUE,\n    align = \"right\"\n  )) %>%\n  ungroup() %>%\n  replace_na(list(mean.count = 0)) %>%\n  select(-count)\n\ncran_downloads_28ma %>%\n  mutate(package = fct_relevel(package, median_daily_downloads$package)) %>%\n  spread(package, mean.count) %>%\n  # filter(date > ymd(\"2017-08-01\")) %>%\n  timetk::tk_xts(date_var = date) %>%\n  dygraph(main = \"Featured htmlwidgets library downloads from CRAN<br> <span style='font-size:18px;'>(28 day moving average)</span>\",\n          width = \"100%\") %>%\n  dyCSS(dygraph_css) %>%\n  dyOptions(colors = RColorBrewer::brewer.pal(12, \"Paired\")) %>%\n  dyAxis(\"y\", label = \"CRAN Downloads\") %>%\n  dyLegend(labelsSeparateLines = TRUE)\n\n\n\n\n\n\nI hoped to get a good idea on Twitter about how many folks were using htmlwidgets libraries without knowing that’s what they were. Unfortuntely, the reach of my poll was very small with the tweet only appearing in a timeline 307 times. I’d really like to collect more data about this and return to this topic in the future.\n\n\n#rstats #dataviz folks! Do you use any of these libraries:leaflet, dygraph, plotly, rbokeh, highcharter, visNetwork, or DT?If so, did you know these are examples of htmlwidgets?(See https://t.co/DtVNBSFJZA if you didn't know!)\n\n— Charlie Joey Hadley (@charliejhadley) February 5, 2018\n\n\n\nPromoting the htmlwidgets brand\nI think it’s shame that there isn’t more exposure for the htmlwidgets brand and underlying technology. I meet a lot of folks in my training courses who have simply never heard the term, despite perhaps using leaflet or plotly already. If these folks knew they were htmlwidgets they could perhaps find more tools for visualising their data in new and interesting ways.\nThese are the things I’d really love to see:\n\nhtmlwidgets hexsticker: Hexstickers are awesome and really popular amongst useRs, it would be awesome to have a dedicated sticker for the htmlwidgets brand.\nhtmlwidgets mentioned in plain simple human language in the following places; GitHub README pages, Package DESCRIPTION files, and in the welcome pages for dedicated package websites. Of course, displaying the htmlwidgets hexsticker on the README would really help promote the brand\nTalk about htmlwidgets more in Tweets and other comms channels from RStudio.\n\nI kind of get the feeling that htmlwidgets is currently in stealth mode and I’m not sure why. Maybe RStudio are waiting until the (exciting!) crosstalk features are added to more htmlwidgets libraries? But that feels like a 2.0 feature!\nI’m going to do everything I can in my teaching, evangelism and general talking about R at parties to promote htmlwidgets and I’d love to see others do the same. I also plan to come back later and revisit this subject, analysing other datasets (like StackOverflow questions) and seeing if anything has changed.\nI will finish off with a little bit of self-promotion. I have a 5 hour+ overview of htmlwidgets on LinkedIn Learning and Lynda.com that you might find useful:\n\n\n\nR: Interactive Visualizations with htmlwidgets by Martin Hadley\n\n\n\n\n\n\nFootnotes\n\n\nThanks to Joe Chung for highlighting @ramnath_vaidya and @timelyportfolio’s contributions on Twitter link.↩︎\nCitation needed.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hadley2018,\n  author = {Charlie Hadley},\n  title = {Where’s the Love for Htmlwidgets?},\n  date = {2018-02-07},\n  url = {https://visibledata.co.uk/posts/2018-02-07_where-s-the-love-for-htmlwidgets},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCharlie Hadley. 2018. “Where’s the Love for Htmlwidgets?”\nFebruary 7, 2018. https://visibledata.co.uk/posts/2018-02-07_where-s-the-love-for-htmlwidgets."
  },
  {
    "objectID": "posts/2022-06-15_verbatim-sql-in-dbplyr-pipe-chains/index.html",
    "href": "posts/2022-06-15_verbatim-sql-in-dbplyr-pipe-chains/index.html",
    "title": "Writing verbatim SQL in pipe chains using {dbplyr}",
    "section": "",
    "text": "Here’s a comparison of code using {DBI} and {dbplyr}\n\nUsing {dbplyr}\n\n\nlibrary(tidyverse)\nlibrary(fivethirtyeight)\nlibrary(dbplyr)\n\nbechdel_tbl <- memdb_frame(bechdel, .name = \"bechdel\")\n\nbechdel_tbl %>% \n  filter(year < 2000) %>% \n  group_by(clean_test) %>% \n  summarise(mean_budget_2013 = mean(budget_2013))\n\n# Source:   SQL [5 x 2]\n# Database: sqlite 3.38.5 [:memory:]\n  clean_test mean_budget_2013\n  <chr>                 <dbl>\n1 dubious           59202525.\n2 men               45896513.\n3 notalk            57963237.\n4 nowomen           51891839.\n5 ok                44990138.\n\n\n\nUsing {DBI}\n\n\nlibrary(tidyverse)\nlibrary(fivethirtyeight)\nlibrary(DBI)\n\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\ncopy_to(con, bechdel, \"bechdel\")\n\ntbl(con, \"bechdel\") %>% \n  filter(year < 2000) %>% \n  group_by(clean_test) %>% \n  summarise(mean_budget_2013 = mean(budget_2013))\n\n# Source:   SQL [5 x 2]\n# Database: sqlite 3.38.5 [:memory:]\n  clean_test mean_budget_2013\n  <chr>                 <dbl>\n1 dubious           59202525.\n2 men               45896513.\n3 notalk            57963237.\n4 nowomen           51891839.\n5 ok                44990138.\n\n\nNow, that’s great for when we want to write new queries. But I wanted to go in the other direction. There’s lots of existing SQL code that I don’t want to re-write into R code, so it would be nice to write verbatim SQL within the pipe syntax. I couldn’t figure it out from the documentation so asked the question on Twitter, and as is often the case Garrick Aden-Buie gave me the solution.\n\ntweetrmd::tweet_embed(\"https://twitter.com/grrrck/status/1537082896124616705\")\n\nif `bechdel_con` is a connection object then you canlibrary(dbplyr)bechdel_con %>% tbl(sql(\"SELECT title FROM bechdel\"))if `bechdel_tbl` is a remote tibble then you canbechdel_tbl %>%  remote_con() %>%  tbl(\"SELECT title FROM bechdel\")— Garrick Aden-Buie (@grrrck) June 15, 2022\n\n\n\nLet’s write that into a code chunk:\n\nbechdel_tbl %>% \n  remote_con() %>% \n  tbl(sql(\"SELECT `clean_test`, AVG(`budget_2013`) AS `mean_budget_2013`\nFROM `bechdel`\nWHERE (`year` < 2000.0)\nGROUP BY `clean_test`\"))\n\n# Source:   SQL [5 x 2]\n# Database: sqlite 3.38.5 [:memory:]\n  clean_test mean_budget_2013\n  <chr>                 <dbl>\n1 dubious           59202525.\n2 men               45896513.\n3 notalk            57963237.\n4 nowomen           51891839.\n5 ok                44990138.\n\n\nNote that we’re still connected to the database here. If we want to return a local tibble we need to use collect() which means our entire workflow is as follows:\n\nbechdel_tbl <- memdb_frame(bechdel, .name = \"bechdel\")\n\nbechdel_tbl %>% \n  remote_con() %>% \n  tbl(sql(\"SELECT `clean_test`, AVG(`budget_2013`) AS `mean_budget_2013`\nFROM `bechdel`\nWHERE (`year` < 2000.0)\nGROUP BY `clean_test`\")) %>% \n  collect()\n\n\n\n# A tibble: 5 × 2\n  clean_test mean_budget_2013\n  <chr>                 <dbl>\n1 dubious           59202525.\n2 men               45896513.\n3 notalk            57963237.\n4 nowomen           51891839.\n5 ok                44990138.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hadley2022,\n  author = {Charlie Hadley},\n  title = {Writing Verbatim {SQL} in Pipe Chains Using \\{Dbplyr\\}},\n  date = {2022-06-15},\n  url = {https://visibledata.co.uk/posts/2022-06-15_verbatim-sql-in-dbplyr-pipe-chains},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCharlie Hadley. 2022. “Writing Verbatim SQL in Pipe Chains Using\n{Dbplyr}.” June 15, 2022. https://visibledata.co.uk/posts/2022-06-15_verbatim-sql-in-dbplyr-pipe-chains."
  },
  {
    "objectID": "posts/2022-06-20_overriding-individual-code-chunk-options/index.html",
    "href": "posts/2022-06-20_overriding-individual-code-chunk-options/index.html",
    "title": "Overriding individual code chunk options",
    "section": "",
    "text": "I’ve just moved this entire blog from {blogdown} to {quarto}. It was mostly simple but for one of the blogposts I really wanted a way to override individual code chunk options. Let’s make a toy .Rmd document to demonstrate this:\n---\noutput: html_document\n---\n  \n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, eval=FALSE)\n```\n\n\nThis code chunk has `eval=TRUE` which I want to override\n\n```{r, eval=TRUE}\n2 + 2\n```\n\nThe default behaviour for code chunks is set in the setup chunk as eval=FALSE\nThere’s an individual code chunk with eval=TRUE\nI want to override the individual code chunk behaviour.\n\nI tweeted about this and Garrick Aden-Buie replied linking me to the documentation for option hooks. I was really pleased to then figure out this solution:\n---\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_hooks$set(force_not_eval = function(value) {\n  value$eval &lt;- FALSE\n  value\n}\n)\nknitr::opts_chunk$set(force_not_eval = TRUE)\n```\n\n```{r, eval=TRUE}\n2 + 2\n```\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hadley2022,\n  author = {Hadley, Charlie},\n  title = {Overriding Individual Code Chunk Options},\n  date = {2022-06-20},\n  url = {https://visibledata.co.uk/posts/2022-06-20_overriding-individual-code-chunk-options},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHadley, Charlie. 2022. “Overriding Individual Code Chunk\nOptions.” June 20, 2022. https://visibledata.co.uk/posts/2022-06-20_overriding-individual-code-chunk-options."
  },
  {
    "objectID": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html",
    "href": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html",
    "title": "Creating an R-Ladies quarto format",
    "section": "",
    "text": "It’s the day after rstudio::conf(2022), and like many folks in the audience I’m very excited about the future of {quarto}. (LINKS). To help me better understand the differences between RMarkdown and quarto I decided I’d convert Alison Hill’s R-Ladies {xaringan} theme into a quarto format\nHowever, what I realised is that first I needed to replicate the {xaringan} format as closely as possible in {quarto}.\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\ntribble(\n  ~feature, ~description,\n  \"Code chunk  appearance\", \"Different. Using accessibility features of quarto\"\n)\n\n# A tibble: 1 × 2\n  feature                description                                      \n  &lt;chr&gt;                  &lt;chr&gt;                                            \n1 Code chunk  appearance Different. Using accessibility features of quarto"
  },
  {
    "objectID": "Untitled/Untitled.html",
    "href": "Untitled/Untitled.html",
    "title": "Untitled",
    "section": "",
    "text": "Notice that the yaml at the beginning of this file includes a latex_engine which will be used when creating a pdf document.\nIt also includes a mainfont setting called DejaVu Sans. This is not the only font that will work to produce the spark graphs. However, it is a free font available through the extrafont package. If you have not installed extrafont you should do so using the normal package installation procedures. You should then make sure that the desired font is installed.\nThe code below will not run automatically when you knit, instead you should run it in the console.\ninstall.packages(c(\"extrafont\"))\nfont_install(\"DejaVu Sans\")\nIf there are any difficulties please read the extrafont documentation."
  },
  {
    "objectID": "Untitled/Untitled.html#experimenting-with-rendering",
    "href": "Untitled/Untitled.html#experimenting-with-rendering",
    "title": "Untitled",
    "section": "Experimenting with rendering",
    "text": "Experimenting with rendering\n\nlibrary(knitr)\nlibrary(skimr)\n\nTry knitting this document to PDF, HTML, doc or any other format you wish to try. You will notice that there are slight differences between them. To understand the impact of the engine and font choices you should experiment with different options.\nThe first example shows what printing the basic skim function looks like. You can try knitting to different formats to see how it changes.\n\nskim(iris)\n\n\nData summary\n\n\nName\niris\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSpecies\n0\n1\nFALSE\n3\nset: 50, ver: 50, vir: 50\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSepal.Length\n0\n1\n5.84\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nSepal.Width\n0\n1\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\n\n\nPetal.Length\n0\n1\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\nPetal.Width\n0\n1\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃\n\n\n\n\n\nIt is possible that the histograms will not print in all of the formats.\nUnfortunately this is outside the control of the skimr team because it relates to the operating system you are using, fonts installed, and locale."
  },
  {
    "objectID": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#how-to-use-the-rladies-quarto-theme",
    "href": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#how-to-use-the-rladies-quarto-theme",
    "title": "Creating an R-Ladies quarto format",
    "section": "How to use the rladies quarto theme",
    "text": "How to use the rladies quarto theme\nIn RMarkdown we use packages to create custom themes, and different output types. But in quarto we need to create quarto extensions so that these can be used by folks using quarto from Python, Julia or ObservableJS. There’s something else very different about quarto extensions:\n\nYou must install extensions into each project, unlike R packages that are installed globally.\n\nEvery time you want to use the format run this in the Terminal (instead of the R console):\n\nquarto install extension r-ladies/r-ladies-quarto"
  },
  {
    "objectID": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#recreating-xaringan-in-quarto",
    "href": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#recreating-xaringan-in-quarto",
    "title": "Creating an R-Ladies quarto format",
    "section": "Recreating {xaringan} in {quarto}",
    "text": "Recreating {xaringan} in {quarto}\nThe new {quarto} system makes use of reveal.js for creating HTML slides, whereas {xaringan} uses remark.js. Despite these libraries sounding similar, they’re very different. This means there’s a lot of styling that needs to be bought across into the new xaringan-quarto theme. But before we progress, let’s think about why we want to replicate the {xaringan} output type:\n\n{xaringan} slides look good!\n{xaringan} has lots of nice features for formatting slides\n\nSo in this new {quarto} format I want to replicate both the styles and the formatting features - as closely as possible, anyhow.\n\nAdding remark.js dependency.\nIt took me ages to figure out how to add a JavaScript dependency to a format. I eventually figured it out from looking at the grouped-tabsets extension. These are the steps we need to go through:\n\nWe need to use a .lua file to add our dependencies, add a file called remark-js.lua into the xaringan folder. Within this file we’re going to add our dependency:\n\nDiv = function(el)\n  if quarto.doc.isFormat(\"html:js\") then\n    quarto.doc.addHtmlDependency({\n        name = \"remarkjs\",\n        scripts = {\"remark-latest.min.js\"}\n      })\n  end\nend \n\nAdd the remark-latest.min.js file to the xaringan folder. In the future I’ll figure out how to get the hosted file, but for now I copy pasted the contents of https://remarkjs.com/downloads/remark-latest.min.js.\nIn the _extension.yml file we need to add our lua dependency, as part of our format. To do so we need to use the common key.\n\ncontributes:\n  formats:\n    revealjs:\n    common:\n       filters:\n           - remark-js.lua\n\n\nAdding .scss fles to formats\nNow we’ve got the remark.js loaded we can move onto setting up the {xaringan} CSS styles. However, there’s a little trick to it. {quarto} makes use of .scss files intead of normal .css files, but that’s to our advantage. By using .scss files we can take advantage of Bootstrap SASS variables which are very powerful, but I’m not sure how much we’ll make use of them here.\nWhen creating a .scss file we need to create at least one “layer boundary” otherwise we’ll get this error message:\nERROR: The file _extensions/xaringan/xaringan-default.scss doesn't contain at least one layer boundary (/*-- scss:defaults --*/, /*-- scss:rules --*/, /*-- scss:mixins --*/, /*-- scss:functions --*/, or /*-- scss:uses --*/)\nHere are the steps required to start-up our custom styling\n\nCreate a xaringan-default.scss file in the xaringan folder. We’ll set paragraph text to red so we can test it the styles are working:\n\n/*-- scss:rules --*/\n\np {\n  color: red;\n}\n\nUpdate the _extension.yml file to make use of the .scss file\n\ncontributes:\n  formats:\n    revealjs:\n       theme: [default, xaringan-default.scss]\n    common:\n       filters:\n           - remark-js.lua\n\n\nAdding a template presentation\nIt’s a good idea to add a template presentation file so folks know how to use the format. But it has another utility! We can render the template file within our RStudio project to continually test the theme without having to use the quarto cli.\nSo, let’s add template.qmd to the top-level directory of our project and some lorem ipsum text:\n---\ntitle: \"Xarignan Slides\"\nformat: xaringan-revealjs\n---\n\n\n\n## First slide\n\nThis paragraph text will be red!\nWe’re now going to iteratively add/remove content to this template while we build up the theme.\n\n\nLetter-boxing and slide scaling\n{xaringan} slides are very distinct because of the letter boxing affect from remark.js, and the size of the content rescales with the size of the browser windows. For a first attempt at replicating this, I’ve added some CSS in this commit. I also needed to add padding around the slide content, I’ve implemented this through the border of the <div> but it’s likely not the best solution.\nWhen it comes to replicating exactly how the slide aspect ratio changes, I’m not sure what to do. These are the options available for scaling the presentation size in reveal.js but at the moment I don’t understand how remark.js does its scaling, and so I can’t replicate it further.\n\n\n\n\n\n\n\nOption\nDescription\n\n\nmargin\nFactor of the display size that should remain empty around the content (defaults to 0.1)\n\n\nmin-scale\nSmallest possible scale to apply to content (defaults to 0.2)\n\n\nmax-scale\nLargest possible scale to apply to content (defaults to 2.0)\n\n\n\n\n\nHello world slide\nLet’s look at duplicating the styling of this slide from the {xaringan}` template:\n\nThere are four thing to look at:\n\nCode chunk formatting\nText and link formatting\nIncremental bullet points\nFootnotes\n\n\n\nCode chunk formatting\nIn {xaringan} the code chunk formatting is very flat. I’m choosing to deliberately use {quarto} code formatting as it improves accessibility and provides further customisation for formats that build on xaringan-quarto and for individual preferences. In the _extension.yml I’ve activated the github highlighting style.\ncontributes:\n  formats:\n    revealjs:\n       theme: [default, xaringan-default.scss, xaringan-default-fonts.scss]\n       highlight-style: github\n       \nYou could use this YAML option for customising all code chunks or individual chunks. The documentation explains how to completely customise chunk appearance and syntax highlighting.\n\n\nText and link formatting\nWe’re going to use the SASS link-color variable to change the colour of hyperlinks, but remember this must be placed in the defaults layer of the .scss file.\n/*-- scss:defaults --*/\n$link-color: rgb(249, 38, 114);\nIn {xaringan} the paragraph text is set to 20px, setting the $presentation-font-size-root to 20px in the .scss results in slides that scale similarly to {xaringan} slides.\n$presentation-font-size-root: 20px;"
  },
  {
    "objectID": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#adding-remark.js-dependency.",
    "href": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#adding-remark.js-dependency.",
    "title": "Creating an R-Ladies quarto format",
    "section": "Adding remark.js dependency.",
    "text": "Adding remark.js dependency.\nIt took me ages to figure out how to add a JavaScript dependency to a format. I eventually figured it out from looking at the grouped-tabsets extension. These are the steps we need to go through:\n\nWe need to use a .lua file to add our dependencies, add a file called remark-js.lua into the xaringan folder. Within this file we’re going to add our dependency:\n\nDiv = function(el)\n  if quarto.doc.isFormat(\"html:js\") then\n    quarto.doc.addHtmlDependency({\n        name = \"remarkjs\",\n        scripts = {\"remark-latest.min.js\"}\n      })\n  end\nend \n\nAdd the remark-latest.min.js file to the xaringan folder. In the future I’ll figure out how to get the hosted file, but for now I copy pasted the contents of https://remarkjs.com/downloads/remark-latest.min.js.\nIn the _extension.yml file we need to add our lua dependency, as part of our format. To do so we need to use the common key.\n\ncontributes:\n  formats:\n    revealjs:\n    common:\n       filters:\n           - remark-js.lua"
  },
  {
    "objectID": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#adding-.scss-fles-to-formats",
    "href": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#adding-.scss-fles-to-formats",
    "title": "Creating an R-Ladies quarto format",
    "section": "Adding .scss fles to formats",
    "text": "Adding .scss fles to formats\nNow we’ve got the remark.js loaded we can move onto setting up the {xaringan} CSS styles. However, there’s a little trick to it. {quarto} makes use of .scss files intead of normal .css files, but that’s to our advantage. By using .scss files we can take advantage of Bootstrap SASS variables which are very powerful, but I’m not sure how much we’ll make use of them here.\nWhen creating a .scss file we need to create at least one “layer boundary” otherwise we’ll get this error message:\nERROR: The file _extensions/xaringan/xaringan-default.scss doesn't contain at least one layer boundary (/*-- scss:defaults --*/, /*-- scss:rules --*/, /*-- scss:mixins --*/, /*-- scss:functions --*/, or /*-- scss:uses --*/)\nHere are the steps required to start-up our custom styling\n\nCreate a xaringan-default.scss file in the xaringan folder. We’ll set paragraph text to red so we can test it the styles are working:\n\n/*-- scss:rules --*/\n\np {\n  color: red;\n}\n\nUpdate the _extension.yml file to make use of the .scss file\n\ncontributes:\n  formats:\n    revealjs:\n       theme: [default, xaringan-default.scss]\n    common:\n       filters:\n           - remark-js.lua"
  },
  {
    "objectID": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#adding-a-template-presentation",
    "href": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#adding-a-template-presentation",
    "title": "Creating an R-Ladies quarto format",
    "section": "Adding a template presentation",
    "text": "Adding a template presentation\nIt’s a good idea to add a template presentation file so folks know how to use the format. But it has another utility! We can render the template file within our RStudio project to continually test the theme without having to use the quarto cli.\nSo, let’s add template.qmd to the top-level directory of our project and some lorem ipsum text:\n---\ntitle: \"Xarignan Slides\"\nformat: xaringan-revealjs\n---\n\n\n\n## First slide\n\nThis paragraph text will be red!\nWe’re now going to iteratively add/remove content to this template while we build up the theme."
  },
  {
    "objectID": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#letter-boxing-and-slide-scaling",
    "href": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#letter-boxing-and-slide-scaling",
    "title": "Creating an R-Ladies quarto format",
    "section": "Letter-boxing and slide scaling",
    "text": "Letter-boxing and slide scaling\n{xaringan} slides are very distinct because of the letter boxing affect from remark.js, and the size of the content rescales with the size of the browser windows. For a first attempt at replicating this, I’ve added some CSS in this commit. I also needed to add padding around the slide content, I’ve implemented this through the border of the &lt;div&gt; but it’s likely not the best solution.\nWhen it comes to replicating exactly how the slide aspect ratio changes, I’m not sure what to do. These are the options available for scaling the presentation size in reveal.js but at the moment I don’t understand how remark.js does its scaling, and so I can’t replicate it further.\n\n\n\n\n\n\n\nOption\nDescription\n\n\nmargin\nFactor of the display size that should remain empty around the content (defaults to 0.1)\n\n\nmin-scale\nSmallest possible scale to apply to content (defaults to 0.2)\n\n\nmax-scale\nLargest possible scale to apply to content (defaults to 2.0)"
  },
  {
    "objectID": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#hello-world-slide",
    "href": "posts/2022-07-29_creating-an-r-ladies-quarto-format/index.html#hello-world-slide",
    "title": "Creating an R-Ladies quarto format",
    "section": "Hello world slide",
    "text": "Hello world slide\nLet’s look at duplicating the styling of this slide from the {xaringan} template:\n\nThere are four thing to look at:\n\nCode chunk and in-line code formatting\nText and link formatting\nIncremental bullet points\nFootnotes\n\n\nCode chunk formatting\nIn {xaringan} the code chunk formatting is very flat. I’m choosing to deliberately use {quarto} code formatting as it improves accessibility and provides further customisation for formats that build on xaringan-quarto and for individual preferences. In the _extension.yml I’ve activated the github highlighting style.\ncontributes:\n  formats:\n    revealjs:\n       theme: [default, xaringan-default.scss, xaringan-default-fonts.scss]\n       highlight-style: github\n       \nYou could use this YAML option for customising all code chunks or individual chunks. The documentation explains how to completely customise chunk appearance and syntax highlighting.\nNow let’s look at the inline code formatting. In the original xaringan format this text is the same colour as paragraph text but printed in a mono spaced font. In this first alpha release of the format I’ve chosen to change this to purple, but this probably won’t be popular.\n\n\nText and link formatting\nWe’re going to use the SASS link-color variable to change the colour of hyperlinks, but remember this must be placed in the defaults layer of the .scss file.\n/*-- scss:defaults --*/\n$link-color: rgb(249, 38, 114);\nIn {xaringan} the paragraph text is set to 20px, setting the $presentation-font-size-root to 28px in the .scss results in slides that scale similarly to {xaringan} slides.\n$presentation-font-size-root: 28px;\nWith {xaringan} we’re free to use whatever level of heading we like for slides. In fact, it’s fairly common to use lower-level headings for longer slide titles. But, in {quarto} we can only start slides with level 2 headings. I’ve taken the heading sizes from {xaringan} and added them to the styles:\n$heading2Size: 55px;\n$heading3Size: 45px;\n$heading4Size: 35px;\nNote that these are still scaled with the size of the browser window.\n\n\nIncremental bullet points\nIn {xaringan} we can make anything incremental with two dashes, eg --. But in the {quarto} reveal.js format we need to create a &lt;div&gt; using the incremental class. There’s two ways of doing this, either with explicit &lt;div&gt;&lt;/div&gt; tags or with matching pairs of :::.\n\n\nFootnotes\nIn {quarto} we use ^[footnote text] notation to add footnotes that are then numbered at the bottom of the slide. In my opinion, the default styling for quarto is better than {xaringan} so I’m sticking with it."
  },
  {
    "objectID": "posts/2022-08-03_how-i-learned-to-stop-replicating-everything-from-xaringan-and-love-quarto/index.html",
    "href": "posts/2022-08-03_how-i-learned-to-stop-replicating-everything-from-xaringan-and-love-quarto/index.html",
    "title": "How I learned to stop replicating everything from {xaringan} and love Quarto",
    "section": "",
    "text": "Since the end of the rstudio::conf(2022) I’ve been working on replicating the {xaringan} RMarkdown presentation format in {quarto}. If you’ve never seen these slides before, here’s the self-documenting template from Yihui Xie (click inside the slides and use your arrow keys).\nThese slides are really quite beautiful. This blog post is about trying to replicate the letter box effect in Quarto. And - specifically - how I learned to stop trying to replicate it because I couldn’t [in a meaningful way]. I’d be very happy to be shown how to replicate it1.\nI was very pleased to see that Beatriz Milz has already ported the R-Ladies {xaringan} theme to Quarto. I will follow up this post with another where I walk through how I’m implementing the default {xaringan} theme sans letterboxing, because I think there’s some useful things to point out."
  },
  {
    "objectID": "posts/2022-08-03_how-i-learned-to-stop-replicating-everything-from-xaringan-and-love-quarto/index.html#replicating-the-letterbox-effect",
    "href": "posts/2022-08-03_how-i-learned-to-stop-replicating-everything-from-xaringan-and-love-quarto/index.html#replicating-the-letterbox-effect",
    "title": "How I learned to stop replicating everything from {xaringan} and love Quarto",
    "section": "Replicating the letterbox effect",
    "text": "Replicating the letterbox effect\nThis repository contains my attempt to replicate the letterbox affect. You can see the template slides below:\n\n\nIt looks like it works! There’s even the nice drop shadow. But extending it to fully replicate how {xaringan} wasn’t something I could figure out. This is the CSS I used to create the letterbox effect:\n\n.quarto-light {\n  background-color: #D8D8D3;\n}\n\n.slides {\n  background-color: #fff;\n  border-width: 20px 80px 20px 80px;\n  border-style: solid;\n  border-color: #fff;\n  box-shadow: rgb(136, 136, 136) 0px 0px 30px 0px;\n}\n\nNow let’s take a look at how the reveal.js format constructs slides:\n\n<body class=\"quarto-light\">\n  <div class=\"slides\">\n    <section id=\"title-slide\"> </section>\n    <section id=\"hello-world\" </section>\n    <section id=\"hello-ninja\" </section>\n    <section id=\"remark.js\" </section>\n  </div>\n</div>\n\nThe grey background behind the slides is applied to the entire body element - which is fine! But notice that all slides are contained within one <div> element. This means we can’t supply a different background to individual slides. That’s a problem for trying to replicate {xaringan} because one of the most obvious features is that some slides have a different background colour - implemented through the inverse class.\nIn {xaringan} that makes sense because of how remark.js constructs slides (notice how the top slide (the title slide) has the inverse class):\n\n<div class=\"remark-slides-area\">\n  <div class=\"remark-slide-container inverse\">\n    <div class=\"remark-slide-scaler\"></div>\n  </div>\n  <div class=\"remark-slide-container\">\n    <div class=\"remark-slide-scaler\"></div>\n  </div>\n</div>\n\nI spent ages (and ages) experimenting with different CSS and SASS variables before I figured this out. I wish I’d have taken a step back and looked at how the HTML was constructed before spending so much time on this. But I did end up learning lots of things while trying and failing. I’ll document those findings in the next blogpost where I will completely ignore the letterboxing.\nI did spend a little bit of time thinking about a complicated way to achieve this. It involved using JavaScript to add parent <div> elements to each <section> but my initial goal was to learn about Quarto extensions not JavaScript. Additionally, it became obvious that adding this letterbox effect broke the PDF printing of the presentation.\nSo unless anyone else comes up with a solution, this is how I learned to stop replicating everything from {xaringan} and love Quarto."
  },
  {
    "objectID": "posts/2022-08-04_how-i-learned-to-stop-replicating-everything-from-xaringan-and-love-quarto/index.html",
    "href": "posts/2022-08-04_how-i-learned-to-stop-replicating-everything-from-xaringan-and-love-quarto/index.html",
    "title": "How I learned to stop replicating everything from {xaringan} and love Quarto",
    "section": "",
    "text": "Since the end of the rstudio::conf(2022) I’ve been working on replicating the {xaringan} RMarkdown presentation format in {quarto}. If you’ve never seen these slides before, here’s the self-documenting template from Yihui Xie (click inside the slides and use your arrow keys).\nThese slides are really quite beautiful. This blog post is about trying to replicate the letter box effect in Quarto. And - specifically - how I learned to stop trying to replicate it because I couldn’t [in a meaningful way]. I’d be very happy to be shown how to replicate it1.\nI was very pleased to see that Beatriz Milz has already ported the R-Ladies {xaringan} theme to Quarto. I will follow up this post with another where I walk through how I’m implementing the default {xaringan} theme sans letterboxing, because I think there’s some useful things to point out."
  },
  {
    "objectID": "posts/2022-08-04_how-i-learned-to-stop-replicating-everything-from-xaringan-and-love-quarto/index.html#replicating-the-letterbox-effect",
    "href": "posts/2022-08-04_how-i-learned-to-stop-replicating-everything-from-xaringan-and-love-quarto/index.html#replicating-the-letterbox-effect",
    "title": "How I learned to stop replicating everything from {xaringan} and love Quarto",
    "section": "Replicating the letterbox effect",
    "text": "Replicating the letterbox effect\nThis repository contains my attempt to replicate the letterbox affect. You can see the template slides below:\n\n\nIt looks like it works! There’s even the nice drop shadow. But extending it to fully replicate how {xaringan} wasn’t something I could figure out. This is the CSS I used to create the letterbox effect:\n\n.quarto-light {\n  background-color: #D8D8D3;\n}\n\n.slides {\n  background-color: #fff;\n  border-width: 20px 80px 20px 80px;\n  border-style: solid;\n  border-color: #fff;\n  box-shadow: rgb(136, 136, 136) 0px 0px 30px 0px;\n}\n\nNow let’s take a look at how the reveal.js format constructs slides:\n\n<body class=\"quarto-light\">\n  <div class=\"slides\">\n    <section id=\"title-slide\"> </section>\n    <section id=\"hello-world\" </section>\n    <section id=\"hello-ninja\" </section>\n    <section id=\"remark.js\" </section>\n  </div>\n</div>\n\nThe grey background behind the slides is applied to the entire body element - which is fine! But notice that all slides are contained within one <div> element. This means we can’t supply a different background to individual slides. That’s a problem for trying to replicate {xaringan} because one of the most obvious features is that some slides have a different background colour - implemented through the inverse class.\nIn {xaringan} that makes sense because of how remark.js constructs slides (notice how the top slide (the title slide) has the inverse class):\n\n<div class=\"remark-slides-area\">\n  <div class=\"remark-slide-container inverse\">\n    <div class=\"remark-slide-scaler\"></div>\n  </div>\n  <div class=\"remark-slide-container\">\n    <div class=\"remark-slide-scaler\"></div>\n  </div>\n</div>\n\nI spent ages (and ages) experimenting with different CSS and SASS variables before I figured this out. I wish I’d have taken a step back and looked at how the HTML was constructed before spending so much time on this. But I did end up learning lots of things while trying and failing. I’ll document those findings in the next blogpost where I will completely ignore the letterboxing.\nI did spend a little bit of time thinking about a complicated way to achieve this. It involved using JavaScript to add parent <div> elements to each <section> but my initial goal was to learn about Quarto extensions not JavaScript. Additionally, it became obvious that adding this letterbox effect broke the PDF printing of the presentation.\nSo unless anyone else comes up with a solution, this is how I learned to stop replicating everything from {xaringan} and love Quarto."
  },
  {
    "objectID": "posts/2022-09-29_change-rmd-template/index.html",
    "href": "posts/2022-09-29_change-rmd-template/index.html",
    "title": "Visible Data",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hadley,\n  author = {Charlie Hadley},\n  url = {https://visibledata.co.uk/posts/2022-09-29_change-rmd-template},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCharlie Hadley. n.d. https://visibledata.co.uk/posts/2022-09-29_change-rmd-template."
  },
  {
    "objectID": "posts/2023-08-21_restart/index.html",
    "href": "posts/2023-08-21_restart/index.html",
    "title": "2023-08-21 Restart in less than an hour",
    "section": "",
    "text": "It’s been over a year since I last published an update to this blog and six months since I’ve loaded up the project.\nCan I get a post written and published succesfully within an hour? It’s been 10mins so far."
  },
  {
    "objectID": "posts/2023-08-21_restart/index.html#finally-using-.by-in-mutate-and-summarise",
    "href": "posts/2023-08-21_restart/index.html#finally-using-.by-in-mutate-and-summarise",
    "title": "2023-08-21 Restart in less than an hour",
    "section": "Finally using .by in mutate() and summarise()",
    "text": "Finally using .by in mutate() and summarise()\nI initially hated the choice to add .by to mutate() and summarise() but slowly realised I was projecting how I learned on a completely different tidyverse ecosystem.\nIt’s actually really beautiful to use .by like this:\n\nlibrary(tidyverse)\n\ngss_party_by_marital &lt;- gss_cat %&gt;% \n  summarise(n_in_subcategory = n(), .by = c(partyid, marital)) %&gt;% \n  mutate(n_in_category = sum(n_in_subcategory), .by = partyid) %&gt;% \n  mutate(partyid = fct_reorder(partyid, n_in_category))\n\ngss_party_by_marital %&gt;% \n  ggplot(aes(x = n_in_subcategory,\n             y = partyid,\n             fill = marital)) +\n  geom_col()"
  },
  {
    "objectID": "posts/2023-08-21_restart/index.html#highcharter-and-factors",
    "href": "posts/2023-08-21_restart/index.html#highcharter-and-factors",
    "title": "2023-08-21 Restart in less than an hour",
    "section": "{highcharter} and factors",
    "text": "{highcharter} and factors\nI’ve intended to write an article about {highcharter} and how it doesn’t rely on factors for years. In lieu of that, here’s a quick 101.\n{highcharter} doesn’t respect the factor ordering:\n\nlibrary(highcharter)\n\ngss_party_by_marital %&gt;% \n  hchart(\n    type = \"bar\",\n    hcaes(\n      y = n_in_subcategory,\n      x = partyid,\n      group = marital\n    )\n  ) %&gt;% \n  hc_plotOptions(series = list(stacking = \"normal\"))\n\n\n\n\n\nThe issue is that {highcharter} needs explicitly empty levels, which the complete() function provides.\n\ngss_completed_data &lt;- gss_cat %&gt;% \n  summarise(n_in_subcategory = n(), .by = c(partyid, marital)) %&gt;% \n  complete(partyid,\n           marital,\n           fill = list(n_in_subcategory = 0)) %&gt;% \n  mutate(n_in_category = sum(n_in_subcategory), .by = partyid) %&gt;% \n  mutate(partyid = fct_reorder(partyid, n_in_category)) %&gt;% \n  arrange(desc(n_in_category))\n\n\ngss_completed_data %&gt;% \n  hchart(\n    type = \"bar\",\n    hcaes(\n      y = n_in_subcategory,\n      x = partyid,\n      group = marital\n    )\n  ) %&gt;% \n  hc_plotOptions(series = list(stacking = \"normal\"))"
  },
  {
    "objectID": "posts/2023-08-21_restart/index.html#and-thats-a-wrap",
    "href": "posts/2023-08-21_restart/index.html#and-thats-a-wrap",
    "title": "2023-08-21 Restart in less than an hour",
    "section": "And that’s a wrap",
    "text": "And that’s a wrap\nIn 50mins I updated R, RStudio, my packages and got the blog deploying successfully."
  },
  {
    "objectID": "posts/2024-02-27_empty-checkboxes/null checkboxgroup check.html",
    "href": "posts/2024-02-27_empty-checkboxes/null checkboxgroup check.html",
    "title": "Visible Data",
    "section": "",
    "text": "ui &lt;- fluidPage( checkboxGroupInput( “checks”, “Check boxes”, choices = month.name, selected = month.name[1] ), actionButton(“search”, “Search”), uiOutput(“ui_stuff”) )\nserver &lt;- function(input, output, session) { reactive_checkboxes &lt;- reactiveValues(checks = c())\nobserveEvent(input$checks, { print(“change”)\nreactive_checkboxes$checks &lt;- input$checks\n})\nobserveEvent(inputsearch, {\n    if (is.null(inputchecks)) { showModal(modalDialog( title = “Invalid search conditions!”, div(p(“You must select at least one theme to filter the data.”), p(“Click dismiss and your previously selected set of themes will be re-selected.”) )))\n  updateCheckboxGroupInput(session,\n                           \"checks\",\n                           selected = reactive_checkboxes$checks)\n  \n}\n})\noutput$ui_stuff &lt;- renderUI({ “dummy element here” }) }\nshinyApp(ui, server)\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hadley,\n  author = {Hadley, Charlie},\n  url = {https://visibledata.co.uk/posts/2024-02-27_empty-checkboxes/null checkboxgroup check.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHadley, Charlie. n.d. https://visibledata.co.uk/posts/2024-02-27_empty-checkboxes/null\ncheckboxgroup check.html."
  },
  {
    "objectID": "posts/2024-10-21_30daychartchallenge-2022-waffles/index.html",
    "href": "posts/2024-10-21_30daychartchallenge-2022-waffles/index.html",
    "title": "From the archive: 30DayChartChallenge 2022 Waffles",
    "section": "",
    "text": "This post has been rescued from a side-project blog that I’ve disposed of and slowly reposting content from. In this post I made this waffle chart:\n\n\nThis chart was built for the “Part-to-whole” prompt in the 2022 #30DayChartChallenge. I wanted to make a waffle chart and also experiment with combining multiple charts together, I’ve still not really committed to {cowplot} or {patchwork} and when I began 2022’s 30DayChartChallenge one of my big goals was to get stuck into comparing them. Although I didn’t get too far with that, it was still constructive to build this chart.\nI’m very fond of the Bechdel dataset from {fivethirtyeight} so this was a simple choice for chart. If you don’t know, knitr::kable() is a really cheap way to make a Markdown table that looks fine in Quarto and RMarkdown:\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\nbechdel_count_clean_test &lt;- bechdel %&gt;% \n  count(clean_test)\n\nlabel_tests_vec &lt;- c(\"nowomen\" = \"No women in the movie\", \"notalk\" = \"No talking between women\", \"men\" = \"Talk about men\", \"dubious\" = \"Dubiously passes\", \"ok\" = \"Passed the test!\")\nlabel_tests_tib &lt;- enframe(label_tests_vec) %&gt;% \n  rename(clean_test = name,\n         test_label = value)\n\n\nbechdel_count_clean_test &lt;- bechdel %&gt;% \n  count(clean_test, sort = TRUE) %&gt;% \n  left_join(label_tests_tib) %&gt;% \n  mutate(test_label = fct_reorder(test_label, n))\n\nbechdel_count_clean_test %&gt;% \n  knitr::kable()\n\n\n\n\nclean_test\nn\ntest_label\n\n\n\n\nok\n803\nPassed the test!\n\n\nnotalk\n514\nNo talking between women\n\n\nmen\n194\nTalk about men\n\n\ndubious\n142\nDubiously passes\n\n\nnowomen\n141\nNo women in the movie\n\n\n\n\n\nIn 2022 it looked like {waffle} package was the best for making waffle charts with {ggplot2}. I got side tracked thinking about the math behind making aesthetically pleasing but eventually settled on the most square possible chart via round(sqrt(nrow(bechdel))).\n\nlibrary(waffle)\nlibrary(ggpomological)\nlibrary(hrbrthemes)\nlibrary(ggtext)\ngg_waffle_bechdel &lt;- bechdel_count_clean_test %&gt;% \n  ggplot(aes(fill = test_label, values = n)) +\n  geom_waffle(n_rows = round(sqrt(nrow(bechdel))), size = 0.33, colour = \"white\", flip = TRUE) +\n  coord_equal()\ngg_waffle_bechdel\n\n\n\n\n\n\n\n\nLet’s beautify:\n\nbechdel_count_clean_test %&gt;% \n  ggplot(aes(fill = test_label, values = n)) +\n  geom_waffle(n_rows = round(sqrt(nrow(bechdel))), size = 0.33, colour = \"white\", flip = TRUE) +\n  scale_fill_pomological(name = \"\") +\n  coord_equal() +\n  theme_ipsum_rc(grid=\"\") +\n  theme_enhance_waffle()\n\n\n\n\n\n\n\n\nLet’s do this by decade. Again, it was fun to experiment with different n_rows() values. It feels to me like the best option is to use prime factors of the decade with most observations, which became 7 * 2.\n\nbechdel_by_decade &lt;- bechdel %&gt;% \n  mutate(decade = floor(year / 10) * 10) %&gt;% \n  count(decade, clean_test, sort = TRUE) %&gt;% \n  left_join(label_tests_tib) %&gt;% \n  mutate(test_label = fct_reorder(test_label, n))\n\nbechdel_by_decade %&gt;% \n  ggplot(aes(fill = clean_test, values = n)) +\n  geom_waffle(n_rows = 7 * 2, size = 0.33, colour = \"white\", flip = TRUE) +\n  facet_wrap(~decade, nrow = 1, strip.position = \"bottom\") +\n  scale_fill_pomological(name = \"\") +\n  coord_equal() +\n  theme_ipsum_rc(grid=\"\") +\n  theme_enhance_waffle()\n\n\n\n\n\n\n\n\nWhile that’s an interesting way to look at the data I abandoned this as I wanted something slightly more squarish for the eventual {cowplot} so I made a waffle per decade and added some title text.\n\ngg_bechdel_decade_prop &lt;- bechdel_by_decade %&gt;% \n  mutate(test_label = fct_relevel(test_label, label_tests_tib$test_label)) %&gt;% \n  ggplot(aes(fill = test_label, values = n)) +\n  geom_waffle(n_rows = 10, size = 0.33, colour = \"white\", flip = TRUE, show.legend = TRUE, make_proportional = TRUE) +\n  facet_wrap(~decade, nrow = 2, strip.position = \"bottom\") +\n  scale_fill_ipsum(name = \"\") +\n  labs(subtitle = paste(\"{waffle} charts are pretty fun!\",\n                      \"&lt;br&gt;\",\n                      \"&lt;b&gt;Square waffles&lt;/b&gt;\",\n                      \"&lt;br&gt;\",\n                      str_wrap(\"For the squarest waffles use this code: round(sqrt(nrow(data))). It'd be fun if this was an optional argument for geom_waffle()\", 20),\n                      \"&lt;br&gt;\",\n                      \"&lt;b&gt;Proportional Faceted waffles&lt;/b&gt;\",\n                      \"&lt;br&gt;\",\n                      \"With proportional faceting n_rows should be a factor of 10. If not proportional, it feels like the best option is finding prime factors for the largest group. Consider using numbers::primeFactors() for that\",\n                      \"&lt;br&gt;\",\n                      \"&lt;b&gt;Paragraphs of text&lt;/b&gt;\",\n                      \"&lt;br&gt;\",\n                      \"I've not really used {patchwork} or {cowplot} to add paragraphs of text. I'm always really impressed by folks who do have these nicely displayed summaries in their charts. So that's something I'm going to experiment with a lot this edition of #30DaysChartChallenge\",\n                      \"&lt;br&gt;\"\n  )) +\n  coord_equal() +\n  theme_ipsum_rc(grid=\"\") +\n  theme_enhance_waffle() +\n  theme(\n    # plot.subtitle = element_markdown(lineheight = 1.25, hjust = 0, size = 13),\n        plot.subtitle = element_textbox_simple(\n      size = 10,\n      padding = margin(2, 2, 2, 2),\n      margin = margin(0, 0, 2, 0),\n      lineheight = 1.25\n    ),\n        text = element_text(colour = \"#242c28\", family = \"Arvo\"))\n\ngg_bechdel_decade_prop\n\n\n\n\n\n\n\n\nThese were then assembled together with {cowplot}:\n\nlibrary(cowplot)\n\ngg_bechdel_waffle &lt;- bechdel_count_clean_test %&gt;% \n  ggplot(aes(fill = test_label, values = n)) +\n  geom_waffle(n_rows = round(sqrt(nrow(bechdel))), size = 0.33, colour = \"white\", flip = TRUE, show.legend = TRUE) +\n  scale_fill_pomological(name = \"\") +\n  guides(fill = guide_legend(nrow = 2)) +\n  coord_equal() +\n  theme_ipsum_rc(grid=\"\") +\n  theme_enhance_waffle() +\n  theme(legend.position = \"bottom\")\n\ncw_bechdel_waffles &lt;- plot_grid(\n  ggdraw() + \n  draw_label(\n    \"#30DayChartChallenge 2022-04-01 Part to Whole: Waffles and the Bechdel Test\",\n    fontface = 'bold',\n    x = 0,\n    hjust = 0\n  ) +\n  theme(\n    plot.margin = margin(0, 0, -10, 10),\n    plot.title = element_text(family = \"Arvo\")\n  ),\n  plot_grid(gg_bechdel_waffle,\n  gg_bechdel_decade_prop,\n  nrow = 1,\n  rel_widths = c(2, 3)),\n  ncol= 1,\n  rel_heights = c(0.05, 1)\n)\n\ncw_bechdel_waffles %&gt;% \n  ggsave(quarto_here(\"cw_bechdel_waffles.png\"),\n         .,\n         width = 18,\n         height = 8)\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hadley2024,\n  author = {Hadley, Charlie},\n  title = {From the Archive: {30DayChartChallenge} 2022 {Waffles}},\n  date = {2024-10-21},\n  url = {https://visibledata.co.uk/posts/2024-10-21_30daychartchallenge-2022-waffles},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHadley, Charlie. 2024. “From the Archive: 30DayChartChallenge 2022\nWaffles.” October 21, 2024. https://visibledata.co.uk/posts/2024-10-21_30daychartchallenge-2022-waffles."
  },
  {
    "objectID": "posts/2024-10-21_30daychartchallenge-2022-emoji-standard-deviation-chart/index.html",
    "href": "posts/2024-10-21_30daychartchallenge-2022-emoji-standard-deviation-chart/index.html",
    "title": "From the archive: 30DayChartChallenge 2022 Pictogram",
    "section": "",
    "text": "This post has been rescued from a side-project blog that I’ve disposed of and slowly reposting content from. In this post I created one of my favourite charts:\n\n\nThis chart was built for the “Pictogram” prompt in the 2022 #30DayChartChallenge. It’s a tired trope to see a normal distribution called something like the “para-normal distribution” with a ghost, I then thought about emoji, 5-sigma and then this idea came about.\nTo begin I created a dotplot and was lucky enough for my seed to generate a single outlier on the right-side:\n\n\nCode\nlibrary(tidyverse)\n\ndata_hist &lt;- {\n  set.seed(1)\n  tibble(\n    x = rnorm(500,\n              mean = 100,\n              sd = 8)\n  )\n}\n\nsd_of_data &lt;- sd(data_hist$x)\nmean_of_data &lt;- mean(data_hist$x)\n\ngg_hist_dot_plot &lt;- data_hist %&gt;%\n  ggplot(aes(x = x)) +\n  geom_dotplot(binwidth = 1, method = \"dotdensity\", dotsize = 1, position = \"dodge\", binpositions=\"bygroup\")\ngg_hist_dot_plot\n\n\n\n\n\n\n\n\n\nIn case you’ve not seen it, you can extract all of the coordinates of geoms from a chart via ggplot_build(). Which I’m going to use to add the emoji in place of the dots in the chart:\nI then extracted the coordinates of the dots via ggplot_build()\n\nbuild_gg &lt;- ggplot_build(gg_hist_dot_plot)\n\nNow I use {emo} to create a tibble containing data for my emojis:\n\nlibrary(\"emo\")\n\nvec_emojis &lt;- c(\"3+\" = emo::ji(\"scream\"), \"3\" = emo::ji(\"fearful\"), \"2\" = emo::ji(\"confused\"), \"1\" = emo::ji(\"grin\"))\n\ndata_emoji_positions &lt;- build_gg$data %&gt;%\n  as.data.frame() %&gt;%\n  as_tibble() %&gt;%\n  select(x, xmin, xmax, y, stackpos) %&gt;%\n  mutate(sds_from_mean = case_when(\n    x &lt; ( mean_of_data - 3 * sd_of_data ) ~ \"3+\",\n    x &lt;= ( mean_of_data - 2 * sd_of_data ) ~ \"3\",\n    x &lt;= ( mean_of_data - 1 * sd_of_data ) ~ \"2\",\n    x &lt;= ( mean_of_data + 1 * sd_of_data ) ~ \"1\",\n    x &lt;= ( mean_of_data + 2 * sd_of_data ) ~ \"2\",\n    x &lt;= ( mean_of_data + 3 * sd_of_data ) ~ \"3\",\n    x &gt; ( mean_of_data + 3 * sd_of_data )~ \"3+\"\n  )) %&gt;%\n  mutate(emoji_symbol = vec_emojis[sds_from_mean])\n\nTo visualise the standard deviation components of the Normal distribution I created two utility functions:\n\nhist_function &lt;- function(x){dnorm(x, mean = 100, sd = 8) * 700}\n\ngeom_dnorm_fill &lt;- function(xlim, fill_color, alpha = 1){\n\n  geom_area(stat = \"function\",\n            fun = hist_function,\n            fill = fill_color,\n            alpha = alpha,\n            xlim = xlim)\n\n}\n\ndata_emoji_positions %&gt;%\n  ggplot(aes(x, y = stackpos)) +\n  geom_dnorm_fill(c(mean_of_data - 4 * sd_of_data,\n                    mean_of_data + 4 * sd_of_data),\n                  viridis::viridis(5)[4]) +\n  geom_dnorm_fill(c(mean_of_data - 3 * sd_of_data,\n                    mean_of_data + 3 * sd_of_data),\n                  viridis::viridis(5)[3]) +\n  geom_dnorm_fill(c(mean_of_data - 2 * sd_of_data,\n                    mean_of_data + 2 * sd_of_data),\n                  viridis::viridis(5)[2]) +\n  geom_dnorm_fill(c(mean_of_data - 1 * sd_of_data,\n                    mean_of_data + 1 * sd_of_data),\n                  viridis::viridis(5)[1])\n\n\n\n\n\n\n\n\nI then combined everything together into the original version of the chart in 2022:\n\n\nCode\nlibrary(hrbrthemes)\nlibrary(ggtext)\n\ngg_emoji_histogram &lt;- data_emoji_positions %&gt;%\n  ggplot(aes(x, y = stackpos)) +\ngeom_dnorm_fill(c(mean_of_data - 4 * sd_of_data,\n                  mean_of_data + 4 * sd_of_data),\n                viridis::viridis(5)[4]) +\ngeom_dnorm_fill(c(mean_of_data - 3 * sd_of_data,\n                  mean_of_data + 3 * sd_of_data),\n                viridis::viridis(5)[3]) +\ngeom_dnorm_fill(c(mean_of_data - 2 * sd_of_data,\n                  mean_of_data + 2 * sd_of_data),\n                viridis::viridis(5)[2]) +\ngeom_dnorm_fill(c(mean_of_data - 1 * sd_of_data,\n                  mean_of_data + 1 * sd_of_data),\n                viridis::viridis(5)[1]) +\ngeom_richtext(data = tibble(label = str_glue(\"{emo::ji('grin')} represent &lt;span style='color:{viridis::viridis(5)[1]};font-weight:bold'&gt;68% of the data&lt;/span&gt;\",\n                                             \"&lt;br&gt;\",\n                                             \"{emo::ji('confused')} represent &lt;span style='color:{viridis::viridis(5)[2]};font-weight:bold'&gt;95% of the data&lt;/span&gt;\",\n                                             \"&lt;br&gt;\",\n                                             \"{emo::ji('fearful')} represent &lt;span style='color:{viridis::viridis(5)[3]};font-weight:bold'&gt;99.7% of the data&lt;/span&gt;\",\n                                             \"&lt;br&gt;\",\n                                             \"{emo::ji('scream')} represent &lt;span style='color:{viridis::viridis(5)[4]};font-weight:bold'&gt;the rest of the data&lt;/span&gt;\")),\n              aes(label = label),\n              family = \"DM Sans\",\n              label.padding = unit(c(0.5, 0.5, 0.5, 0.5), \"lines\"),\n              label.margin = unit(c(0, 0, 0, 0), \"lines\"),\n              size = 7,\n              x = 110,\n              y = 30,\n              hjust=0) +\n  geom_curve(\n    data = tibble(x = 120, y = 15, xend = max(data_emoji_positions$x) - 1, yend = 1),\n    aes(x, y, yend = yend, xend = xend),\n    # x = 120, y = 10, xend = max(data_emoji_positions$x), yend = 1,\n    # data = df,\n    arrow = arrow(length = unit(0.03, \"npc\")),\n    curvature = 0.2,\n    angle = 90\n  ) +\n  # geom_point() +\n  geom_richtext(data = tibble(x = 115,\n                y = 15,\n                label = \"Yup, that's me. You're probably&lt;br&gt;wondering how I ended up in&lt;br&gt; this situation...\"),\n                aes(x, y, label = label),\n                label.colour = \"transparent\",\n                hjust = 0,\n                family = \"Comic Sans MS\",\n                label.padding = unit(c(0, 0.25, 0.25, 0.25), \"lines\"),\n                label.margin = unit(c(0, 0, 0, 0), \"lines\"),\n                size = 6) +\n  geom_richtext(aes(label = emoji_symbol),\n                size = 5,\n                fill = NA,\n                label.color = NA, # remove background and outline\n                label.padding = grid::unit(rep(0, 4), \"pt\")) +\n  scale_y_continuous(expand = expansion(add = c(0, 5))) +\n  NULL +\n  labs(title = \"#30DayChartChallenge 2022-04-02 Pictogram: Emojis and Standard Deviations\",\n       subtitle = \"With apologies to everyone I present the &lt;i&gt;Emoji Standard Deviation Chart&lt;/i&gt;&lt;br&gt;Author: @charliejhadley\",\n       x = \"\",\n       y = \"\") +\ntheme_ipsum_rc(grid=\"\") +\n  theme(plot.title = element_text(family = \"Arvo\"),\n        plot.subtitle = element_markdown(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank())\n\ngg_emoji_histogram %&gt;%\n  ggsave(quarto_here(\"gg_emoji_histogram.png\"),\n         .,\n         width = 18,\n         height = 10,\n         bg = \"white\")\n\n\nWhen I recovered this post in 2024 I remembered about {geomtextpath} for adding labels to curves and moved the title to bound the distribution:\n\n\nCode\nlibrary(\"geomtextpath\")\n\ngg_emoji_histogram_fancy &lt;- data_emoji_positions %&gt;%\n  ggplot(aes(x, y = stackpos)) +\ngeom_dnorm_fill(c(mean_of_data - 4 * sd_of_data,\n                  mean_of_data + 4 * sd_of_data),\n                viridis::viridis(5)[4]) +\ngeom_dnorm_fill(c(mean_of_data - 3 * sd_of_data,\n                  mean_of_data + 3 * sd_of_data),\n                viridis::viridis(5)[3]) +\ngeom_dnorm_fill(c(mean_of_data - 2 * sd_of_data,\n                  mean_of_data + 2 * sd_of_data),\n                viridis::viridis(5)[2]) +\ngeom_dnorm_fill(c(mean_of_data - 1 * sd_of_data,\n                  mean_of_data + 1 * sd_of_data),\n                viridis::viridis(5)[1]) +\n  geom_richtext(data = tibble(label = str_glue(\"{emo::ji('grin')} represent &lt;span style='color:{viridis::viridis(5)[1]};font-weight:bold'&gt;68% of the data&lt;/span&gt;\",\n                                             \"&lt;br&gt;\",\n                                             \"{emo::ji('confused')} represent &lt;span style='color:{viridis::viridis(5)[2]};font-weight:bold'&gt;95% of the data&lt;/span&gt;\",\n                                             \"&lt;br&gt;\",\n                                             \"{emo::ji('fearful')} represent &lt;span style='color:{viridis::viridis(5)[3]};font-weight:bold'&gt;99.7% of the data&lt;/span&gt;\",\n                                             \"&lt;br&gt;\",\n                                             \"{emo::ji('scream')} represent &lt;span style='color:{viridis::viridis(5)[4]};font-weight:bold'&gt;the rest of the data&lt;/span&gt;\")),\n              aes(label = label),\n              family = \"DM Sans\",\n              label.padding = unit(c(0.5, 0.5, 0.5, 0.5), \"lines\"),\n              label.margin = unit(c(0, 0, 0, 0), \"lines\"),\n              size = 7,\n              x = 110,\n              y = 30,\n              hjust=0) +\n  geom_curve(\n    data = tibble(x = 120, y = 15, xend = max(data_emoji_positions$x) - 1, yend = 1),\n    aes(x, y, yend = yend, xend = xend),\n    # x = 120, y = 10, xend = max(data_emoji_positions$x), yend = 1,\n    # data = df,\n    arrow = arrow(length = unit(0.03, \"npc\")),\n    curvature = 0.2,\n    angle = 90\n  ) +\n  # geom_point() +\n  geom_richtext(data = tibble(x = 115,\n                y = 15,\n                label = \"Yup, that's me. You're probably&lt;br&gt;wondering how I ended up in&lt;br&gt; this situation...\"),\n                aes(x, y, label = label),\n                label.colour = \"transparent\",\n                hjust = 0,\n                family = \"Comic Sans MS\",\n                label.padding = unit(c(0, 0.25, 0.25, 0.25), \"lines\"),\n                label.margin = unit(c(0, 0, 0, 0), \"lines\"),\n                size = 6) +\n  geom_richtext(aes(label = emoji_symbol),\n                data = data_emoji_positions,\n                size = 5,\n                fill = NA,\n                label.color = NA, # remove background and outline\n                label.padding = grid::unit(rep(0, 4), \"pt\")) +\n  geom_textpath(\n    # data = filter(data_emoji_positions, x &lt;= 100),\n    stat = \"function\", \n    vjust = -0.5,\n    linewidth = 0,\n    fun = function(x){ifelse(between(x, 86, 100), dnorm(x, mean = 100, sd = 8) * 700, NA)},\n    label = \"#30DayChartChallenge 2022\",\n    # family = \"Comic Sans MS\",\n    rich = TRUE,\n    size = 8\n  ) +\n  geom_textpath(\n    # data = filter(data_emoji_positions, x &lt;= 100),\n    stat = \"function\", \n    vjust = -0.2,\n    linewidth = 0,\n    fun = function(x){ifelse(between(x, 100, 117), dnorm(x, mean = 100, sd = 8) * 700, NA)},\n    label = \"Pictogram: Emojis and Standard Deviations\",\n    family = \"Comic Sans MS\",\n    rich = TRUE,\n    size = 8\n  ) +\n  scale_y_continuous(expand = expansion(add = c(0, 5))) +\n  labs(x = \"\",\n       y = \"\",\n       caption = \"Author: @charliejhadley\") +\ntheme_ipsum_rc(grid=\"\") +\n  theme(plot.title = element_text(family = \"Arvo\"),\n        plot.subtitle = element_markdown(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        plot.caption.position = \"plot\")\n\ngg_emoji_histogram_fancy %&gt;% \n  ggsave(quarto_here(\"gg_emoji_histogram_fancy.png\"),\n         .,\n         width = 18,\n         height = 10,\n         bg = \"white\")\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hadley2024,\n  author = {Hadley, Charlie},\n  title = {From the Archive: {30DayChartChallenge} 2022 {Pictogram}},\n  date = {2024-10-21},\n  url = {https://visibledata.co.uk/posts/2024-10-21_30daychartchallenge-2022-emoji-standard-deviation-chart},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHadley, Charlie. 2024. “From the Archive: 30DayChartChallenge 2022\nPictogram.” October 21, 2024. https://visibledata.co.uk/posts/2024-10-21_30daychartchallenge-2022-emoji-standard-deviation-chart."
  },
  {
    "objectID": "posts/2024-10-24_Highchart-doesnt-like-empty-factors/index.html",
    "href": "posts/2024-10-24_Highchart-doesnt-like-empty-factors/index.html",
    "title": "{highcharts} doesn’t like empty factors (and the .by argument is quite nice, actually)",
    "section": "",
    "text": "Let’s demonstrate how {ggplot2} handles ordering of factor levels that are empty:\nlibrary(tidyverse)\n\ngss_party_by_marital &lt;- gss_cat %&gt;% \n  summarise(n_in_subcategory = n(), .by = c(partyid, marital)) %&gt;% \n  mutate(n_in_category = sum(n_in_subcategory), .by = partyid) %&gt;% \n  mutate(partyid = fct_reorder(partyid, n_in_category))\n\ngss_party_by_marital %&gt;% \n  ggplot(aes(x = n_in_subcategory,\n             y = partyid,\n             fill = marital)) +\n  geom_col() +\n  scale_fill_viridis_d(option = \"A\", direction = 1) +\n  guides(fill = guide_legend(reverse = TRUE))\nThat’s a nicely ordered stacked bar chart! We’ve got the bars going from big to small, even though the final two categories don’t contain all of the marital states.\ngss_party_by_marital %&gt;% \n  filter(partyid %in% c(\"No answer\", \"Don't know\")) %&gt;% \n  knitr::kable()\n\n\n\n\npartyid\nmarital\nn_in_subcategory\nn_in_category\n\n\n\n\nNo answer\nMarried\n60\n154\n\n\nNo answer\nDivorced\n29\n154\n\n\nNo answer\nNever married\n35\n154\n\n\nNo answer\nWidowed\n16\n154\n\n\nNo answer\nSeparated\n9\n154\n\n\nNo answer\nNo answer\n5\n154\n\n\nDon’t know\nMarried\n1\n1\nLet’s throw this into {highcharter} and we see just a complete mess.\nlibrary(highcharter)\n\ngss_party_by_marital %&gt;% \n  hchart(\n    type = \"bar\",\n    hcaes(\n      y = n_in_subcategory,\n      x = partyid,\n      group = marital\n    )\n  ) %&gt;% \n  hc_plotOptions(series = list(stacking = \"normal\"))"
  },
  {
    "objectID": "posts/2024-10-24_Highchart-doesnt-like-empty-factors/index.html#finally-using-.by-in-mutate-and-summarise",
    "href": "posts/2024-10-24_Highchart-doesnt-like-empty-factors/index.html#finally-using-.by-in-mutate-and-summarise",
    "title": "{highcharts} doesn’t like empty factors (and the .by argument is quite nice, actually)",
    "section": "Finally using .by in mutate() and summarise()",
    "text": "Finally using .by in mutate() and summarise()\nI initially hated the choice to add .by to mutate() and summarise() but slowly realised I was projecting how I learned on a completely different tidyverse ecosystem.\nIt’s actually really beautiful to use .by like this:\n\nlibrary(tidyverse)\n\ngss_party_by_marital &lt;- gss_cat %&gt;% \n  summarise(n_in_subcategory = n(), .by = c(partyid, marital)) %&gt;% \n  mutate(n_in_category = sum(n_in_subcategory), .by = partyid) %&gt;% \n  mutate(partyid = fct_reorder(partyid, n_in_category))\n\ngss_party_by_marital %&gt;% \n  ggplot(aes(x = n_in_subcategory,\n             y = partyid,\n             fill = marital)) +\n  geom_col()"
  },
  {
    "objectID": "posts/2024-10-24_Highchart-doesnt-like-empty-factors/index.html#highcharter-and-factors",
    "href": "posts/2024-10-24_Highchart-doesnt-like-empty-factors/index.html#highcharter-and-factors",
    "title": "{highcharts} doesn’t like empty factors (and the .by argument is quite nice, actually)",
    "section": "{highcharter} and factors",
    "text": "{highcharter} and factors\nI’ve intended to write an article about {highcharter} and how it doesn’t rely on factors for years. In lieu of that, here’s a quick 101.\n{highcharter} doesn’t respect the factor ordering:\n\nlibrary(highcharter)\n\ngss_party_by_marital %&gt;% \n  hchart(\n    type = \"bar\",\n    hcaes(\n      y = n_in_subcategory,\n      x = partyid,\n      group = marital\n    )\n  ) %&gt;% \n  hc_plotOptions(series = list(stacking = \"normal\"))\n\n\n\n\n\nThe issue is that {highcharter} needs explicitly empty levels, which the complete() function provides.\n\ngss_completed_data &lt;- gss_cat %&gt;% \n  summarise(n_in_subcategory = n(), .by = c(partyid, marital)) %&gt;% \n  complete(partyid,\n           marital,\n           fill = list(n_in_subcategory = 0)) %&gt;% \n  mutate(n_in_category = sum(n_in_subcategory), .by = partyid) %&gt;% \n  mutate(partyid = fct_reorder(partyid, n_in_category)) %&gt;% \n  arrange(desc(n_in_category))\n\n\ngss_completed_data %&gt;% \n  hchart(\n    type = \"bar\",\n    hcaes(\n      y = n_in_subcategory,\n      x = partyid,\n      group = marital\n    )\n  ) %&gt;% \n  hc_plotOptions(series = list(stacking = \"normal\"))"
  },
  {
    "objectID": "posts/2024-10-24_Highchart-doesnt-like-empty-factors/index.html#and-thats-a-wrap",
    "href": "posts/2024-10-24_Highchart-doesnt-like-empty-factors/index.html#and-thats-a-wrap",
    "title": "{highcharts} doesn’t like empty factors (and the .by argument is quite nice, actually)",
    "section": "And that’s a wrap",
    "text": "And that’s a wrap\nIn 50mins I updated R, RStudio, my packages and got the blog deploying successfully."
  },
  {
    "objectID": "posts/2024-10-24_Highchart-doesnt-like-empty-factors/index.html#using-complete-and-talking-about-that-.by-argument",
    "href": "posts/2024-10-24_Highchart-doesnt-like-empty-factors/index.html#using-complete-and-talking-about-that-.by-argument",
    "title": "{highcharts} doesn’t like empty factors (and the .by argument is quite nice, actually)",
    "section": "Using complete() and talking about that .by argument",
    "text": "Using complete() and talking about that .by argument\nYou might have missed it in my code above, but I used groups without group_by() or ungroup()! Back in late 2022 there was a suggestion that the grouping functions gain a .by argument which was added as an experimental feature.\nI initially I really didn’t like it. But, look how neat this is. I’ve highlighted the summarise() and mutate() lines that make use of the .by argument, it saves having to call group_by() and ungroup()… and just to be clear complete() is filling in empty factor levels with an explicit value of 0.\n\ngss_completed_data &lt;- gss_cat %&gt;% \n  summarise(n_in_subcategory = n(), .by = c(partyid, marital)) %&gt;% \n  complete(partyid,\n           marital,\n           fill = list(n_in_subcategory = 0)) %&gt;% \n  mutate(n_in_category = sum(n_in_subcategory), .by = partyid) %&gt;% \n  mutate(partyid = fct_reorder(partyid, n_in_category)) %&gt;% \n  arrange(desc(n_in_category))\n\ngss_completed_data %&gt;% \n  filter(partyid %in% c(\"No answer\", \"Don't know\")) %&gt;% \n  knitr::kable()\n\n\n\n\npartyid\nmarital\nn_in_subcategory\nn_in_category\n\n\n\n\nNo answer\nNo answer\n5\n154\n\n\nNo answer\nNever married\n35\n154\n\n\nNo answer\nSeparated\n9\n154\n\n\nNo answer\nDivorced\n29\n154\n\n\nNo answer\nWidowed\n16\n154\n\n\nNo answer\nMarried\n60\n154\n\n\nDon’t know\nNo answer\n0\n1\n\n\nDon’t know\nNever married\n0\n1\n\n\nDon’t know\nSeparated\n0\n1\n\n\nDon’t know\nDivorced\n0\n1\n\n\nDon’t know\nWidowed\n0\n1\n\n\nDon’t know\nMarried\n1\n1\n\n\n\n\n\nNow those empty factor levels are filled we can throw it back into {highcharter}. Make sure to interact with the chart, and click on the legend items to see just how beautiful {highcharter} is.\n\ngss_completed_data %&gt;% \n  hchart(\n    type = \"bar\",\n    hcaes(\n      y = n_in_subcategory,\n      x = partyid,\n      group = marital\n    )\n  ) %&gt;% \n  hc_plotOptions(series = list(stacking = \"normal\"))"
  },
  {
    "objectID": "posts/2024-10-24_Highchart-doesnt-like-empty-factors/index.html#a-necessary-aside-on-licensing",
    "href": "posts/2024-10-24_Highchart-doesnt-like-empty-factors/index.html#a-necessary-aside-on-licensing",
    "title": "{highcharts} doesn’t like empty factors (and the .by argument is quite nice, actually)",
    "section": "A necessary aside on licensing",
    "text": "A necessary aside on licensing\n{highcharter} is a wrapper for the incredible https://highcharts.com/ JavaScript framework. It is not free to use {highcharter} in commercial projects, but there are free license options for personal projects and educational usage."
  },
  {
    "objectID": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#footnotes",
    "href": "posts/2019-03-25-shiny-modules-for-useful-controls/index.html#footnotes",
    "title": "Shiny modules for useful controls",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI was interested in how important file size warnings are in tutorials, my Twitter poll revealed that most folks wanted a warning for files bigger than 10Mb↩︎"
  },
  {
    "objectID": "posts/2024-10-28_bordering-country-graph/index.html",
    "href": "posts/2024-10-28_bordering-country-graph/index.html",
    "title": "Bordering countries graph",
    "section": "",
    "text": "In this post we’re going to build up this graph which shows all countries with at least one border and the land border connections that remain after removing countries with a single border. Along the way we’ll get to see guide_custom(), {tidygraph} and {ggraph}.\n\n\n\n\n\n\n\n\nI’ve been thinking about TidyTuesday datasets with country data and how it could be interesting to use country borders as a component of the chart making process. And what’s better than working on an actual TidyTuesday visualisation than getting distracted with something tangetial to it?\nWell, to ensure I have something that’s reproducible and easy to use I’ve added a tidygraph object to my utility package {cjhRutils}. If you’re following along with this post you would need to install the package with remotes::install_github(\"charliejhadley/cjhRutils\"). Now let’s take a look at the object:\n\nlibrary(\"tidyverse\")\nlibrary(\"tidygraph\")\nlibrary(\"cjhRutils\")\nlibrary(\"ggtext\")\n\nggraph_bordering_countries\n\n# A tbl_graph: 173 nodes and 608 edges\n#\n# A directed simple graph with 26 components\n#\n# A tibble: 173 × 8\n     id iso_a2 iso_a3 name                 name_long name_en region_wb continent\n  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;    \n1     1 AE     ARE    United Arab Emirates United A… United… Middle E… Asia     \n2     2 AF     AFG    Afghanistan          Afghanis… Afghan… South As… Asia     \n3     3 AL     ALB    Albania              Albania   Albania Europe &… Europe   \n4     4 AM     ARM    Armenia              Armenia   Armenia Europe &… Asia     \n5     5 AO     AGO    Angola               Angola    Angola  Sub-Saha… Africa   \n6     6 AQ     ATA    Antarctica           Antarcti… Antarc… Antarcti… Antarcti…\n# ℹ 167 more rows\n#\n# A tibble: 608 × 3\n   from    to border_region             \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                     \n1     1   120 Middle East & North Africa\n2     1   136 Middle East & North Africa\n3     2    34 Cross Region              \n# ℹ 605 more rows\n\n\nLet’s visualise the countries with at least one border then we end up with 4 connected components. Two of them only contain two countries, I think we should forget about them in the next chart.\n\n\nCode\nlibrary(\"ggraph\")\n\nset.seed(1)\nggraph_bordering_countries %&gt;% \n  activate(nodes) %&gt;%\n  mutate(node_degree = tidygraph::centrality_degree()) %&gt;%\n  filter(node_degree &gt; 0) %&gt;%\n  ggraph(layout = 'nicely') +\n  aes(colour = region_wb) +\n  geom_node_point() +\n  geom_edge_link(aes(colour = border_region))\n\n\n\n\n\n\n\n\n\nOkay! Now I want to look at only the nodes with at least one edge. The guide/legend for that chart is a little bit complicated. Let’s look at why:\n\nNodes are coloured by the continent the node belongs to.\nEdges are coloured by if the two nodes belong to the same continent.\nThere is a single node from “North America” but no edges with the border_region of “North America”\nThere are edges that need to be coloured “Cross Region” but no nodes with that colour.\n\nThe first 3 points are easy to deal with. I’ve chosen to generate colours from the &lt;coolors.co&gt; service, let’s make sure these colours can be ordered by the frequency with which they occur in the data (except for “Cross Region”). I’ll also dispose of the\n\n# https://coolors.co/448aff-1565c0-009688-8bc34a-ffc107-ff9800-f44336-ad1457\nvec_colours &lt;- c(\"Cross Region\" = \"#F44336\", \n  \"East Asia & Pacific\" = \"#1565C0\", \n  \"Europe & Central Asia\" = \"#009688\", \n\"Latin America & Caribbean\" = \"#8BC34A\", \"Middle East & North Africa\" = \"#FFC107\", \"North America\" = \"#FF9800\", \n\"South Asia\" = \"#448AFF\", \"Sub-Saharan Africa\" = \"#AD1457\")\n\nvec_order_borders &lt;- ggraph_bordering_countries %&gt;% \n  activate(edges) %&gt;% \n  as_tibble() %&gt;% \n  count(border_region, sort = TRUE) %&gt;% \n  pull(border_region)\n\nvec_colours &lt;- vec_colours[vec_order_borders]\n\nggraph_one_edge_plus &lt;- ggraph_bordering_countries %&gt;% \n  activate(nodes) %&gt;%\n  mutate(node_degree = tidygraph::centrality_degree()) %&gt;%\n  filter(node_degree &gt; 1)  %&gt;%\n  mutate(region_wb = fct_relevel(region_wb, vec_order_borders)) %&gt;% \n  activate(edges) %&gt;% \n  mutate(border_region = fct_expand(border_region, \"North America\"),\n         border_region = fct_relevel(border_region ))\n\nNow to handle the “Cross Border” edges. I’m going to solve this with something that’s new to me - geom_custom() was added in late 2023. It makes it so much easier to insert additional legends, which I used to achieve by hijacking alpha or another unused aesthetic in the chart.\n\nset.seed(1)\ngg_graph_countries &lt;- ggraph_one_edge_plus %&gt;% \n  ggraph(layout = 'nicely') +\n  aes(colour = region_wb) +\n  geom_edge_link(aes(colour = border_region), show.legend = FALSE, edge_width = 0.5) +\n    geom_node_point(aes(fill = region_wb),\n                  # show.legend = TRUE,\n                  colour = \"black\",\n                  size = 6,\n                  pch = 21) +\n  geom_node_text(aes(label = iso_a2),\n                 size = 2,\n                 colour = \"white\",\n                 fontface = \"bold\")  +\n  geom_curve(\n    data = tibble(x = 5.08 - 8.5, y = -4.96 , xend = 5.08 - 1.3, yend = -4.96 - 0.2),\n    aes(x, y, yend = yend, xend = xend),\n    inherit.aes = FALSE,\n    arrow = arrow(length = unit(0.01, \"npc\")),\n    curvature = 0.2,\n    angle = 90\n  ) +\n    geom_label(data = tibble(x = 5.08 - 8.5,\n                y = -4.96 - 0.5,\n                label = str_wrap(\"The US borders Canada and Mexico. But - Canada doesn't have two borders and therefore isn't included in the graph.\", 30)),\n                aes(x, y, label = label),\n                fill = colorspace::darken(\"#D8E4EA\"),\n                hjust = 0,\n               colour = \"black\",\n                inherit.aes = FALSE,\n                size = 4) +\n  scale_edge_colour_manual(values = vec_colours,\n                           drop=FALSE) +\n  scale_fill_manual(values = vec_colours,\n                      drop=FALSE) +\n    guides(custom = guide_custom(\n    title = \"Cross regional borders\",\n    grob = grid::linesGrob(x = unit(c(0, 5.4), \"cm\"),\n                           y = unit(c(0, 0), \"cm\"),\n                           gp = grid::gpar(col = '#F44336', lwd = 3.5))\n  ),\nfill = guide_legend(title = \"\")) +\n  labs(title = \"All countries that border at least two countries and their connections\",\n       x = \"\",\n       y = \"\",\n       caption = \"Author: charliejhadley\") +\n  theme_minimal(base_size = 16, base_family= \"Noah\") +\n  theme(legend.title = element_text(size = 15), \n        panel.grid = element_line(colour = \"#D8E4EA\"),\n        plot.title = element_text(family = \"Noah\"),\n        panel.grid.major.y = element_blank(),\n        axis.line = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        legend.text=element_text(size=15),\n        # legend.spacing.y = unit(2.0, \"cm\"),\n        legend.key.size = unit(1, \"cm\"),\n    legend.key = element_rect(color = NA, fill = NA),\n    plot.caption.position = \"plot\")\n\ngg_graph_countries %&gt;% \n  ggsave(quarto_here(\"gg_graph_countries.png\"),\n         .,\n         width = 4.25 * 3,\n         height = 3.4 * 3,\n         bg = \"#D8E4EA\")\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hadley2024,\n  author = {Hadley, Charlie},\n  title = {Bordering Countries Graph},\n  date = {2024-10-28},\n  url = {https://visibledata.co.uk/posts/2024-10-28_bordering-country-graph},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHadley, Charlie. 2024. “Bordering Countries Graph.” October\n28, 2024. https://visibledata.co.uk/posts/2024-10-28_bordering-country-graph."
  },
  {
    "objectID": "posts/2024-10-28_bordering-country-graph/index.html#connected-countries",
    "href": "posts/2024-10-28_bordering-country-graph/index.html#connected-countries",
    "title": "Bordering countries graph",
    "section": "Connected Countries",
    "text": "Connected Countries\nI’ve been thinking about TidyTuesday datasets with country data and how it could be interesting to use country borders as a component of the chart making process. And what’s better than working on an actual TidyTuesday visualisation than getting distracted with something tangential to it?\nIn my utility package {cjhRutils} I have tidygraph object containing the nodes and edges of the connected countries, the code can be found in this script. After loading the package (alongside {tidygrapph}) we can see our dataset:\n\nlibrary(\"tidyverse\")\nlibrary(\"tidygraph\")\nlibrary(\"cjhRutils\")\nlibrary(\"ggtext\")\n\nggraph_bordering_countries\n\n# A tbl_graph: 173 nodes and 608 edges\n#\n# A directed simple graph with 26 components\n#\n# A tibble: 173 × 8\n     id iso_a2 iso_a3 name                 name_long name_en region_wb continent\n  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;    \n1     1 AE     ARE    United Arab Emirates United A… United… Middle E… Asia     \n2     2 AF     AFG    Afghanistan          Afghanis… Afghan… South As… Asia     \n3     3 AL     ALB    Albania              Albania   Albania Europe &… Europe   \n4     4 AM     ARM    Armenia              Armenia   Armenia Europe &… Asia     \n5     5 AO     AGO    Angola               Angola    Angola  Sub-Saha… Africa   \n6     6 AQ     ATA    Antarctica           Antarcti… Antarc… Antarcti… Antarcti…\n# ℹ 167 more rows\n#\n# A tibble: 608 × 3\n   from    to border_region             \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                     \n1     1   120 Middle East & North Africa\n2     1   136 Middle East & North Africa\n3     2    34 Cross Region              \n# ℹ 605 more rows\n\n\nThe graph contains all countries with at least one connection, let’s filter the graph to only include countries with two border or more… and visualise that naively with {ggraph}\n\n\nCode\nlibrary(\"ggraph\")\n\nset.seed(1)\nggraph_bordering_countries %&gt;% \n  activate(nodes) %&gt;%\n  mutate(node_degree = tidygraph::centrality_degree()) %&gt;%\n  filter(node_degree &gt; 1) %&gt;%\n  ggraph(layout = 'nicely') +\n  geom_node_point(aes(colour = region_wb)) +\n  geom_edge_link(aes(colour = border_region))\n\n\n\n\n\n\n\n\n\nOkay! Now I want to look at only the nodes with at least one edge. The guide/legend for that chart is a little bit complicated. Let’s look at why:\n\nNodes are coloured by the continent the node belongs to.\nEdges are coloured by if the two nodes belong to the same continent.\nThere is a single node from “North America” but no edges with the border_region of “North America”\nThere are edges that need to be coloured “Cross Region” but no nodes with that colour.\n\nThe first 3 points are easy to deal with. I’ve chosen to generate colours from the &lt;coolors.co&gt; service, let’s make sure these colours can be ordered by the frequency with which they occur in the data (except for “Cross Region”). I’ll also dispose of the\n\n# https://coolors.co/448aff-1565c0-009688-8bc34a-ffc107-ff9800-f44336-ad1457\nvec_colours &lt;- c(\"Cross Region\" = \"#F44336\", \n  \"East Asia & Pacific\" = \"#1565C0\", \n  \"Europe & Central Asia\" = \"#009688\", \n\"Latin America & Caribbean\" = \"#8BC34A\", \"Middle East & North Africa\" = \"#FFC107\", \"North America\" = \"#FF9800\", \n\"South Asia\" = \"#448AFF\", \"Sub-Saharan Africa\" = \"#AD1457\")\n\nvec_order_borders &lt;- ggraph_bordering_countries %&gt;% \n  activate(edges) %&gt;% \n  as_tibble() %&gt;% \n  count(border_region, sort = TRUE) %&gt;% \n  pull(border_region)\n\nvec_colours &lt;- vec_colours[vec_order_borders]\n\nggraph_one_edge_plus &lt;- ggraph_bordering_countries %&gt;% \n  activate(nodes) %&gt;%\n  mutate(node_degree = tidygraph::centrality_degree()) %&gt;%\n  filter(node_degree &gt; 1)  %&gt;%\n  mutate(region_wb = fct_relevel(region_wb, vec_order_borders)) %&gt;% \n  activate(edges) %&gt;% \n  mutate(border_region = fct_expand(border_region, \"North America\"),\n         border_region = fct_relevel(border_region ))\n\nNow to handle the “Cross Border” edges. I’m going to solve this with something that’s new to me - geom_custom() was added in late 2023. It makes it so much easier to insert additional legends, which I used to achieve by hijacking alpha or another unused aesthetic in the chart.\n\nset.seed(1)\ngg_graph_countries &lt;- ggraph_one_edge_plus %&gt;% \n  ggraph(layout = 'nicely') +\n  aes(colour = region_wb) +\n  geom_edge_link(aes(colour = border_region), show.legend = FALSE, edge_width = 0.5) +\n    geom_node_point(aes(fill = region_wb),\n                  # show.legend = TRUE,\n                  colour = \"black\",\n                  size = 6,\n                  pch = 21) +\n  geom_node_text(aes(label = iso_a2),\n                 size = 2,\n                 colour = \"white\",\n                 fontface = \"bold\")  +\n  geom_curve(\n    data = tibble(x = 5.08 - 8.5, y = -4.96 , xend = 5.08 - 1.3, yend = -4.96 - 0.2),\n    aes(x, y, yend = yend, xend = xend),\n    inherit.aes = FALSE,\n    arrow = arrow(length = unit(0.01, \"npc\")),\n    curvature = 0.2,\n    angle = 90\n  ) +\n    geom_label(data = tibble(x = 5.08 - 8.5,\n                y = -4.96 - 0.5,\n                label = str_wrap(\"The US borders Canada and Mexico. But - Canada doesn't have two borders and therefore isn't included in the graph.\", 30)),\n                aes(x, y, label = label),\n                fill = colorspace::darken(\"#D8E4EA\"),\n                hjust = 0,\n               colour = \"black\",\n                inherit.aes = FALSE,\n                size = 4) +\n  scale_edge_colour_manual(values = vec_colours,\n                           drop=FALSE) +\n  scale_fill_manual(values = vec_colours,\n                      drop=FALSE) +\n    guides(custom = guide_custom(\n    title = \"Cross regional borders\",\n    grob = grid::linesGrob(x = unit(c(0, 5.4), \"cm\"),\n                           y = unit(c(0, 0), \"cm\"),\n                           gp = grid::gpar(col = '#F44336', lwd = 3.5))\n  ),\nfill = guide_legend(title = \"\")) +\n  labs(title = \"All countries that border at least two countries and their connections\",\n       x = \"\",\n       y = \"\",\n       caption = \"Author: charliejhadley\") +\n  theme_minimal(base_size = 16, base_family= \"Noah\") +\n  theme(legend.title = element_text(size = 15), \n        panel.grid = element_line(colour = \"#D8E4EA\"),\n        plot.title = element_text(family = \"Noah\"),\n        panel.grid.major.y = element_blank(),\n        axis.line = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        legend.text=element_text(size=15),\n        # legend.spacing.y = unit(2.0, \"cm\"),\n        legend.key.size = unit(1, \"cm\"),\n    legend.key = element_rect(color = NA, fill = NA),\n    plot.caption.position = \"plot\")\n\ngg_graph_countries %&gt;% \n  ggsave(quarto_here(\"gg_graph_countries.png\"),\n         .,\n         width = 4.25 * 3,\n         height = 3.4 * 3,\n         bg = \"#D8E4EA\")"
  },
  {
    "objectID": "posts/2024-10-28_bordering-country-graph/index.html#connected-countries-with-tidygraph-and-ggraph",
    "href": "posts/2024-10-28_bordering-country-graph/index.html#connected-countries-with-tidygraph-and-ggraph",
    "title": "Bordering countries graph",
    "section": "Connected Countries with tidygraph and ggraph",
    "text": "Connected Countries with tidygraph and ggraph\nI’ve been thinking about TidyTuesday datasets with country data and how it could be interesting to use country borders as a component of the chart making process. And what’s better than working on an actual TidyTuesday visualisation than getting distracted with something tangential to it?\nIn my utility package {cjhRutils} I have a tidygraph object containing the nodes and edges of the connected countries, the code can be found in this script. After loading the package (alongside {tidygrapph}) we can see our dataset:\n\nlibrary(\"tidyverse\")\nlibrary(\"tidygraph\")\nlibrary(\"cjhRutils\")\nlibrary(\"ggtext\")\n\nggraph_bordering_countries\n\n# A tbl_graph: 173 nodes and 608 edges\n#\n# A directed simple graph with 26 components\n#\n# A tibble: 173 × 8\n     id iso_a2 iso_a3 name                 name_long name_en region_wb continent\n  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;    \n1     1 AE     ARE    United Arab Emirates United A… United… Middle E… Asia     \n2     2 AF     AFG    Afghanistan          Afghanis… Afghan… South As… Asia     \n3     3 AL     ALB    Albania              Albania   Albania Europe &… Europe   \n4     4 AM     ARM    Armenia              Armenia   Armenia Europe &… Asia     \n5     5 AO     AGO    Angola               Angola    Angola  Sub-Saha… Africa   \n6     6 AQ     ATA    Antarctica           Antarcti… Antarc… Antarcti… Antarcti…\n# ℹ 167 more rows\n#\n# A tibble: 608 × 3\n   from    to border_region             \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                     \n1     1   120 Middle East & North Africa\n2     1   136 Middle East & North Africa\n3     2    34 Cross Region              \n# ℹ 605 more rows\n\n\nThe graph contains all countries with at least one connection, let’s filter the graph to only include countries with two border or more… and visualise that naively with {ggraph}\n\n\nCode\nlibrary(\"ggraph\")\n\nset.seed(1)\nggraph_bordering_countries %&gt;%\n  activate(nodes) %&gt;%\n  mutate(node_degree = tidygraph::centrality_degree()) %&gt;%\n  filter(node_degree &gt; 1) %&gt;%\n  ggraph(layout = 'nicely') +\n  geom_node_point(aes(colour = region_wb)) +\n  geom_edge_link(aes(colour = border_region))"
  },
  {
    "objectID": "posts/2024-10-28_bordering-country-graph/index.html#custom-ggplot2-legends-with-guide_custom",
    "href": "posts/2024-10-28_bordering-country-graph/index.html#custom-ggplot2-legends-with-guide_custom",
    "title": "Bordering countries graph",
    "section": "Custom ggplot2 legends with guide_custom()",
    "text": "Custom ggplot2 legends with guide_custom()\nThe guide/legend for that chart is a little bit complicated. Let’s look at why:\n\nNodes are coloured by the continent the node belongs to.\nEdges are coloured by if the two nodes belong to the same continent.\nThere is a single node from “North America” but no edges with the border_region of “North America”\nThere are edges that need to be coloured “Cross Region” but no nodes with that colour.\n\nTo solve this I thought of using my old trick of hijacking an unused aesthetic and manipulating its guide. However! That’s not really possible in this case, so I googled for alternatives and was extremeley geom_custom() was added in late 2023. In the chart below I’ve used guide_custom() to add a red line that I can use to label cross regional borders.\n\nset.seed(1)\ngg_graph_for_coords &lt;- ggraph_bordering_countries %&gt;%\n  activate(nodes) %&gt;%\n  mutate(node_degree = tidygraph::centrality_degree()) %&gt;%\n  filter(node_degree &gt; 1) %&gt;%\n  ggraph(layout = 'nicely') +\n  geom_node_point(aes(colour = region_wb)) +\n  geom_node_label(aes(label = iso_a2), size = 0) +\n  geom_edge_link() +\n  guides(custom = guide_custom(\n    title = \"Cross regional borders\",\n    grob = grid::linesGrob(\n      x = unit(c(0, 5.4), \"cm\"),\n      y = unit(c(0, 0), \"cm\"),\n      gp = grid::gpar(col = '#F44336', lwd = 3)\n    )\n  ))\n\ngg_graph_for_coords\n\n\n\n\n\n\n\n\nIn the final chart I’d like to add a label to the US to explain why it’s included but Canada isn’t. So let’s grab the coordinates of the node so I can use them to help figure out where to place the label\n\nggplot_build(gg_graph_for_coords)$data[[3]] %&gt;%\n  filter(label == \"US\") %&gt;%\n  select(x, y) %&gt;%\n  as_tibble()\n\n# A tibble: 0 × 2\n# ℹ 2 variables: x &lt;dbl&gt;, y &lt;dbl&gt;\n\n\nNice. Now we can think about beautification. I’ve chosen to use colours from the &lt;coolors.co&gt; service and have found a subjective balance of colours that I think looks good based on how many nodes are in each group. To ensure a little bit of sense to the colours, I’ll order them as a factor so that the group with the most nodes appears at the top of the legend.\n\n\nCode\n# https://coolors.co/448aff-1565c0-009688-8bc34a-ffc107-ff9800-f44336-ad1457\nvec_colours &lt;- c(\n  \"Cross Region\" = \"#F44336\",\n  \"East Asia & Pacific\" = \"#1565C0\",\n  \"Europe & Central Asia\" = \"#009688\",\n  \"Latin America & Caribbean\" = \"#8BC34A\",\n  \"Middle East & North Africa\" = \"#FFC107\",\n  \"North America\" = \"#FF9800\",\n  \"South Asia\" = \"#448AFF\",\n  \"Sub-Saharan Africa\" = \"#AD1457\"\n)\n\nvec_order_borders &lt;- ggraph_bordering_countries %&gt;%\n  activate(edges) %&gt;%\n  as_tibble() %&gt;%\n  count(border_region, sort = TRUE) %&gt;%\n  pull(border_region)\n\nvec_colours &lt;- vec_colours[vec_order_borders]\n\nggraph_one_edge_plus &lt;- ggraph_bordering_countries %&gt;%\n  activate(nodes) %&gt;%\n  mutate(node_degree = tidygraph::centrality_degree()) %&gt;%\n  filter(node_degree &gt; 1)  %&gt;%\n  mutate(region_wb = fct_relevel(region_wb, vec_order_borders)) %&gt;%\n  activate(edges) %&gt;%\n  mutate(\n    border_region = fct_expand(border_region, \"North America\"),\n    border_region = fct_relevel(border_region)\n  )\n\nset.seed(1)\ngg_graph_before_label &lt;- ggraph_one_edge_plus %&gt;%\n  ggraph(layout = 'nicely') +\n  aes(colour = region_wb) +\n  geom_edge_link(aes(colour = border_region),\n                 show.legend = FALSE,\n                 edge_width = 0.5) +\n  geom_node_point(\n    aes(fill = region_wb),\n    # show.legend = TRUE,\n    colour = \"black\",\n    size = 5,\n    pch = 21\n  ) +\n  geom_node_text(\n    aes(\n      label = iso_a2,\n      colour = ifelse(\n        region_wb %in% c(\"Middle East & North Africa\", \"North America\"),\n        \"black\",\n        \"white\"\n      )\n    ),\n    size = 2.3,\n    family = \"Source Code Pro\",\n    fontface = \"bold\"\n  )  +\n  scale_colour_identity() +\n  scale_edge_colour_manual(values = vec_colours, drop = FALSE) +\n  scale_fill_manual(values = vec_colours, drop = FALSE) +\n  guides(\n    custom = guide_custom(\n      title = \"Cross regional borders\",\n      grob = grid::linesGrob(\n        x = unit(c(0, 6.9), \"cm\"),\n        y = unit(c(0, 0), \"cm\"),\n        gp = grid::gpar(col = '#F44336', lwd = 3.5)\n      )\n    ),\n    fill = guide_legend(title = \"\")\n  ) +\n  labs(\n    title = \"Who's connected to who?\",\n    subtitle = \"Countries with at least one land border\",\n    x = \"\",\n    y = \"\",\n    caption = \"@charliejhadley | Source: geodatasource.com/addon/country-borders\"\n  ) +\n  theme_minimal(base_size = 12, base_family = \"Roboto\") +\n  theme(\n    legend.title = element_text(size = 12 * 1.618),\n    panel.grid = element_blank(),\n    plot.caption = element_text(\n      size = 12,\n      family = \"Roboto\",\n      lineheight = 0.5,\n      margin = margin(t = -5)\n    ),\n    plot.title = element_text(\n      family = \"Roboto\",\n      size = 12 * 1.618 ^ 3,\n      margin = margin(t = 20)\n    ),\n    plot.subtitle = element_text(size = 12 * 1.618 ^ 2, margin = margin(b = -10)),\n    panel.grid.major.y = element_blank(),\n    axis.line = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    legend.text = element_text(size = 12 * 1.618),\n    # legend.spacing.y = unit(2.0, \"cm\"),\n    legend.key.size = unit(1, \"cm\"),\n    legend.key = element_rect(color = NA, fill = NA),\n    plot.caption.position = \"plot\"\n  )\n\ngg_graph_before_label\n\n\n\nLet’s add in my labels, which are manually placed but use the node position extracted earlier to help place them.\n\ngg_graph_countries &lt;- gg_graph_before_label +\n  geom_curve(\n    data = tibble(\n      x = 5.08 - 8.5,\n      y = -4.96 ,\n      xend = 5.08 - 1.3,\n      yend = -4.96 - 0.2\n    ),\n    aes(x, y, yend = yend, xend = xend),\n    \n    inherit.aes = FALSE,\n    arrow = arrow(length = unit(0.01, \"npc\")),\n    curvature = 0.2,\n    angle = 90\n  ) +\n  geom_label(\n    data = tibble(\n      x = 5.08 - 8.5,\n      y = -4.96 - 0.5,\n      label = str_wrap(\n        \"Canada isn't here. It only has a single land border with the US - which is included as it has two borders\",\n        30\n      )\n    ),\n    aes(x, y, label = label),\n    fill = colorspace::darken(\"#D8E4EA\"),\n    label.padding = unit(0.4, \"lines\"),\n    hjust = 0,\n    colour = \"black\",\n    inherit.aes = FALSE,\n    size = 4\n  )\n\ngg_graph_countries %&gt;%\n  ggsave(\n    quarto_here(\"gg_graph_countries.png\"),\n    .,\n    width = 4.25 * 3,\n    height = 3.4 * 3,\n    bg = \"#D8E4EA\"\n  )"
  },
  {
    "objectID": "posts/2024-11-08_positives-and-negatives-lists/index.html",
    "href": "posts/2024-11-08_positives-and-negatives-lists/index.html",
    "title": "Pros, Cons and Neutrals lists?",
    "section": "",
    "text": "I thought it would be interesting for me to keep better track of the Pros, Cons and Neutrals that I discover/decide on and to find a way to visualise these nicely. The idea came to me when I was wanting to create a new Quarto blogpost and googled for a quick solution to find this issue where it’s noted\nThat gets to the heart of something that could put off some R users migrating from RMarkdown to Quarto… but also that’s the whole point of Quarto to be cross-platform.\nRight! So what would a Pros / Cons / Neutrals list look like? Well, {bslib} has nice cards available. So it could be something like this:"
  },
  {
    "objectID": "posts/2024-11-08_positives-and-negatives-lists/index.html#feature-creep",
    "href": "posts/2024-11-08_positives-and-negatives-lists/index.html#feature-creep",
    "title": "Pros, Cons and Neutrals lists?",
    "section": "Feature creep",
    "text": "Feature creep\nEver heard of feature creep? She’s a beast.\nI decided the fastest way to record these would be in a Google Sheet that I can then read easily into a Shiny app. But then I thought - I’d love to MoSCoW this. Which then led to me building up a bunch of data validation rules:\n … and gosh, I’d discovered I was procrastinating. I’m really aiming to up my data blogging output and feel part of the tech community again. So, let’s settle with something that’s workable… an iOS note that I can also modify on my laptop."
  },
  {
    "objectID": "posts/2024-11-08_curves-and-stones/index.html",
    "href": "posts/2024-11-08_curves-and-stones/index.html",
    "title": "Curves and stones",
    "section": "",
    "text": "Have you ever seen Megan Harris’ incredible generative art built with R? If not - please take a look! While I was researching ways to make wavy lines with {ggplot2} I came across Megan’s blogpost where she makes this beautiful chart.\nBorrowing pretty much directly from Megan’s code, here’s a nice sine curve that looks a little like the lines on the stones:\nCode\nlibrary(\"tidyverse\")\n\ntheta &lt;- seq(from = 0,\n             to = 2*pi, \n             length.out = 100)\n\nsine &lt;- tibble(x = theta,\n               y = sin(theta),\n               label = 1:length(theta))\n\nwave_theta &lt;- seq(from = 0,\n                  to = 2 * pi, \n                  by = .1) \n\ncurve_top &lt;- tibble(x = wave_theta,\n                    y = sin(x)) %&gt;%\n  arrange(x)\n\ncurve_top %&gt;%\n  ggplot(aes(x=x, y=y))+\n  geom_path(arrow = arrow(type=\"closed\"), linewidth = 3) +\n  coord_fixed(xlim = c(0, 2 * pi),\n              ratio = 1 / 2) +\n  theme_void()\nNice! Okay. So let’s stack 6 of these on top of one another:\nCode\nwave_theta &lt;- seq(from = 0,\n                  to = 2 * pi, \n                  by = .1) \n\ntibble(x = rep(seq(from = 0,\n                  to = 2 * pi, \n                  by = .1) , 6),\n       line = rep(1:6, each = length(wave_theta))) %&gt;% \n  mutate(y = line + sin(x),\n         line = as.character(line)) %&gt;% \n  ggplot(aes(x=x, y=y, group = line))+\n  geom_path(arrow = arrow(type=\"closed\"), linewidth = 5, show.legend = FALSE) +\n  scale_x_continuous(expand = expansion(mult = 0, add = c(-0.1, 0.1))) +\n  coord_fixed(ratio = 1 / 2) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"#9A7D66\", colour = \"transparent\"),\n        panel.border = element_blank())\nI then played around with reflecting these and also making vertical lines… it didn’t quite look right:\nCode\nlibrary(\"patchwork\")\n\ngg_L2R_sin_arrowed &lt;- tibble(x = rep(seq(from = 0,\n                  to = 2 * pi, \n                  by = .1) , 6),\n       line = rep(1:6, each = length(wave_theta))) %&gt;% \n  mutate(y = line + sin(x),\n         line = as.character(line)) %&gt;% \n  ggplot(aes(x=x, y=y, group = line))+\n  geom_path(arrow = arrow(type=\"closed\", ends = \"last\"), linewidth = 5, show.legend = FALSE) +\n  scale_x_continuous(expand = expansion(mult = 0, add = c(-0.1, 0.1))) +\n  scale_y_continuous(expand = expansion(0, c(0.3, 0.3))) +\n  coord_fixed(ratio = 1 / 2,\n              ylim = c(0, 7)) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"#9A7D66\", colour = \"transparent\"),\n        panel.border = element_blank()) \n\ngg_R2L_cos_arrowed &lt;- tibble(x = rep(seq(from = 0,\n                  to = 2 * pi, \n                  by = .1) , 6),\n       line = rep(-1:4, each = length(wave_theta))) %&gt;% \n  mutate(y = line + cos(x),\n         line = as.character(line)) %&gt;% \n  ggplot(aes(x=x, y=y, group = line))+\n  geom_path(arrow = arrow(type=\"closed\", ends = \"first\"), linewidth = 5, show.legend = FALSE) +\n  scale_x_continuous(expand = expansion(mult = 0, add = c(0.1, -0.1))) +\n  scale_y_continuous(expand = expansion(0, c(0.3, 0.3))) +\n  coord_fixed(ratio = 1 / 2,\n              ylim = c(-2, 5.2)) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"#9A7D66\", colour = \"transparent\"),\n        panel.border = element_blank())\n\nggptch_both_horiz &lt;- gg_L2R_sin_arrowed + theme(plot.margin = margin(r = 10)) | gg_R2L_cos_arrowed\n\nggptch_both_horiz %&gt;% \n  ggsave(quarto_here(\"ggptch_both_horiz.png\"),\n         .,\n         width = 5.22 * 2,\n         height = 4 )"
  },
  {
    "objectID": "posts/2024-11-08_curves-and-stones/index.html#arrows-in-the-middle",
    "href": "posts/2024-11-08_curves-and-stones/index.html#arrows-in-the-middle",
    "title": "Curves and stones",
    "section": "Arrows in the middle?",
    "text": "Arrows in the middle?\nWith the arrows at the end of the lines it means that the images need to be padded asymmetrically - or at least it’s not a simple swap between the charts. Let’s see move the arrows to the middle of the lines and swap to using sin for both left and right\n\n\nCode\nseq_x &lt;- seq(from = 0,\n                  to = 2 * pi, \n                  by = pi / 100)\nn_lines &lt;- 11\n\ndata_left_and_right &lt;- tibble(x = rep(seq_x, 9),\n       line = rep(seq(-1, 7), each = length(seq_x))) %&gt;% \n  mutate(y = line + sin(x)) \n\ndata_arrows_left_and_right &lt;- data_left_and_right %&gt;% \n  filter(x %in% c(seq_x[c(100, 102)]),\n         between(line, 1, 6)) %&gt;% \n  group_by(line) %&gt;%\n  summarise(xmin = min(x),\n        xmax = max(x),\n        ymax = max(y),\n        ymin = min(y))\n\ngg_left_to_right &lt;- data_left_and_right %&gt;%\n  ggplot(aes(x = x, y = y, group = line)) +\n    geom_segment(data = data_arrows_left_and_right,\n               aes(x = xmin, y = ymax, xend = xmax, yend = ymin, group = line),\n               arrow = arrow(type=\"closed\", ends = \"last\"), linewidth = 5, show.legend = FALSE) + \n  geom_path(linewidth = 5,\n            show.legend = FALSE,\n            aes(colour = ifelse(between(line, 1, 6), \"main\", \"background\"))) +\n  scale_x_continuous(expand = expansion(0, -0.1)) +\n  scale_y_continuous(expand = expansion(0, 0)) +\n  scale_colour_manual(values = c(\"main\" = \"black\",\n                                 \"background\" = \"grey80\")) +\n  coord_fixed(ratio = 1 / 2, ylim = c(0, 7), xlim = c(0, 2*pi)) +\n  theme_void() +\n  theme(\n    panel.background = element_rect(fill = \"#9A7D66\", colour = \"transparent\"),\n    panel.border = element_blank()\n  ) \n\ngg_right_to_left &lt;- data_left_and_right %&gt;%\n  ggplot(aes(x = x, y = y, group = line)) +\n    geom_segment(data = data_arrows_left_and_right,\n               aes(x = xmin, y = ymax, xend = xmax, yend = ymin, group = line),\n               arrow = arrow(type=\"closed\", ends = \"first\"), linewidth = 5, show.legend = FALSE) + \n  geom_path(linewidth = 5,\n            show.legend = FALSE,\n            aes(colour = ifelse(between(line, 1, 6), \"main\", \"background\"))) +\n  scale_x_continuous(expand = expansion(0, -0.1)) +\n  scale_y_continuous(expand = expansion(0, 0)) +\n  scale_colour_manual(values = c(\"main\" = \"black\",\n                                 \"background\" = \"grey80\")) +\n  coord_fixed(ratio = 1 / 2, ylim = c(0, 7), xlim = c(0, 2*pi)) +\n  theme_void() +\n  theme(\n    panel.background = element_rect(fill = \"#9A7D66\", colour = \"transparent\"),\n    panel.border = element_blank()\n  ) \n\nggptch_left_and_right &lt;- gg_left_to_right + theme(plot.margin = margin(r = 20)) | gg_right_to_left\n\nggptch_left_and_right %&gt;% \n  ggsave(quarto_here(\"ggptch_left_and_right.png\"),\n         .,\n         width = 5 * 2 + 0.5,\n         height = 2.5 * 2 + 1.5)\n\n\n\nCool! I like those. Now let’s make the vertical versions by swapping the x and y coordinates and combine them altogether with {patchwork}\n\n\nCode\nseq_x_tb &lt;- seq(from = -0.5,\n                  to = 2.5 * pi, \n                  by = pi / 100)\nn_lines &lt;- 11\n\ndata_top_and_bottom &lt;- tibble(x = rep(seq_x_tb, 9),\n       line = rep(seq(-1, 7), each = length(seq_x_tb))) %&gt;% \n  mutate(y = line + sin(x)) \n\ndata_arrows_bottom_and_top &lt;- data_top_and_bottom %&gt;% \n  filter(x %in% c(seq_x_tb[c(length(seq_x_tb) / 2 -1 , length(seq_x_tb) / 2 + 1)]),\n         between(line, 1, 6)) %&gt;% \n  group_by(line) %&gt;%\n  summarise(xmin = min(x),\n        xmax = max(x),\n        ymax = max(y),\n        ymin = min(y))\n\ngg_top_to_bottom &lt;- data_top_and_bottom %&gt;%\n  ggplot(aes(y = x, x = y, group = line)) +\n    geom_segment(data = data_arrows_bottom_and_top,\n               aes(y = xmin, x = ymax, yend = xmax, xend = ymin, group = line),\n               arrow = arrow(type=\"closed\", ends = \"first\"), linewidth = 5, show.legend = FALSE) +\n  geom_path(linewidth = 5,\n            show.legend = FALSE,\n            aes(colour = ifelse(between(line, 1, 6), \"main\", \"background\"))) +\n  scale_x_continuous(expand = expansion(0, -0.1)) +\n  scale_y_continuous(expand = expansion(0, 0)) +\n  scale_colour_manual(values = c(\"main\" = \"black\",\n                                 \"background\" = \"grey80\")) +\n  coord_fixed(ratio = 1 / 2, ylim = c(0, 7), xlim = c(0, 2 * pi)) +\n  theme_void() +\n  theme(\n    panel.background = element_rect(fill = \"#9A7D66\", colour = \"transparent\"),\n    panel.border = element_blank()\n  ) \n\ngg_bottom_to_top &lt;- data_top_and_bottom %&gt;%\n  ggplot(aes(y = x, x = y, group = line)) +\n    geom_segment(data = data_arrows_bottom_and_top,\n               aes(y = xmin, x = ymax, yend = xmax, xend = ymin, group = line),\n               arrow = arrow(type=\"closed\", ends = \"last\"), linewidth = 5, show.legend = FALSE) +\n  geom_path(linewidth = 5,\n            show.legend = FALSE,\n            aes(colour = ifelse(between(line, 1, 6), \"main\", \"background\"))) +\n  scale_x_continuous(expand = expansion(0, -0.1)) +\n  scale_y_continuous(expand = expansion(0, 0)) +\n  scale_colour_manual(values = c(\"main\" = \"black\",\n                                 \"background\" = \"grey80\")) +\n  coord_fixed(ratio = 1 / 2, ylim = c(0, 7), xlim = c(0, 2 * pi)) +\n  theme_void() +\n  theme(\n    panel.background = element_rect(fill = \"#9A7D66\", colour = \"transparent\"),\n    panel.border = element_blank()\n  ) \n\n\n(gg_top_to_bottom + theme(plot.margin = margin(r = 20)) | gg_bottom_to_top)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggptch_all_directions &lt;- (gg_left_to_right + theme(plot.margin = margin(r = 20)) | gg_right_to_left) / \n(gg_top_to_bottom + theme(plot.margin = margin(r = 20)) | gg_bottom_to_top)\n\nggptch_all_directions %&gt;% \n  ggsave(quarto_here(\"ggptch_all_directions.png\"),\n         .,\n         width = 5 * 2 + 0.5,\n         height = 2.5 * 2 + 1.5)\n\n\n In an ideal world I’d play around with the vertical images to make them align better, but I can’t quite figure out how to do that today. Hopefully you’ll be seeing these charts again in a project soon. But even if not, I’m quite happy with how they look 😀"
  },
  {
    "objectID": "posts/2024-11-13_motorway-service-stations-info/index.html",
    "href": "posts/2024-11-13_motorway-service-stations-info/index.html",
    "title": "Data Quest: Motorway Services UK",
    "section": "",
    "text": "For very sensible reasons they don’t allow data to be scraped (we can’t use {rvest}), so I’ve manually downloaded all 107 web pages for the motorway service stations and have them in this folder:\nCode\nlibrary(\"cjhRutils\")\nlibrary(\"tidyverse\")\nlist.files(quarto_here(\"service-stations/\"), \".html\") %&gt;% head()\n\n\n[1] \"Abington Services M74 - Motorway Services Information.html\"          \n[2] \"Annandale Water Services A74(M) - Motorway Services Information.html\"\n[3] \"Baldock Services A1(M) - Motorway Services Information.html\"         \n[4] \"Beaconsfield Services M40 - Motorway Services Information.html\"      \n[5] \"Birch Services M62 - Motorway Services Information.html\"             \n[6] \"Birchanger Green Services M11 - Motorway Services Information.html\"\nNow we can use {rvest} to read these HTML files - let’s target the info I want\nCode\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\nexample_abingdon &lt;- read_html(quarto_here(\"service-stations/Abington Services M74 - Motorway Services Information.html\")) %&gt;% \n  html_nodes(\".infotext\") %&gt;% \n  html_text() %&gt;% \n  tibble(\n    info = .\n  ) %&gt;% \n  separate_wider_delim(info, \":\", names = c(\"property\", \"value\")) %&gt;% \n  mutate(value = str_trim(value))\n\nexample_abingdon\n\n\n# A tibble: 18 × 2\n   property                       value                                         \n   &lt;chr&gt;                          &lt;chr&gt;                                         \n 1 Motorway                       \"M74\"                                         \n 2 Where                          \"at J13\"                                      \n 3 County                         \"South Lanarkshire\"                           \n 4 Postcode                       \"ML12 6RG\"                                    \n 5 Type                           \"Single site, used by traffic in both directi…\n 6 Operator                       \"Welcome Break\"                               \n 7 Contact Phone                  \"01864 502637\"                                \n 8 Eat-In Food                    \"Starbucks, Papa John's, Burger King, Dunkin'…\n 9 Takeaway Food / General        \"Retail Shop\"                                 \n10 Other Non-Food Shops           \"WH Smith\"                                    \n11 Picnic Area                    \"yes\"                                         \n12 Cash Machines in main building \"Yes (transaction charge applies)\"            \n13 Parking Charges                \"Cars free for the first 2 hours then £5 for …\n14 Other Facilities/Information   \"GameZone, Tourist Information, BT Openzone\"  \n15 Motel                          \"Days Inn Hotel Abington (Glasgow)\"           \n16 Fuel Brand                     \"Shell\"                                       \n17 LPG available                  \"Yes\"                                         \n18 Cash Machines at fuel station  \"Yes (transaction charge applies)\"\nOkay! That’s enough processing to a function I can use to read in all of the data:\nCode\nread_motorway_services_info &lt;- function(file_path){\n  name_service_station &lt;- str_remove(basename(file_path), \" - Motorway Services Information.html\")\n  \n  read_html(file_path) %&gt;% \n  html_nodes(\".infotext\") %&gt;% \n  html_text() %&gt;% \n  tibble(\n    info = .\n  ) %&gt;%\n  separate_wider_delim(info, \":\", names = c(\"property\", \"value\"), too_many = \"merge\") %&gt;%\n  mutate(value = str_trim(value)) %&gt;%\n  mutate(service_station = name_service_station) %&gt;% \n  identity()\n}\n\nquarto_here(\"service-stations/Baldock Services A1(M) - Motorway Services Information.html\") %&gt;% \n  read_motorway_services_info()\n\n\n# A tibble: 19 × 3\n   property                       value                          service_station\n   &lt;chr&gt;                          &lt;chr&gt;                          &lt;chr&gt;          \n 1 Motorway                       A1(M)                          Baldock Servic…\n 2 Where                          at J10 and from A507           Baldock Servic…\n 3 County                         Hertfordshire                  Baldock Servic…\n 4 Postcode                       SG7 5TR                        Baldock Servic…\n 5 Type                           Single site, used by traffic … Baldock Servic…\n 6 Operator                       Extra MSA                      Baldock Servic…\n 7 Contact Phone                  01494 678876                   Baldock Servic…\n 8 Eat-In Food                    KFC, Le Petit Four, McDonalds… Baldock Servic…\n 9 Takeaway Food / General        M&S Simply Food, WH Smith (wi… Baldock Servic…\n10 Picnic Area                    yes                            Baldock Servic…\n11 Children's Playground          Yes                            Baldock Servic…\n12 Cash Machines in main building Yes (transaction charge appli… Baldock Servic…\n13 Parking Charges                First two hours free for all … Baldock Servic…\n14 Other Facilities/Information   Fast Food & Bakeries and Conv… Baldock Servic…\n15 Motel                          Days Inn Stevenage North       Baldock Servic…\n16 Fuel Brand                     Shell                          Baldock Servic…\n17 LPG available                  Yes                            Baldock Servic…\n18 Cash Machines at fuel station  Yes (free)                     Baldock Servic…\n19 Other Facilities/Information   Costa Express & Deli2Go avail… Baldock Servic…\n\n\nCode\ndata_raw_services &lt;- list.files(quarto_here(\"service-stations/\"), \"[.]html\", full.names = TRUE) %&gt;% \n  map_dfr(~read_motorway_services_info(.x))"
  },
  {
    "objectID": "posts/2024-11-13_motorway-service-stations-info/index.html#northbound-southbound-and-eastbound-westbound",
    "href": "posts/2024-11-13_motorway-service-stations-info/index.html#northbound-southbound-and-eastbound-westbound",
    "title": "Data Quest: Motorway Services UK",
    "section": "Northbound / Southbound and Eastbound / Westbound",
    "text": "Northbound / Southbound and Eastbound / Westbound\nSome service stations come in pairs (dual-site service areas or twin sites) that are split by the motorway and yet still have the same name. For instance, Rownhams Services has a McDonalds when accessed westbound but not eastbound. If you looked at a map of the services it appears that they’re not connected (that’s an overhead sign not a footbridge!).\n But they are! There’s a subway connecting them, which is apparently difficult to discover. Thankfully, our data source www.motorwayservices.info knows they’re connected but does suggest it’s a footbridge.\n\n\nCode\ndata_raw_services %&gt;% \n  filter(service_station == \"Rownhams Services M27\") %&gt;% \n  filter(property == \"Type\") %&gt;% \n  pull(value)\n\n\n[1] \"Separate facilities for each carriageway, but linked by a pedestrian footbridge\"\n\n\nWe need a way to identify these stations. It turns out the “Eat-In Food” property is our friend and identifies the 6 twin-site stations:\n\n\nCode\nvec_eat_in_pairs &lt;- data_raw_services %&gt;% \n  filter(property == \"Eat-In Food\",\n         str_detect(value, \"Northbound|Eastbound\")) %&gt;% \n  pull(service_station)\nvec_eat_in_pairs\n\n\n[1] \"Northampton Services M1\" \"Rownhams Services M27\"  \n[3] \"Sandbach Services M6\"    \"Strensham Services M5\"  \n[5] \"Tibshelf Services M1\"    \"Watford Gap Services M1\""
  },
  {
    "objectID": "posts/2024-11-13_motorway-service-stations-info/index.html#where-can-we-eat",
    "href": "posts/2024-11-13_motorway-service-stations-info/index.html#where-can-we-eat",
    "title": "Data Quest: Motorway Services UK",
    "section": "Where can we eat",
    "text": "Where can we eat\nThe Eat-In variable is the most complicated, interesting and ripe for visualisation. So let’s treat it separately. First we’ll identify our twin-site restaurants:\n\n\nCode\ndata_raw_eat_in &lt;- data_raw_services %&gt;% \n  filter(property == \"Eat-In Food\") %&gt;% \n  mutate(directional = str_detect(value,\n                                \"Northbound|Eastbound\"))\n\ndata_raw_directional_eat &lt;- data_raw_eat_in %&gt;% \n  filter(directional == TRUE) %&gt;% \n  mutate(direction = case_when(\n    str_detect(value, \"Northbound\") ~ \"Northbound|Southbound\",\n    str_detect(value, \"Eastbound\") ~ \"Eastbound|Westbound\"\n  )) %&gt;% \n  separate_longer_delim(direction,\n                        delim = \"|\") %&gt;% \n  mutate(value = case_when(\n    direction == \"Northbound\" ~ str_extract(value,\n                                  \"(?&lt;=Northbound: ).*(?=Southbound)\"),\n    direction == \"Southbound\" ~ str_extract(value, \"(?&lt;=Southbound).*\"),\n    direction == \"Eastbound\" ~ str_extract(value,\n                                  \"(?&lt;=Eastbound: ).*(?=Westbound)\"),\n    direction == \"Westbound\" ~ str_extract(value,\n                                  \"(?&lt;=Westbound: ).*\")\n  )) \n\ndata_raw_directional_eat\n\n\n# A tibble: 12 × 5\n   property    value                       service_station directional direction\n   &lt;chr&gt;       &lt;chr&gt;                       &lt;chr&gt;           &lt;lgl&gt;       &lt;chr&gt;    \n 1 Eat-In Food \"Costa, Restbite, The Burg… Northampton Se… TRUE        Northbou…\n 2 Eat-In Food \" Costa, Hot Food Co., McD… Northampton Se… TRUE        Southbou…\n 3 Eat-In Food \"Costa, Restbite. \"         Rownhams Servi… TRUE        Eastbound\n 4 Eat-In Food \"Costa, Restbite, McDonald… Rownhams Servi… TRUE        Westbound\n 5 Eat-In Food \"Costa and Restbite, \"      Sandbach Servi… TRUE        Northbou…\n 6 Eat-In Food \": Costa, McDonald's, Hot … Sandbach Servi… TRUE        Southbou…\n 7 Eat-In Food \"Soho Coffee Company, Hot … Strensham Serv… TRUE        Northbou…\n 8 Eat-In Food \": Costa, Hot Food Co., Mc… Strensham Serv… TRUE        Southbou…\n 9 Eat-In Food \"Costa, Restbite, McDonald… Tibshelf Servi… TRUE        Northbou…\n10 Eat-In Food \": Costa, Restbite, McDona… Tibshelf Servi… TRUE        Southbou…\n11 Eat-In Food \"Costa, Fresh Food Cafe, M… Watford Gap Se… TRUE        Northbou…\n12 Eat-In Food \": Costa, Restbite, The Bu… Watford Gap Se… TRUE        Southbou…\n\n\nFrustratingly, Strensham Services has an extra little bit of data about Subway being in the Northbound Forecourt. That’ll need manual removal. But other than that I think we end up with fairly well structured data for the eat-in component that we can begin to clean up.\n\n\nCode\ndata_raw_directional_eat &lt;- data_raw_directional_eat %&gt;% \n  mutate(value = str_remove(value, \":\"),\n         value = str_remove(value, \" Northbound.*\"),\n         value = str_trim(value))\n\ndata_raw_directionless_eat &lt;- data_raw_eat_in %&gt;% \n  filter(directional == FALSE) %&gt;% \n  mutate(value = str_remove(value, \":|;\"),\n         value = str_remove(value, \"(Westbound)\"),\n         value = str_trim(value),\n         direction = \"Directionless\")\n\ndata_clean_eat_in &lt;- data_raw_directionless_eat %&gt;% \n  bind_rows(data_raw_directional_eat) %&gt;% \n  select(-directional)\n\n\nThere are lots of alternative spellings in the data, here’s a case_when to grab them all. At some point in the future it would be interesting to see if edit distances could help, but for now let’s concentrate on getting a useful dataset.\n\n\nCode\nfn_fix_value_columns &lt;- function(data){\n  data %&gt;% \n    mutate(value = case_when(\n      str_detect(tolower(value), \"arlo\") ~ \"Arlo's\", \n      str_detect(tolower(value), \"^bk$\") ~ \"Burger King\",\n      str_detect(tolower(value), \"cotton\") ~ \"Cotton Traders\", \n      str_detect(tolower(value), \"chozen\") ~ \"Chozen Noodles\", \n      str_detect(tolower(value), \"cornwall\") ~ \"West Cornwall Pasty Company\", \n      str_detect(tolower(value), \"costa\") ~ \"Costa\", \n      str_detect(tolower(value), \"eat & drink co\") ~ \"Eat & Drink Co\", \n      str_detect(tolower(value), \"edc\") ~ \"Eat & Drink Co\", \n      str_detect(tolower(value), \"fone\") ~ \"FoneBiz\", \n      str_detect(tolower(value), \"full house\") ~ \"Full House\", \n      str_detect(tolower(value), \"greg\") ~ \"Greggs\", \n      str_detect(tolower(value), \"harry\") ~ \"Harry Ramsden's\", \n      str_detect(tolower(value), \"hot food co\") ~ \"Hot Food Co\",\n      str_detect(tolower(value), \"krispy\") ~ \"Krispy Kreme\", \n      str_detect(tolower(value), \"le petit\") ~ \"Le Petit Four\", \n      str_detect(tolower(value), \"lucky coin\") ~ \"Lucky Coin\", \n      str_detect(tolower(value), \"m&s\") ~ \"M&S\", \n      str_detect(tolower(value), \"marks\") ~ \"M&S\", \n      str_detect(tolower(value), \"mcdona\") ~ \"McDonald's\", \n      str_detect(tolower(value), \"papa john\") ~ \"Papa John's\", \n      str_detect(tolower(value), \"pizza hut\") ~ \"Pizza Hut\", \n      str_detect(tolower(value), \"quicksilver\") ~ \"Quicksilver\", \n      str_detect(tolower(value), \"regus\") ~ \"Regus Business Lounge\", \n      str_detect(tolower(value), \"restbite\") ~ \"Restbite\", \n      str_detect(tolower(value), \"soho\") ~ \"SOHO Coffee Co\", \n      str_detect(tolower(value), \"spar\") ~ \"SPAR\", \n      str_detect(tolower(value), \"starbucks\") ~ \"Starbucks\", \n      str_detect(tolower(value), \"the burger\") ~ \"The Burger Company\", \n      str_detect(tolower(value), \"top gift\") ~ \"Top Gift\", \n      str_detect(tolower(value), \"tourist information\") ~ \"Tourist Information\", \n      str_detect(tolower(value), \"upper\") ~ \"Upper Crust\", \n      str_detect(tolower(value), \"whs\") ~ \"WHSmiths\", \n      str_detect(tolower(value), \"wild\") ~ \"Wild Bean Cafe\", \n      tolower(value) %in% tolower(c(\"WH Smith\", \"WHSMiths\", \"Whsmith\",\"W H Smiths\", \"W.H.Smiths\", \"W H Smith\", \"WH Smiths\", \"Wh Smith\", \"WH smith\")) ~ \"WHSmiths\", \n      value == \"Buger King\" ~ \"Burger King\", \n      value == \"M & S Simply food\" ~ \"M&S\",\n      TRUE ~ value\n    ))\n}\n\ndata_long_eat_in &lt;- data_clean_eat_in %&gt;% \n  separate_longer_delim(value,\n                        delim = \",\") %&gt;% \n  mutate(value = str_trim(value)) %&gt;% \n  filter(value != \"\") %&gt;% \n  fn_fix_value_columns() %&gt;% \n  select(retailer = value,\n          service_station,\n         direction)\n\n\nNow… I’m a little unsure about what to do with the “Takeaway Food / General” property as it also contains information about where we can get food but for the 6 twin stations the direction isn’t provided. Let’s deal with the directionless other retailers now:\n\n\nCode\ndata_long_other_shops_directionless &lt;- data_raw_services %&gt;% \n  filter(!service_station %in% vec_eat_in_pairs) %&gt;% \n  filter(property %in% c(\"Takeaway Food / General\", \"Other Non-Food Shops\")) %&gt;% \n  select(value, service_station) %&gt;% \n  filter(value != \"01823680370\") %&gt;% \n  separate_longer_delim(value,\n                        delim = \",\") %&gt;% \n  mutate(value = str_trim(value, side = \"both\")) %&gt;% \n  fn_fix_value_columns() %&gt;% \n  mutate(value = str_remove(value, \"[(].*y[)]\"),\n         value = str_trim(value)) %&gt;% \n  reframe(retailer = value,\n          service_station = service_station,\n         direction = \"Directionless\")\n\n## There's one bad record\ndata_long_other_shops_directionless &lt;- tibble(\n  retailer = c(\"Gamezone\", \"WHSmiths\", \"Waitrose\"),\n  service_station = \"Newport Pagnell Services M1\",\n  direction = \"Directionless\"\n) %&gt;%\n  bind_rows(filter(\n    data_long_other_shops_directionless,!str_detect(retailer, \"24hr Gamezone WHSmith & Waitrose\")\n  ))\n\n\nAnd now I’ll expand out the twin stations:\n\n\nCode\n## Expand out the twins\ndata_long_other_shops_w_direction &lt;- data_raw_services %&gt;% \n  filter(service_station %in% vec_eat_in_pairs) %&gt;% \n  filter(property %in% c(\"Other Non-Food Shops\", \"Takeaway Food / General\")) %&gt;% \n  select(value, service_station) %&gt;% \n  mutate(direction = case_when(\n    service_station == \"Rownhams Services M27\" ~ \"Eastbound;Westbound\",\n    TRUE ~ \"Northbound;Southbound\"\n  )) %&gt;% \n  separate_longer_delim(direction,\n                        delim = \";\") %&gt;% \n  fn_fix_value_columns() %&gt;% \n  rename(retailer = value)\n\n\nIt’s time to combine everything together into a list of retailers which I’ll export into Excel and quickly categorise.\n\n\nCode\ndata_long_retailers &lt;- bind_rows(data_long_eat_in, data_long_other_shops_w_direction, data_long_other_shops_directionless)\n\ndata_long_retailers %&gt;% \n  distinct(retailer) %&gt;% \n  arrange(retailer) %&gt;% \n  write_csv(quarto_here(\"retailer_types.csv\"))\n\n\nLet’s impose these categorisations:\n\nis_food_retailer:\n\nDo we KNOW we it sells some food items?\n\nis_retaurant:\n\nDo we KNOW we can order food to eat in?\n\nis_takeaway:\n\nDo we KNOW we can order food to takeaway\n\nis_prepared_food_only\n\nDo we KNOW that there is no hot/fresh food, Tesco\n\nis_coffee_shop\n\nDo we KNOW you’d nip there for a coffee and it’ll be good? Controversially, McDonald’s isn’t included.\n\n\n\n\nCode\nlibrary(\"readxl\")\ndata_type_of_retailer &lt;- read_excel(quarto_here(\"retailer_types.xlsx\"))\n\ndata_services_retailers &lt;- data_long_retailers %&gt;% \n  left_join(data_type_of_retailer) %&gt;% \n  mutate(across(starts_with(\"is\"), ~ case_when(\n    .x == \"Y\" ~ TRUE,\n    .x == \"N\" ~ FALSE,\n    TRUE ~ NA\n    )))\n\ndata_services_retailers\n\n\n# A tibble: 650 × 8\n   retailer service_station direction is_food_retailer is_restaurant is_takeaway\n   &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;     &lt;lgl&gt;            &lt;lgl&gt;         &lt;lgl&gt;      \n 1 Starbuc… Abington Servi… Directio… TRUE             NA            TRUE       \n 2 Papa Jo… Abington Servi… Directio… TRUE             TRUE          TRUE       \n 3 Burger … Abington Servi… Directio… TRUE             TRUE          TRUE       \n 4 Dunkin'… Abington Servi… Directio… TRUE             FALSE         TRUE       \n 5 Harry R… Abington Servi… Directio… TRUE             TRUE          TRUE       \n 6 Costa    Annandale Wate… Directio… TRUE             FALSE         TRUE       \n 7 Restbite Annandale Wate… Directio… TRUE             NA            NA         \n 8 The Bur… Annandale Wate… Directio… TRUE             TRUE          TRUE       \n 9 KFC      Baldock Servic… Directio… TRUE             TRUE          TRUE       \n10 Le Peti… Baldock Servic… Directio… TRUE             FALSE         TRUE       \n# ℹ 640 more rows\n# ℹ 2 more variables: is_prepared_food_only &lt;lgl&gt;, is_coffee_shop &lt;lgl&gt;"
  },
  {
    "objectID": "posts/2024-11-13_motorway-service-stations-info/index.html#non-food-information",
    "href": "posts/2024-11-13_motorway-service-stations-info/index.html#non-food-information",
    "title": "Data Quest: Motorway Services UK",
    "section": "Non-food information",
    "text": "Non-food information\nThe non-food information is so much easier to deal with. Because I want to create an {sf} object and potentially support exporting as ESRI shapefiles let’s make sure our colnanes have a maximum of 10 characters.\n\n\nCode\ndata_wide_services &lt;- data_raw_services %&gt;% \n  filter(property %in% c(\"Motorway\",\n                         \"Where\",\n                         \"Postcode\",\n                         \"Type\",\n                         \"Operator\",\n                         \"Parking Charges\",\n                         \"LPG available\",\n                         \"Electric Charge Point\")) %&gt;% \n  mutate(property = case_when(\n    property == \"LPG available\" ~ \"has_lpg\",\n    property == \"Electric Charge Point\" ~ \"has_electric_charge\",\n    TRUE ~ property\n  )) %&gt;% \n  mutate(value = str_replace_all(value, \"ï��[0-9]{1,}\", \"£\"),\n         value = str_remove_all(value, \"Â\")) %&gt;% \n  pivot_wider(names_from = property,\n              values_from = value) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(is_single_site = str_detect(type, \"Single site\"),\n         is_twin_station = str_detect(type, \"Separate facilities\"),\n         has_walkway_between_twins = case_when(\n           is_twin_station == TRUE & str_detect(type, \"linked\") ~ TRUE,\n           is_twin_station == TRUE & str_detect(type, \"no link\") ~ FALSE,\n           TRUE ~ NA),\n         is_ireland = str_detect(service_station, \"Ireland\")) %&gt;% \n  reframe(\n    name = service_station,\n    motorway,\n    where,\n    postcode,\n    type,\n    operator,\n    is_ireland,\n    p_charges = parking_charges,\n    has_charge = has_electric_charge,\n    is_single = is_single_site,\n    is_twin = is_twin_station,\n    has_walk = has_walkway_between_twins\n  )\n\n\nThere are some services like Gloucester Services M5 that appear as two distinct rows but they still pass is_single == FALSE. Let’s identify these services and mark them in the dataset as pair_name.\n\n\nCode\ndata_paired_services &lt;- data_wide_services %&gt;% \n  filter(is_single == FALSE) %&gt;% \n  filter(str_detect(name, \"North|South|East|West\")) %&gt;% \n  select(name) %&gt;% \n  separate_wider_delim(name, delim = \"Services\",\n                       names = c(\"name\", \"direction\")) %&gt;% \n  mutate(across(everything(), ~str_trim(.))) %&gt;% \n  separate_wider_delim(direction,\n                       delim = \" \",\n                       names = c(\"direction\",\n                                 \"motorway\"),\n                       too_few = \"align_end\") %&gt;% \n  add_count(name, motorway) %&gt;% \n  filter(n &gt; 1) %&gt;% \n  reframe(name = paste(name, \"Services\", direction, motorway),\n          pair_name = paste(name, motorway))\n\n\ndata_services_info &lt;- data_wide_services %&gt;% \n  left_join(data_paired_services) %&gt;% \n  mutate(is_pair = ifelse(is.na(pair_name), FALSE, TRUE))\n\ndata_services_info\n\n\n# A tibble: 106 × 14\n   name   motorway where postcode type  operator is_ireland p_charges has_charge\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;    &lt;lgl&gt;      &lt;chr&gt;     &lt;chr&gt;     \n 1 Abing… M74      at J… ML12 6RG Sing… Welcome… FALSE      \"Cars fr… &lt;NA&gt;      \n 2 Annan… A74(M)   at J… DG11 1HD Sing… RoadChef FALSE      \"Parking… Yes (More…\n 3 Baldo… A1(M)    at J… SG7 5TR  Sing… Extra M… FALSE      \"First t… &lt;NA&gt;      \n 4 Beaco… M40      at J… HP9 2SE  Sing… Extra M… FALSE       &lt;NA&gt;     &lt;NA&gt;      \n 5 Birch… M62      betw… OL10 2HQ Sepa… Moto     FALSE      \"Car - £… &lt;NA&gt;      \n 6 Birch… M11      at J… CM23 5QZ Sing… Welcome… FALSE      \"Parking… &lt;NA&gt;      \n 7 Black… M65      at J… BB3 0AT  Sing… Extra M… FALSE      \"Parking… &lt;NA&gt;      \n 8 Blyth… A1(M)    at J… S81 8HG  Sing… Moto     FALSE      \"Free fo… Yes (More…\n 9 Bothw… M74      betw… G71 8BG  Faci… RoadChef FALSE      \"Parking… Yes (More…\n10 Bridg… M5       at J… TA6 6TS  Sing… Moto     FALSE      \"Cars - … &lt;NA&gt;      \n# ℹ 96 more rows\n# ℹ 5 more variables: is_single &lt;lgl&gt;, is_twin &lt;lgl&gt;, has_walk &lt;lgl&gt;,\n#   pair_name &lt;chr&gt;, is_pair &lt;lgl&gt;"
  },
  {
    "objectID": "posts/2024-11-13_motorway-service-stations-info/index.html#getting-the-locations-of-the-services",
    "href": "posts/2024-11-13_motorway-service-stations-info/index.html#getting-the-locations-of-the-services",
    "title": "Data Quest: Motorway Services UK",
    "section": "Getting the locations of the services…",
    "text": "Getting the locations of the services…\nI’ve gone and got the coords from Google Maps and stored them in an Excel file (because I’m not perfect). Here’s a very quick interactive {leaflet} map showing where they are:\n\n\nCode\nlibrary(\"sf\")\nlibrary(\"leaflet\")\ndata_raw_service_locations &lt;- read_excel(quarto_here(\"services-locations.xlsx\"))\n\ndata_clean_long_lat &lt;- data_raw_service_locations %&gt;% \n  separate_wider_delim(google_pin,\n                       delim = \",\", \n                       names = c(\"lat\", \"long\")) %&gt;% \n  mutate(long = as.numeric(long),\n         lat = as.numeric(lat)) %&gt;% \n  select(name, long, lat)\n\n\ndata_sf_service_locs &lt;- data_clean_long_lat %&gt;% \n  full_join(data_services_info) %&gt;% \n  st_as_sf(coords = c(\"long\", \"lat\"),\n           crs = 4326)\n  \ndata_sf_service_locs %&gt;% \n  filter(is_ireland == FALSE) %&gt;% \n  leaflet() %&gt;% \n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  addCircleMarkers()"
  },
  {
    "objectID": "posts/2024-11-13_motorway-service-stations-info/index.html#operators",
    "href": "posts/2024-11-13_motorway-service-stations-info/index.html#operators",
    "title": "Data Quest: Motorway Services UK",
    "section": "Operators",
    "text": "Operators\nI want to make sure we’re not over counting operators due to pair sites! So let’s make some explicit counts.\n\nn_named_sites_all: How many uniquely named sites are there across Great Britian? Gloucester Northbound Services M5 and Gloucester Southbound Services M5 are distinctly named, but the Birch Services M62 is listed once despite being a twinned site.\nn_named_sites_mainland: same as above but discounting services in Ireland\nn_named_sites_ireland: only counts uniquely named services in Ireland\nn_single_sites: How many sites are accessible by traffic in both directions\nn_twins: How many sites are twinned, two locations on each side of the motorway with or without a walkway between them\nn_pairs: How many sites have paired names, eg Gloucester Northbound Services M5 and Gloucester Southbound Services M5\n\n\n\nCode\ndata_process_ops_single &lt;- data_services_info %&gt;% \n  select(name, operator, is_single) %&gt;% \n  count(operator, is_single) %&gt;% \n  filter(is_single == TRUE) %&gt;% \n  reframe(operator,\n          n_single_sites = n)\n\ndata_process_ops_twins &lt;- data_services_info %&gt;% \n  count(operator, is_twin) %&gt;% \n  filter(is_twin == TRUE) %&gt;% \n  reframe(operator,\n          n_twins = n)\n\ndata_process_ops_pair &lt;- data_services_info %&gt;% \n  select(name, operator, is_pair) %&gt;% \n  count(operator, is_pair) %&gt;% \n  filter(is_pair == TRUE) %&gt;% \n  reframe(operator,\n          n_pairs = n)\n\ndata_process_ops_simple_all &lt;- data_services_info %&gt;% \n  count(operator, name = \"n_named_sites_all\") \n\ndata_process_ops_simple_ireland &lt;- data_services_info %&gt;% \n  filter(str_detect(name, \"Ireland\")) %&gt;% \n  count(operator, name = \"n_named_sites_ireland\") \n\n\ndata_operators &lt;- data_process_ops_simple_all %&gt;% \n  left_join(data_process_ops_simple_ireland) %&gt;% \n  left_join(data_process_ops_single) %&gt;% \n  left_join(data_process_ops_twins) %&gt;% \n  left_join(data_process_ops_pair) %&gt;% \n  mutate(across(everything(), ~replace_na(.x, 0))) %&gt;% \n  mutate(n_named_sites_mainland = n_named_sites_all - n_named_sites_ireland) %&gt;% \n  select(\n    operator,\n    n_named_sites_all,\n         n_named_sites_mainland,\n         n_named_sites_ireland,\n         everything())\n\ndata_operators\n\n\n# A tibble: 9 × 7\n  operator      n_named_sites_all n_named_sites_mainland n_named_sites_ireland\n  &lt;chr&gt;                     &lt;int&gt;                  &lt;int&gt;                 &lt;int&gt;\n1 Applegreen                    3                      0                     3\n2 BP Connect                    1                      1                     0\n3 Euro Garages                  2                      2                     0\n4 Extra MSA                     7                      7                     0\n5 Moto                         39                     39                     0\n6 RoadChef                     20                     20                     0\n7 Stop 24                       1                      1                     0\n8 Welcome Break                27                     27                     0\n9 Westmorland                   6                      6                     0\n# ℹ 3 more variables: n_single_sites &lt;int&gt;, n_twins &lt;int&gt;, n_pairs &lt;int&gt;"
  },
  {
    "objectID": "posts/2024-11-13_motorway-service-stations-info/index.html#exporting-all-that-good-data",
    "href": "posts/2024-11-13_motorway-service-stations-info/index.html#exporting-all-that-good-data",
    "title": "Data Quest: Motorway Services UK",
    "section": "Exporting all that good data",
    "text": "Exporting all that good data\nI’d really love this dataset to become a Tidy Tuesday dataset! So while writing this post I’ve created a fork of the repo. If my eventual pull request gets accepted we’d be able to pull the data from the official TidyTuesday repo, but until then it’s available as follows\n\n\nCode\nhead(read_csv(\"https://raw.githubusercontent.com/charliejhadley/tidytuesday/refs/heads/Motorway-Services-UK/data/curated/motorway-services-uk/data_service_locations.csv\"))\n\n\n# A tibble: 6 × 15\n  name    long   lat motorway where postcode type  operator p_charges has_charge\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     \n1 Abin… -3.70   55.5 M74      at J… ML12 6RG Sing… Welcome… \"Cars fr… &lt;NA&gt;      \n2 Anna… -3.41   55.2 A74(M)   at J… DG11 1HD Sing… RoadChef \"Parking… Yes (More…\n3 Bald… -0.205  52.0 A1(M)    at J… SG7 5TR  Sing… Extra M… \"First t… &lt;NA&gt;      \n4 Beac… -0.630  51.6 M40      at J… HP9 2SE  Sing… Extra M…  &lt;NA&gt;     &lt;NA&gt;      \n5 Birc… -2.23   53.6 M62      betw… OL10 2HQ Sepa… Moto     \"Car - £… &lt;NA&gt;      \n6 Birc…  0.192  51.9 M11      at J… CM23 5QZ Sing… Welcome… \"Parking… &lt;NA&gt;      \n# ℹ 5 more variables: is_single &lt;lgl&gt;, is_twin &lt;lgl&gt;, has_walk &lt;lgl&gt;,\n#   pair_name &lt;chr&gt;, is_pair &lt;lgl&gt;"
  },
  {
    "objectID": "posts/2024-11-13_motorway-service-stations-info/index.html#lets-make-a-map",
    "href": "posts/2024-11-13_motorway-service-stations-info/index.html#lets-make-a-map",
    "title": "Data Quest: Motorway Services UK",
    "section": "Let’s make a map",
    "text": "Let’s make a map\nI’ve obtained some high quality shapefiles for the UK from the ONS which I’m going to immediately start throwing information away from.\n\nThere aren’t any true service stations in Northern Ireland, so we’ll include only England, Scotland and Wales\nThere are only service stations on the mainland! So let’s discount any polygon with an area smaller than 1E10m^2\n\n\n\nCode\ndata_sf_uk &lt;- read_sf(quarto_here(\"Countries_December_2023_Boundaries_UK_BFC_-5189344684762562119/\"))\n\ndata_sf_gb_mainland &lt;- data_sf_uk %&gt;% \n  filter(CTRY23NM != \"Northern Ireland\") %&gt;% \n  st_cast(\"POLYGON\") %&gt;% \n  mutate(area = as.numeric(st_area(geometry))) %&gt;% \n  filter(area &gt;= 1E10) %&gt;%\n  # st_union() %&gt;%\n  st_as_sf()\n\ndata_sf_gb_mainland\n\n\nSimple feature collection with 3 features and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 134112.4 ymin: 11429.67 xmax: 655653.8 ymax: 976859.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 3 × 10\n  CTRY23CD  CTRY23NM CTRY23NMW  BNG_E  BNG_N  LONG   LAT GlobalID               \n* &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                  \n1 E92000001 England  Lloegr    394883 370883 -2.08  53.2 ea73ad5d-1f4e-4f07-8e0…\n2 S92000003 Scotland Yr Alban  277744 700060 -3.97  56.2 f2267107-2e4a-442e-bc8…\n3 W92000004 Wales    Cymru     263405 242881 -3.99  52.1 d818bd0d-8e08-446f-889…\n# ℹ 2 more variables: geometry &lt;POLYGON [m]&gt;, area &lt;dbl&gt;\n\n\nIt takes a fair amount of time to plot, so I can use `{rmapshaper} to simplify the borders, which look okay:\n\n\nCode\nlibrary(\"rmapshaper\")\n\ndata_sf_simpler_mainland &lt;- ms_simplify(data_sf_gb_mainland, keep = 0.0005)\n\nggplot() +\n  geom_sf(data = data_sf_simpler_mainland) +\n  geom_sf(data = filter(data_sf_service_locs, is_ireland == FALSE )) +\n  coord_sf(crs = 4326,\n           ylim = c(50, 59))\n\n\n\n\n\n\n\n\n\nLet’s build towards an okay looking chart:\n\n\nCode\nlibrary(\"patchwork\")\nlibrary(\"ggtext\")\n\ndata_plot_services &lt;- data_sf_service_locs %&gt;% \n  filter(is_ireland == FALSE) %&gt;% \n  left_join(select(data_operators, operator, n_named_sites_all)) %&gt;% \n  mutate(operator = fct_reorder(operator, n_named_sites_all))\n\ncolour_motorway_blue &lt;- \"#3070B5\"\n\ngg_services_roadless &lt;- ggplot() +\n  geom_sf(data = st_transform(data_sf_simpler_mainland, crs = 4326),\n          fill = colour_motorway_blue,\n          colour = \"white\",\n          linewidth = 0.8) +\n  geom_sf(data = st_transform(data_plot_services, crs = 4326),\n          aes(fill = operator),\n          pch = 21,\n          size = 3.5,\n          colour = \"white\") +\n  geom_richtext(aes(x = -9,\n                    y = 54,\n                    label = \"Tiredness can kill&lt;br&gt;Take a break\"),\n                family = \"Transport\",,\n                fill = \"transparent\",\n                label.color = NA,\n                colour = \"white\"\n  ) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  guides(fill = guide_legend(\n    # override.aes = list(size = 8), \n                             title = \"\", reverse = TRUE)\n         ) +\n  scale_x_continuous(labels = scales::label_number(accuracy = 0.01)) +\n  scale_y_continuous(labels = scales::label_number(accuracy = 0.01)) +\n  coord_sf(crs = 4326,\n           ylim = c(50, 59),\n           xlim = c(-12, 1.76)) + \n  # theme_classic(base_family = \"Transport\") +\n  theme_void(base_family = \"Transport\") +\n  theme(legend.text = element_text(colour = \"white\"),\n        # legend.spacing.y = unit(2.0, \"cm\"),\n        legend.background = element_rect(fill = colour_motorway_blue, colour = \"transparent\"),\n        plot.background = element_rect(fill = colour_motorway_blue),\n        panel.background = element_blank()\n  )\n\ngg_services_roadless\n\n\n\n\n\n\n\n\n\n\nMaking it look like a motorway sign\nThere is a simply beautiful design guide for UK traffic signs that goes into all of the detail, for instance:\n\nAt some point it could be fun to take all of this and convert it into a {ggplot2} theme - but that’s a lot of work. I want to focus on getting that nice round white border on my chart. That’s more difficult than I originally thought, there are two pathways:\n\nFiddle around with grobs thanks to Claus Wilke’s great StackOverflow Answer on adding round corners to the panel border.\nShove a rounded rectangle onto the chart through the geom_rrect() function from {ggchicklet}… which is much easier:\n\n\n\nCode\nlibrary(\"ggchicklet\") # remotes::install_github(\"hrbrmstr/ggchicklet\")\ngg_services_roadless +\n  geom_rrect(aes(xmin = -12, xmax = 1.76, ymin = 50, ymax = 59),\n             fill = \"transparent\",\n             colour = \"white\",\n             r = unit(0.1, 'npc'))\n\n\n\n\n\n\n\n\n\nNow let’s rebuild the chart and set the sizing to work well on export :)\n\n\nCode\nlims_x &lt;- list(min = -12.9, max = 1.86)\nlims_y &lt;- list(min = 50.2, max = 58.5)\n\ngg_services_roadless &lt;- ggplot() +\n  geom_rrect(aes(xmin = lims_x$min - 0.8, \n                 xmax = lims_x$max + 0.8, \n                 ymin = lims_y$min - 0.65, \n                 ymax = lims_y$max + 0.65),\n             fill = colour_motorway_blue,\n             colour = \"white\",\n             size = 15,\n             r = unit(0.1, 'npc')) +\n  geom_sf(data = st_transform(data_sf_simpler_mainland, crs = 4326),\n          fill = colour_motorway_blue,\n          colour = \"white\",\n          linewidth = 0.8) +\n  geom_sf(data = st_transform(data_plot_services, crs = 4326),\n          aes(fill = operator),\n          pch = 21,\n          size = 3.5,\n          colour = \"white\") +\n  geom_richtext(aes(x = -8.5,\n                    y = 53.7,\n                    label = \"Tiredness can kill&lt;br&gt;Take a break\"), \n                size = 20,\n                family = \"Transport\",,\n                fill = \"transparent\",\n                label.color = NA,\n                colour = \"white\"\n                ) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  guides(fill = guide_legend(override.aes = list(size = 8), title = \"\", reverse = TRUE)) +\n  scale_x_continuous(labels = scales::label_number(accuracy = 0.01), expand = expansion(add = 1)) +\n  scale_y_continuous(labels = scales::label_number(accuracy = 0.01), expand = expansion(add = c(1, 1))) +\n  coord_sf(crs = 4326,\n           ylim = as.numeric(lims_y),\n           xlim = as.numeric(lims_x)) + \n  # theme_classic(base_family = \"Transport\") +\n  theme_void(base_family = \"Transport\") +\n  theme(legend.position=c(.85,.75),\n        legend.text = element_text(colour = \"white\", size = 20),\n        legend.spacing.y = unit(2.0, \"cm\"),\n    legend.key.size = unit(1.7, \"cm\"),\n    legend.background = element_rect(fill = \"transparent\", colour = \"transparent\"),\n    plot.background = element_rect(fill = \"grey90\", colour = \"transparent\"),\n        panel.background = element_blank(),\n        plot.margin = margin(t = 1, r = 0, b = 1, l = 0)\n    )\n\ngg_services_roadless\n\n\n\n\n\n\n\n\n\nCode\nggsave(quarto_here(\"gg_services_roadless_simple.png\"),\n       gg_services_roadless,\n       width = 2 * 7.2,\n       height = 2 * 7.5,\n       bg = \"grey90\")"
  },
  {
    "objectID": "posts/2024-11-13_motorway-service-stations-info/index.html#ol",
    "href": "posts/2024-11-13_motorway-service-stations-info/index.html#ol",
    "title": "Data Quest: Motorway Services UK",
    "section": "OL",
    "text": "OL\n\n\nCode\nlibrary(\"patchwork\")\nlibrary(elementalist) # devtools::install_github(\"teunbrand/elementalist\")\nlibrary(grid)\n\ncolour_motorway_blue &lt;- \"#3070B5\"\n\ngg_services &lt;- ggplot() +\n  geom_sf(data = data_sf_simpler_mainland,\n          fill = colour_motorway_blue,\n          colour = \"white\",\n          linewidth = 0.8) +\n  geom_sf(data = data_plot_services,\n          aes(fill = operator),\n          pch = 21,\n          size = 3.5,\n          colour = \"white\") +\n  coord_sf(crs = 4326,\n           ylim = c(50, 59)) + \n  guides(fill = guide_legend(override.aes = list(size = 10), title = \"\", reverse = TRUE)) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  theme_void(base_family = \"Transport\") +\n  theme(legend.position=c(.8,.8),\n        legend.text = element_text(size = 4 * 6,\n                                   colour = \"white\"),\n    # legend.spacing.y = unit(2.0, \"cm\"),\n    legend.key.size = unit(1.7, \"cm\")\n    )\n\ngg_title &lt;- ggplot() +\n  annotate(\"text\", x = 0.4, y = 0.5, label = \"Tiredness can kill\\nTake a break\", size = 3 * 6, hjust = 0.5, colour = \"white\", family = \"Transport\") +\n  coord_cartesian(xlim = c(0, 0.75)) +\n  theme_void() +\n  theme()\n\ngg_combined &lt;- gg_title + gg_services +\n  plot_layout(nrow = 1, widths = c(1.5, 2)) &\n  # theme(plot.margin = unit(c(.2, .2, .2, .2), \"cm\")) &\n  plot_annotation(theme = theme(\n    plot.background = element_rect_round(\n      color  = 'white',\n      linewidth = 10,\n      linetype = 'solid',\n      fill = colour_motorway_blue,\n      radius = unit(0.1, \"snpc\")\n    )\n  ))\n\n# \n# # switch out background grob\n# g &lt;- ggplotGrob(gg_combined)\n# bg &lt;- g$grobs[[1]]\n# round_bg &lt;- roundrectGrob(x=bg$x, y=bg$y, width=bg$width, height=bg$height,\n#                           r=unit(0.1, \"snpc\"),\n#                           just=bg$just, name=bg$name, gp=bg$gp, vp=bg$vp)\n# g$grobs[[1]] &lt;- round_bg\n# \n# plot(g)\n\nggsave(quarto_here(\"gg_tiredness_can_kill.png\"),\n       gg_combined,\n       width = 2 * 8,\n       height = 2 * {1 * 8})"
  },
  {
    "objectID": "posts/2024-11-29_capital-circles/index.html",
    "href": "posts/2024-11-29_capital-circles/index.html",
    "title": "Capital Circles",
    "section": "",
    "text": "I really enjoyed seeing other’s #30DayMapChallenge posts, and particularly those of Ansgar Wolsing. I spent a while thinking about this glorious post\n\n\nAnd had the idea of creating a map of circles centered around cities with the radius being the shortest distance to another city above a certain population threshold. So let’s throw something together.\nFirst I’ll load my packages, and Ansgar introduced me to {giscoR} for obtaining shapefiles. Previously, I’ve used {rnaturalearthdata} but think I might change the default going forwards.\n\nlibrary(\"tidyverse\")\nlibrary(\"maps\")\nlibrary(\"sf\")\nlibrary(\"giscoR\")\nlibrary(\"countrycode\")\n\ncountries_sf &lt;- gisco_get_countries() %&gt;% \n  as_tibble() %&gt;% \n  st_as_sf()\n\n# Manually remove duplicate San Jose and Nicosia\ncities_clean &lt;- world.cities %&gt;% \n  as_tibble() %&gt;% \n  # filter(capital == 1) %&gt;% \n  filter(!(name == \"San Jose\" & lat == 10.97),\n         !(name == \"Nicosia\" & long == 33.37)) %&gt;% \n  mutate(iso_3c = countrycode(country.etc, \"country.name\", \"iso3c\")) %&gt;% \n  drop_na() %&gt;% \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)\n\nLet’s only look at cities with a population of at least 1 million\n\ncities_of_interest &lt;- cities_clean %&gt;% \n  filter(pop &gt;= 1000000)\n\nHere’s a neat little function that finds the max and min distance to each row in the data\n\nadd_max_min_point_to_row &lt;- function(data, row_n){\n  target_row &lt;- slice(data, row_n)\n  \n  all_others &lt;- slice(data, setdiff(1:nrow(data), row_n))\n  \n  all_distances &lt;- st_distance(st_as_sfc(target_row), st_as_sfc(all_others))\n  \n  min_iso_3c &lt;- slice(all_others, which.min(as.numeric(all_distances))) %&gt;% \n    pull(iso_3c)\n  \n  max_iso_3c &lt;- slice(all_others, which.max(as.numeric(all_distances))) %&gt;% \n    pull(iso_3c)\n  \n  target_row %&gt;% \n    mutate(min_iso_3c = min_iso_3c,\n           min_distance = min(all_distances),\n           max_iso_3c = max_iso_3c,\n           max_distance = max(all_distances))\n}\n\n\ncities_of_interest_distances &lt;- 1:nrow(cities_of_interest) %&gt;% \n  map_dfr(~add_max_min_point_to_row(cities_of_interest, .x))\n\nAnd map!\n\ngg_city_circles &lt;- ggplot() +\n  geom_sf(data = cities_of_interest_distances %&gt;% \n            mutate(geometry = st_buffer(geometry, dist = min_distance, nQuadSegs = 100)) %&gt;% \n            st_wrap_dateline() ,\n          fill = \"#663399\",\n          linewidth = 0) + \n  geom_sf(data = countries_sf %&gt;% \n            filter(!NAME_ENGL %in% c(\"Antarctica\")) %&gt;%\n            mutate(area = as.numeric(st_area(geometry))) %&gt;% \n            filter(area &gt;= 1E11),\n          fill = \"transparent\",\n          colour = colorspace::lighten(\"lightblue\", amount = 0.2)) + \n  labs(title = \"Closest cities with populations of least 1 million\") +\n  coord_sf(crs = \"+proj=robin\") +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"lightblue\")) +\n  NULL\n\nggsave(quarto_here(\"gg_city_circles.png\"),\n       gg_city_circles,\n       width = 2 * 4,\n       height = 2 * 2.37)\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hadley2024,\n  author = {Hadley, Charlie},\n  title = {Capital {Circles}},\n  date = {2024-11-30},\n  url = {https://visibledata.co.uk/posts/2024-11-29_capital-circles/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHadley, Charlie. 2024. “Capital Circles.” November 30,\n2024. https://visibledata.co.uk/posts/2024-11-29_capital-circles/."
  },
  {
    "objectID": "posts/2024-11-30_capital-circles/index.html",
    "href": "posts/2024-11-30_capital-circles/index.html",
    "title": "Capital Circles",
    "section": "",
    "text": "I really enjoyed seeing other’s #30DayMapChallenge posts, and particularly those of Ansgar Wolsing. I spent a while thinking about this glorious post\n\n\nAnd had the idea of creating a map of circles centered around cities with the radius being the shortest distance to another city above a certain population threshold. So let’s throw something together.\nFirst I’ll load my packages, and Ansgar introduced me to {giscoR} for obtaining shapefiles. Previously, I’ve used {rnaturalearthdata} but think I might change the default going forwards.\n\nlibrary(\"tidyverse\")\nlibrary(\"maps\")\nlibrary(\"sf\")\nlibrary(\"giscoR\")\nlibrary(\"countrycode\")\n\ncountries_sf &lt;- gisco_get_countries() %&gt;% \n  as_tibble() %&gt;% \n  st_as_sf()\n\n# Manually remove duplicate San Jose and Nicosia\ncities_clean &lt;- world.cities %&gt;% \n  as_tibble() %&gt;% \n  # filter(capital == 1) %&gt;% \n  filter(!(name == \"San Jose\" & lat == 10.97),\n         !(name == \"Nicosia\" & long == 33.37)) %&gt;% \n  mutate(iso_3c = countrycode(country.etc, \"country.name\", \"iso3c\")) %&gt;% \n  drop_na() %&gt;% \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)\n\nLet’s only look at cities with a population of at least 1 million\n\ncities_of_interest &lt;- cities_clean %&gt;% \n  filter(pop &gt;= 1000000)\n\nHere’s a neat little function that finds the max and min distance to each row in the data\n\nadd_max_min_point_to_row &lt;- function(data, row_n){\n  target_row &lt;- slice(data, row_n)\n  \n  all_others &lt;- slice(data, setdiff(1:nrow(data), row_n))\n  \n  all_distances &lt;- st_distance(st_as_sfc(target_row), st_as_sfc(all_others))\n  \n  min_iso_3c &lt;- slice(all_others, which.min(as.numeric(all_distances))) %&gt;% \n    pull(iso_3c)\n  \n  max_iso_3c &lt;- slice(all_others, which.max(as.numeric(all_distances))) %&gt;% \n    pull(iso_3c)\n  \n  target_row %&gt;% \n    mutate(min_iso_3c = min_iso_3c,\n           min_distance = min(all_distances),\n           max_iso_3c = max_iso_3c,\n           max_distance = max(all_distances))\n}\n\n\ncities_of_interest_distances &lt;- 1:nrow(cities_of_interest) %&gt;% \n  map_dfr(~add_max_min_point_to_row(cities_of_interest, .x))\n\nAnd map!\n\ngg_city_circles &lt;- ggplot() +\n  geom_sf(data = cities_of_interest_distances %&gt;% \n            mutate(geometry = st_buffer(geometry, dist = min_distance, nQuadSegs = 100)) %&gt;% \n            st_wrap_dateline() ,\n          fill = \"#663399\",\n          linewidth = 0) + \n  geom_sf(data = countries_sf %&gt;% \n            filter(!NAME_ENGL %in% c(\"Antarctica\")) %&gt;%\n            mutate(area = as.numeric(st_area(geometry))) %&gt;% \n            filter(area &gt;= 1E11),\n          fill = \"transparent\",\n          colour = colorspace::lighten(\"lightblue\", amount = 0.2)) + \n  labs(title = \"Closest cities with populations of least 1 million\") +\n  coord_sf(crs = \"+proj=robin\") +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"lightblue\")) +\n  NULL\n\nggsave(quarto_here(\"gg_city_circles.png\"),\n       gg_city_circles,\n       width = 2 * 4,\n       height = 2 * 2.37)\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{hadley2024,\n  author = {Hadley, Charlie},\n  title = {Capital {Circles}},\n  date = {2024-11-30},\n  url = {https://visibledata.co.uk/posts/2024-11-30_capital-circles/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHadley, Charlie. 2024. “Capital Circles.” November 30,\n2024. https://visibledata.co.uk/posts/2024-11-30_capital-circles/."
  }
]