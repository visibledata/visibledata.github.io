---
title: 'The Office: Who''s who?'
author: ''
date: '2019-12-16'
slug: the-office-who-s-who
categories: []
tags: []
editor_options: 
  chunk_output_type: console
---

I'm going to basically copy Julia Silge's work from here: https://juliasilge.com/blog/tidy-text-classification/

```{r}
library("tidyverse")
library("tidytext")
library("schrute")
```

I'd like to tidy up the data a little bit:

```{r}
theoffice_characters <- theoffice %>%
  select(season:text) %>%
  mutate(season = as.numeric(season),
         episode = as.numeric(episode))
theoffice_characters
```

In Julia's article she adds a `document` row without really highlighting why... I think it's actually crucial, so let's do that:

```{r}
theoffice_characters <- theoffice_characters %>%
  mutate(document = row_number())
```


I want to see how good Jim and Dwight's impressions of one another are, which appear in "Product Recall" - Season 3, Episode 21. 

Let's use every episode before this as our training set:

```{r}
theoffice_before_product_recall <- theoffice_characters %>%
  filter(season <= 3,
         episode < 21)
```

For the moment we only care about Jim and Dwight, so let's extract out the unigrams for each speaker using `unnest_tokens()` and throw away the stop words:

```{r}
jim_dwight_unigrams_before_product_recall <- theoffice_characters %>%
  filter(character %in% c("Dwight", "Jim")) %>%
  select(character, text, document) %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords())
```

I'm going to duplicate one of Julia's charts so we can compare the most common words used by Dwight and Jim.

```{r}
jim_dwight_unigrams_before_product_recall %>%
  count(character, word, sort = TRUE) %>%
  group_by(character) %>%
  top_n(20) %>%
  ungroup() %>%
  ggplot(aes(reorder_within(word, n, character), n,
    fill = character
  )) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~character, scales = "free") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(
    x = NULL, y = "Word count",
    title = "Most frequent words after removing stop words",
    subtitle = "Words like 'said' occupy similar ranks but other words are quite different"
  )
```

These lists are very similar to one another. That's because the stop words in the `{tidytext}` package are collated from prose, and not from spoken word - or **dialogue**.

We 

```{r}
theoffice_characters %>%
  filter(character %in% c("Dwight", "Jim")) %>%
  select(character, text, document) %>%
  unnest_tokens(ngrams, text, token = "ngrams", n = 2) %>%
  separate(ngrams, into = c("word_1",
                            "word_2")) %>%
  filter(!word_1 %in% stop_words$word,
         !word_2 %in% stop_words$word) %>%
  count(character, word_1, word_2, sort = TRUE)
```

We need to create ourselves a set of filler words:

```{r}
filler_words <- tibble(
  word = c("yeah", "oh", "well", "like", "uh", "okay", "just", "um")
)
```

Now we have ourselves 

```{r}
jim_dwight_unigrams_before_product_recall <- theoffice_characters %>%
  filter(character %in% c("Dwight", "Jim")) %>%
  select(character, text, document) %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords()) %>%
  anti_join(filler_words)

jim_dwight_unigrams_before_product_recall %>%
  count(character, word, sort = TRUE) %>%
  group_by(character) %>%
  top_n(20) %>%
  ungroup() %>%
  ggplot(aes(reorder_within(word, n, character), n,
    fill = character
  )) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~character, scales = "free") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(
    x = NULL, y = "Word count",
    title = "Most frequent words after removing stop words",
    subtitle = "Words like 'said' occupy similar ranks but other words are quite different"
  )

```

## Building our model

The `{rsample}` package is designe for splitting up data into training and test sets.

```{r}
library("rsample")
```

But we're going to be unfair and split our data as follows:

- Training: the episodes before the impressions
- Test: the episode of the impressions

```{r}
tidy_training_data <- theoffice_before_product_recall %>%
  select(character, text, document) %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords()) %>%
  anti_join(filler_words)
```

Transform into a sparse matrix:

```{r}
theoffice_sparse_words <- tidy_training_data %>%
  count(document, word) %>%
  inner_join(tidy_training_data) %>%
  cast_sparse(document, word, n)
```



Now we build a data.frame to store our response variable:

```{r}
word_rownames <- as.integer(rownames(theoffice_sparse_words))

theoffice_rejoined_product_recall <- tibble(document = word_rownames) %>%
  left_join(theoffice_before_product_recall %>%
              select(document, character))

```

```{r}
dim(theoffice_rejoined_product_recall)

dim(theoffice_sparse_words)



```


```{r}
library("glmnet")
library("doMC")
registerDoMC(cores = 8)

is_jim <- theoffice_rejoined_product_recall$character == "Jim"
model <- cv.glmnet(theoffice_sparse_words, is_jim,
  family = "binomial",
  parallel = TRUE, keep = TRUE
)
```

```{r}
library(broom)

coefs <- model$glmnet.fit %>%
  tidy() %>%
  filter(lambda == model$lambda.1se)
```


```{r}
coefs %>%
  group_by(estimate > 0) %>%
  top_n(10, abs(estimate)) %>%
  ungroup() %>%
  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) 
```










